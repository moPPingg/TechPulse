# ğŸ“Š BASELINE MODELS CHO TIME SERIES
## ARIMA, GARCH vÃ  Linear Models - Ná»n táº£ng Ä‘á»ƒ so sÃ¡nh

---

## ğŸ“š Má»¤C Lá»¤C

1. [Táº¡i sao cáº§n Baseline?](#1-táº¡i-sao-cáº§n-baseline)
2. [Linear Regression](#2-linear-regression)
3. [ARIMA Models](#3-arima-models)
4. [GARCH Models](#4-garch-models)
5. [Naive Forecasting](#5-naive-forecasting)
6. [So sÃ¡nh cÃ¡c Baselines](#6-so-sÃ¡nh-cÃ¡c-baselines)
7. [BÃ i táº­p thá»±c hÃ nh](#7-bÃ i-táº­p-thá»±c-hÃ nh)

---

## 1. Táº I SAO Cáº¦N BASELINE?

### ğŸ¯ Baseline lÃ  gÃ¬?

> **Baseline = Model Ä‘Æ¡n giáº£n nháº¥t Ä‘á»ƒ so sÃ¡nh**

**Má»¥c Ä‘Ã­ch:**
- Äo lÆ°á»ng xem model phá»©c táº¡p cÃ³ thá»±c sá»± tá»‘t hÆ¡n khÃ´ng
- TrÃ¡nh "overkill" (dÃ¹ng model phá»©c táº¡p cho bÃ i toÃ¡n Ä‘Æ¡n giáº£n)
- Hiá»ƒu Ä‘Æ°á»£c data trÆ°á»›c khi dÃ¹ng deep learning

### ğŸ“Š VÃ­ dá»¥ thá»±c táº¿

**TÃ¬nh huá»‘ng:**
```
Báº¡n: "TÃ´i dÃ¹ng LSTM dá»± Ä‘oÃ¡n FPT, MSE = 5.0"
Reviewer: "So vá»›i baseline?"
Báº¡n: "á»ªm... chÆ°a cÃ³ baseline..."
Reviewer: "Náº¿u chá»‰ dá»± Ä‘oÃ¡n = giÃ¡ hÃ´m qua, MSE = 3.0"
â†’ LSTM cá»§a báº¡n cÃ²n tá»‡ hÆ¡n baseline! âŒ
```

**ÄÃºng cÃ¡ch:**
```
Báº¡n: "Baseline (Naive) MSE = 5.0"
Báº¡n: "Linear Regression MSE = 4.2"
Báº¡n: "ARIMA MSE = 3.8"
Báº¡n: "LSTM MSE = 2.5"
â†’ LSTM tá»‘t hÆ¡n baseline 50%! âœ…
```

### ğŸ’¡ Quy táº¯c vÃ ng

> **LUÃ”N LUÃ”N implement baseline trÆ°á»›c khi lÃ m model phá»©c táº¡p!**

---

## 2. LINEAR REGRESSION

### ğŸ¯ Linear Regression cho Time Series

**Ã tÆ°á»Ÿng:**
```
price_tomorrow = w1Ã—close_today + w2Ã—ma20 + w3Ã—rsi + w4Ã—macd + b
```

**Æ¯u Ä‘iá»ƒm:**
- ÄÆ¡n giáº£n, nhanh
- Dá»… interpret (xem weights)
- Baseline tá»‘t

**NhÆ°á»£c Ä‘iá»ƒm:**
- Giáº£ Ä‘á»‹nh linear relationship
- KhÃ´ng capture Ä‘Æ°á»£c non-linear patterns

### ğŸ”§ Implementation

**BÆ°á»›c 1: Chuáº©n bá»‹ data**
```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Load features
df = pd.read_csv('data/features/vn30/FPT.csv')
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date')

# Chá»n features
feature_cols = ['close', 'ma_20', 'rsi_14', 'macd', 'volatility_20']
X = df[feature_cols]

# Target: giÃ¡ ngÃ y mai
y = df['close'].shift(-1)

# Drop NaN
data = pd.concat([X, y.rename('target')], axis=1).dropna()
X = data[feature_cols]
y = data['target']

# Train/test split (80/20)
split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print(f"Training samples: {len(X_train)}")
print(f"Test samples: {len(X_test)}")
```

**BÆ°á»›c 2: Train model**
```python
# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

print("\nModel trained!")
print(f"Intercept: {model.intercept_:.2f}")
print("\nCoefficients:")
for feat, coef in zip(feature_cols, model.coef_):
    print(f"  {feat}: {coef:.4f}")
```

**BÆ°á»›c 3: Evaluate**
```python
# Training metrics
train_mse = mean_squared_error(y_train, y_pred_train)
train_mae = mean_absolute_error(y_train, y_pred_train)
train_rmse = np.sqrt(train_mse)

# Test metrics
test_mse = mean_squared_error(y_test, y_pred_test)
test_mae = mean_absolute_error(y_test, y_pred_test)
test_rmse = np.sqrt(test_mse)

print("\n=== EVALUATION ===")
print(f"Training MSE:  {train_mse:.2f}")
print(f"Training RMSE: {train_rmse:.2f}")
print(f"Training MAE:  {train_mae:.2f}")
print()
print(f"Test MSE:  {test_mse:.2f}")
print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MAE:  {test_mae:.2f}")
```

**BÆ°á»›c 4: Visualize**
```python
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))

# Training predictions
plt.subplot(1, 2, 1)
plt.plot(y_train.values, label='Actual', alpha=0.7)
plt.plot(y_pred_train, label='Predicted', alpha=0.7)
plt.title(f'Training Set (MSE={train_mse:.2f})')
plt.xlabel('Days')
plt.ylabel('Price')
plt.legend()

# Test predictions
plt.subplot(1, 2, 2)
plt.plot(y_test.values, label='Actual', alpha=0.7)
plt.plot(y_pred_test, label='Predicted', alpha=0.7)
plt.title(f'Test Set (MSE={test_mse:.2f})')
plt.xlabel('Days')
plt.ylabel('Price')
plt.legend()

plt.tight_layout()
plt.savefig('linear_regression_results.png', dpi=300)
plt.show()
```

### ğŸ’¡ Interpret Results

**Feature Importance (tá»« coefficients):**
```python
# Sáº¯p xáº¿p features theo importance
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'coefficient': model.coef_,
    'abs_coefficient': np.abs(model.coef_)
}).sort_values('abs_coefficient', ascending=False)

print("\nFeature Importance:")
print(feature_importance)

# Visualize
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['feature'], feature_importance['coefficient'])
plt.xlabel('Coefficient')
plt.title('Feature Importance (Linear Regression)')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300)
plt.show()
```

---

## 3. ARIMA MODELS

### ğŸ¤” ARIMA lÃ  gÃ¬?

**ARIMA = AutoRegressive Integrated Moving Average**

**PhÃ¢n tÃ­ch tá»«ng pháº§n:**
- **AR (AutoRegressive):** DÃ¹ng giÃ¡ trá»‹ quÃ¡ khá»© Ä‘á»ƒ dá»± Ä‘oÃ¡n
- **I (Integrated):** Differencing Ä‘á»ƒ lÃ m stationary
- **MA (Moving Average):** DÃ¹ng errors quÃ¡ khá»© Ä‘á»ƒ dá»± Ä‘oÃ¡n

### ğŸ“ ARIMA(p, d, q)

**p (AR order):**
- Sá»‘ lags cá»§a giÃ¡ trá»‹ quÃ¡ khá»©
- VÃ­ dá»¥: p=2 â†’ dÃ¹ng t-1 vÃ  t-2

**d (Differencing order):**
- Sá»‘ láº§n differencing
- d=0: KhÃ´ng differencing
- d=1: First difference (price[t] - price[t-1])
- d=2: Second difference

**q (MA order):**
- Sá»‘ lags cá»§a errors quÃ¡ khá»©
- VÃ­ dá»¥: q=1 â†’ dÃ¹ng error táº¡i t-1

### ğŸ¯ VÃ­ dá»¥: ARIMA(1,1,1)

**CÃ´ng thá»©c:**
```
Î”y(t) = c + Ï†â‚Ã—Î”y(t-1) + Î¸â‚Ã—Îµ(t-1) + Îµ(t)
 â†‘       â†‘    â†‘           â†‘
 Diff  Const  AR(1)       MA(1)

Trong Ä‘Ã³:
- Î”y(t) = y(t) - y(t-1) (first difference)
- Ï†â‚: AR coefficient
- Î¸â‚: MA coefficient
- Îµ(t): Error táº¡i thá»i Ä‘iá»ƒm t
```

### ğŸ”§ Implementation

**BÆ°á»›c 1: Kiá»ƒm tra Stationarity**
```python
from statsmodels.tsa.stattools import adfuller

def check_stationarity(series, name='Series'):
    """
    Kiá»ƒm tra stationarity báº±ng ADF test
    """
    result = adfuller(series.dropna())
    
    print(f"\n=== ADF Test for {name} ===")
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    print(f"Critical Values:")
    for key, value in result[4].items():
        print(f"  {key}: {value:.4f}")
    
    if result[1] < 0.05:
        print("â†’ STATIONARY (p-value < 0.05)")
    else:
        print("â†’ NON-STATIONARY (p-value >= 0.05)")
    
    return result[1] < 0.05

# Kiá»ƒm tra price
is_stationary_price = check_stationarity(df['close'], 'Price')

# Kiá»ƒm tra returns
df['returns'] = df['close'].pct_change()
is_stationary_returns = check_stationarity(df['returns'], 'Returns')
```

**BÆ°á»›c 2: Chá»n p, d, q**

**CÃ¡ch 1: Auto ARIMA**
```python
from pmdarima import auto_arima

# Auto ARIMA sáº½ tá»± Ä‘á»™ng tÃ¬m p, d, q tá»‘t nháº¥t
model = auto_arima(
    df['close'],
    start_p=0, max_p=5,
    start_q=0, max_q=5,
    d=None,  # Tá»± Ä‘á»™ng tÃ¬m d
    seasonal=False,
    trace=True,  # In ra quÃ¡ trÃ¬nh tÃ¬m kiáº¿m
    error_action='ignore',
    suppress_warnings=True,
    stepwise=True
)

print(f"\nBest model: ARIMA{model.order}")
print(model.summary())
```

**CÃ¡ch 2: Manual (dÃ¹ng ACF/PACF)**
```python
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# ACF plot (Ä‘á»ƒ chá»n q)
plot_acf(df['returns'].dropna(), lags=20, ax=axes[0])
axes[0].set_title('ACF Plot')

# PACF plot (Ä‘á»ƒ chá»n p)
plot_pacf(df['returns'].dropna(), lags=20, ax=axes[1])
axes[1].set_title('PACF Plot')

plt.tight_layout()
plt.show()

# Quy táº¯c:
# - Náº¿u ACF cuts off sau lag q â†’ MA(q)
# - Náº¿u PACF cuts off sau lag p â†’ AR(p)
# - Náº¿u cáº£ 2 Ä‘á»u decay dáº§n â†’ ARMA(p,q)
```

**BÆ°á»›c 3: Train ARIMA**
```python
from statsmodels.tsa.arima.model import ARIMA

# Split data
train_size = int(len(df) * 0.8)
train = df['close'][:train_size]
test = df['close'][train_size:]

# Train ARIMA(1,1,1)
model = ARIMA(train, order=(1, 1, 1))
model_fit = model.fit()

print(model_fit.summary())
```

**BÆ°á»›c 4: Forecast**
```python
# Forecast test period
forecast = model_fit.forecast(steps=len(test))

# Metrics
test_mse = mean_squared_error(test, forecast)
test_mae = mean_absolute_error(test, forecast)
test_rmse = np.sqrt(test_mse)

print(f"\n=== ARIMA EVALUATION ===")
print(f"Test MSE:  {test_mse:.2f}")
print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MAE:  {test_mae:.2f}")

# Visualize
plt.figure(figsize=(14, 6))
plt.plot(train.index, train, label='Training', alpha=0.7)
plt.plot(test.index, test, label='Actual Test', alpha=0.7)
plt.plot(test.index, forecast, label='ARIMA Forecast', alpha=0.7)
plt.title(f'ARIMA(1,1,1) Forecast (MSE={test_mse:.2f})')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.tight_layout()
plt.savefig('arima_forecast.png', dpi=300)
plt.show()
```

### ğŸ’¡ Khi nÃ o dÃ¹ng ARIMA?

**DÃ¹ng khi:**
- Data cÃ³ autocorrelation máº¡nh
- Muá»‘n model Ä‘Æ¡n giáº£n, interpret Ä‘Æ°á»£c
- Dá»± Ä‘oÃ¡n ngáº¯n háº¡n (1-7 ngÃ y)

**KHÃ”NG dÃ¹ng khi:**
- Data cÃ³ nhiá»u external factors (news, events)
- Cáº§n dá»± Ä‘oÃ¡n dÃ i háº¡n (>1 thÃ¡ng)
- Data cÃ³ non-linear patterns phá»©c táº¡p

---

## 4. GARCH MODELS

### ğŸ¤” GARCH lÃ  gÃ¬?

**GARCH = Generalized AutoRegressive Conditional Heteroskedasticity**

**Má»¥c Ä‘Ã­ch:**
- Dá»± Ä‘oÃ¡n **VOLATILITY** (Ä‘á»™ biáº¿n Ä‘á»™ng)
- KHÃ”NG dá»± Ä‘oÃ¡n giÃ¡ trá»±c tiáº¿p

**Táº¡i sao quan trá»ng?**
- Volatility cao = Rá»§i ro cao
- Volatility clustering: Biáº¿n Ä‘á»™ng lá»›n thÆ°á»ng theo sau biáº¿n Ä‘á»™ng lá»›n
- Quan trá»ng cho risk management

### ğŸ“ GARCH(1,1)

**CÃ´ng thá»©c:**
```
ÏƒÂ²(t) = Ï‰ + Î±Ã—ÎµÂ²(t-1) + Î²Ã—ÏƒÂ²(t-1)
 â†‘       â†‘    â†‘           â†‘
Vol(t) Const Error(t-1)  Vol(t-1)

Trong Ä‘Ã³:
- ÏƒÂ²(t): Variance (volatilityÂ²) táº¡i thá»i Ä‘iá»ƒm t
- ÎµÂ²(t-1): Squared error táº¡i t-1
- Î±: ARCH coefficient
- Î²: GARCH coefficient
```

**Ã nghÄ©a:**
- Î± cao: Shocks áº£nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n volatility
- Î² cao: Volatility persistence (biáº¿n Ä‘á»™ng kÃ©o dÃ i)
- Î± + Î² â‰ˆ 1: Volatility ráº¥t persistent

### ğŸ”§ Implementation

**BÆ°á»›c 1: Chuáº©n bá»‹ returns**
```python
from arch import arch_model

# TÃ­nh returns (%)
df['returns'] = df['close'].pct_change() * 100
returns = df['returns'].dropna()

# Split
train_size = int(len(returns) * 0.8)
train_returns = returns[:train_size]
test_returns = returns[train_size:]

print(f"Training samples: {len(train_returns)}")
print(f"Test samples: {len(test_returns)}")
```

**BÆ°á»›c 2: Train GARCH**
```python
# Define GARCH(1,1) model
model = arch_model(
    train_returns,
    vol='Garch',  # GARCH model
    p=1,          # GARCH order
    q=1           # ARCH order
)

# Fit model
model_fit = model.fit(disp='off')
print(model_fit.summary())
```

**BÆ°á»›c 3: Forecast Volatility**
```python
# Forecast
forecast = model_fit.forecast(horizon=len(test_returns))

# Extract forecasted variance
forecast_variance = forecast.variance.values[-1, :]
forecast_volatility = np.sqrt(forecast_variance)

# Actual volatility (rolling std)
actual_volatility = test_returns.rolling(window=20).std()

# Metrics
vol_mse = mean_squared_error(
    actual_volatility.dropna(),
    forecast_volatility[:len(actual_volatility.dropna())]
)

print(f"\n=== GARCH EVALUATION ===")
print(f"Volatility MSE: {vol_mse:.4f}")
```

**BÆ°á»›c 4: Visualize**
```python
plt.figure(figsize=(14, 8))

# Returns
plt.subplot(2, 1, 1)
plt.plot(test_returns.index, test_returns, label='Returns', alpha=0.5)
plt.title('Test Returns')
plt.ylabel('Returns (%)')
plt.legend()

# Volatility
plt.subplot(2, 1, 2)
plt.plot(actual_volatility.index, actual_volatility, 
         label='Actual Volatility (Rolling Std)', alpha=0.7)
plt.plot(test_returns.index[:len(forecast_volatility)], forecast_volatility, 
         label='GARCH Forecast', alpha=0.7)
plt.title('Volatility Forecast')
plt.ylabel('Volatility (%)')
plt.xlabel('Date')
plt.legend()

plt.tight_layout()
plt.savefig('garch_forecast.png', dpi=300)
plt.show()
```

### ğŸ’¡ Khi nÃ o dÃ¹ng GARCH?

**DÃ¹ng khi:**
- Cáº§n dá»± Ä‘oÃ¡n volatility/risk
- Data cÃ³ volatility clustering
- Risk management, option pricing

**KHÃ”NG dÃ¹ng khi:**
- Cáº§n dá»± Ä‘oÃ¡n giÃ¡ trá»±c tiáº¿p (dÃ¹ng ARIMA hoáº·c ML)

---

## 5. NAIVE FORECASTING

### ğŸ¯ Naive Methods

**ÄÆ¡n giáº£n nhÆ°ng hiá»‡u quáº£!**

#### **1. Naive Forecast (Last Value)**

**CÃ´ng thá»©c:**
```
Å·(t+1) = y(t)

VÃ­ dá»¥:
GiÃ¡ hÃ´m nay: 100
Dá»± Ä‘oÃ¡n ngÃ y mai: 100
```

**Code:**
```python
def naive_forecast(train, test):
    """
    Naive forecast: Dá»± Ä‘oÃ¡n = giÃ¡ trá»‹ cuá»‘i cÃ¹ng cá»§a training
    """
    forecast = np.full(len(test), train.iloc[-1])
    return forecast

# Evaluate
forecast = naive_forecast(train, test)
mse = mean_squared_error(test, forecast)
print(f"Naive MSE: {mse:.2f}")
```

#### **2. Seasonal Naive**

**CÃ´ng thá»©c:**
```
Å·(t+1) = y(t-m)

Trong Ä‘Ã³ m = seasonal period

VÃ­ dá»¥ (weekly seasonality, m=5):
Dá»± Ä‘oÃ¡n Thá»© 2 tuáº§n nÃ y = Thá»© 2 tuáº§n trÆ°á»›c
```

**Code:**
```python
def seasonal_naive_forecast(train, test, period=5):
    """
    Seasonal naive: Dá»± Ä‘oÃ¡n = giÃ¡ trá»‹ cÃ¹ng ká»³ trÆ°á»›c
    """
    forecast = []
    for i in range(len(test)):
        if i < period:
            # DÃ¹ng giÃ¡ trá»‹ tá»« training
            forecast.append(train.iloc[-(period-i)])
        else:
            # DÃ¹ng giÃ¡ trá»‹ tá»« test
            forecast.append(forecast[i-period])
    return np.array(forecast)
```

#### **3. Moving Average**

**CÃ´ng thá»©c:**
```
Å·(t+1) = (y(t) + y(t-1) + ... + y(t-k+1)) / k

VÃ­ dá»¥ (k=5):
Dá»± Ä‘oÃ¡n ngÃ y mai = Trung bÃ¬nh 5 ngÃ y gáº§n nháº¥t
```

**Code:**
```python
def moving_average_forecast(train, test, window=5):
    """
    Moving average forecast
    """
    forecast = []
    history = list(train[-window:])
    
    for i in range(len(test)):
        # Dá»± Ä‘oÃ¡n = trung bÃ¬nh window gáº§n nháº¥t
        pred = np.mean(history)
        forecast.append(pred)
        
        # Update history vá»›i actual value
        history.append(test.iloc[i])
        history.pop(0)
    
    return np.array(forecast)
```

---

## 6. SO SÃNH CÃC BASELINES

### ğŸ“Š Benchmark Template

```python
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

def evaluate_model(name, y_true, y_pred):
    """
    Evaluate and return metrics
    """
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    return {
        'Model': name,
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'MAPE': mape
    }

# Collect results
results = []

# 1. Naive
forecast_naive = naive_forecast(train, test)
results.append(evaluate_model('Naive', test, forecast_naive))

# 2. Moving Average
forecast_ma = moving_average_forecast(train, test, window=5)
results.append(evaluate_model('Moving Average (5)', test, forecast_ma))

# 3. Linear Regression
# (Ä‘Ã£ train á»Ÿ trÃªn)
results.append(evaluate_model('Linear Regression', y_test, y_pred_test))

# 4. ARIMA
# (Ä‘Ã£ train á»Ÿ trÃªn)
results.append(evaluate_model('ARIMA(1,1,1)', test, forecast))

# Create comparison table
comparison_df = pd.DataFrame(results)
comparison_df = comparison_df.sort_values('MSE')

print("\n=== BASELINE COMPARISON ===")
print(comparison_df.to_string(index=False))

# Visualize
comparison_df.plot(x='Model', y=['MSE', 'MAE'], kind='bar', figsize=(10, 6))
plt.title('Baseline Models Comparison')
plt.ylabel('Error')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('baseline_comparison.png', dpi=300)
plt.show()
```

### ğŸ’¡ Interpret Results

**VÃ­ dá»¥ káº¿t quáº£:**
```
Model                 MSE    RMSE    MAE    MAPE
Naive                 5.2    2.28    1.8    1.85%
Moving Average (5)    4.8    2.19    1.7    1.75%
ARIMA(1,1,1)         3.9    1.97    1.5    1.52%
Linear Regression     3.2    1.79    1.3    1.35%
```

**Káº¿t luáº­n:**
- Linear Regression tá»‘t nháº¥t (MSE tháº¥p nháº¥t)
- ARIMA tá»‘t hÆ¡n Naive 25%
- Moving Average tá»‘t hÆ¡n Naive 8%
- Baseline Ä‘Ã£ set Ä‘Æ°á»£c "bar" cho deep learning models

---

## 7. BÃ€I Táº¬P THá»°C HÃ€NH

### ğŸ¯ BÃ i táº­p 1: Implement Full Baseline Pipeline

**Äá» bÃ i:**
Implement vÃ  so sÃ¡nh 4 baselines cho FPT:
1. Naive
2. Moving Average (window=5, 10, 20)
3. Linear Regression
4. ARIMA(p,d,q) - tá»± chá»n p,d,q

**YÃªu cáº§u:**
- Train trÃªn 80% data
- Test trÃªn 20% data
- TÃ­nh MSE, MAE, RMSE, MAPE
- Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh
- Viáº¿t bÃ¡o cÃ¡o ngáº¯n (200-300 tá»«)

**Kiá»ƒm tra:**
- [ ] Implement Ä‘Æ°á»£c 4 baselines
- [ ] TÃ­nh Ä‘Æ°á»£c metrics Ä‘áº§y Ä‘á»§
- [ ] Váº½ Ä‘Æ°á»£c biá»ƒu Ä‘á»“ Ä‘áº¹p
- [ ] Viáº¿t Ä‘Æ°á»£c bÃ¡o cÃ¡o phÃ¢n tÃ­ch

---

### ğŸ¯ BÃ i táº­p 2: GARCH cho Volatility Forecasting

**Äá» bÃ i:**
DÃ¹ng GARCH(1,1) dá»± Ä‘oÃ¡n volatility cá»§a FPT

**YÃªu cáº§u:**
- TÃ­nh returns
- Train GARCH(1,1)
- Forecast volatility cho test period
- So sÃ¡nh vá»›i actual volatility (rolling std)
- PhÃ¢n tÃ­ch Î± vÃ  Î² coefficients

**Kiá»ƒm tra:**
- [ ] Train Ä‘Æ°á»£c GARCH
- [ ] Forecast Ä‘Æ°á»£c volatility
- [ ] So sÃ¡nh vá»›i actual
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c Î±, Î²

---

### ğŸ¯ BÃ i táº­p 3: Feature Engineering cho Linear Regression

**Äá» bÃ i:**
Cáº£i thiá»‡n Linear Regression báº±ng feature engineering

**Gá»£i Ã½ features:**
- Lagged features (close_lag1, close_lag5, ...)
- Rolling statistics (rolling_mean_5, rolling_std_10, ...)
- Interaction features (close Ã— ma20, rsi Ã— volume_ratio, ...)
- Polynomial features (closeÂ², closeÂ³, ...)

**YÃªu cáº§u:**
- ThÃªm Ã­t nháº¥t 10 features má»›i
- Train Linear Regression vá»›i features má»›i
- So sÃ¡nh vá»›i baseline Linear Regression
- PhÃ¢n tÃ­ch feature importance

**Kiá»ƒm tra:**
- [ ] Táº¡o Ä‘Æ°á»£c features má»›i
- [ ] Train Ä‘Æ°á»£c model
- [ ] Cáº£i thiá»‡n Ä‘Æ°á»£c MSE
- [ ] PhÃ¢n tÃ­ch Ä‘Æ°á»£c features quan trá»ng

---

## âœ… KIá»‚M TRA HIá»‚U BÃ€I

TrÆ°á»›c khi sang bÃ i tiáº¿p theo, hÃ£y Ä‘áº£m báº£o báº¡n:

- [ ] Hiá»ƒu táº¡i sao cáº§n baseline models
- [ ] Implement Ä‘Æ°á»£c Linear Regression cho time series
- [ ] Hiá»ƒu Ä‘Æ°á»£c ARIMA(p,d,q) vÃ  cÃ¡ch chá»n p,d,q
- [ ] Implement Ä‘Æ°á»£c ARIMA
- [ ] Hiá»ƒu Ä‘Æ°á»£c GARCH vÃ  khi nÃ o dÃ¹ng
- [ ] Implement Ä‘Æ°á»£c cÃ¡c naive methods
- [ ] So sÃ¡nh Ä‘Æ°á»£c cÃ¡c baselines
- [ ] LÃ m Ä‘Æ°á»£c 3 bÃ i táº­p thá»±c hÃ nh

**Náº¿u chÆ°a pass háº¿t checklist, Ä‘á»c láº¡i pháº§n tÆ°Æ¡ng á»©ng!**

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

**Books:**
- "Forecasting: Principles and Practice" - Rob Hyndman
- "Time Series Analysis and Its Applications" - Shumway & Stoffer

**Papers:**
- "Forecasting with Exponential Smoothing" - Hyndman et al.
- "ARIMA Models and the Box-Jenkins Methodology" - Box & Jenkins

**Libraries:**
- `statsmodels`: ARIMA, SARIMAX
- `pmdarima`: Auto ARIMA
- `arch`: GARCH models

---

## ğŸš€ BÆ¯á»šC TIáº¾P THEO

Sau khi hoÃ n thÃ nh bÃ i nÃ y, sang:
- `02_ML_MODELS.md` - XGBoost, LightGBM, Random Forest

**ChÃºc báº¡n há»c tá»‘t! ğŸ“**
