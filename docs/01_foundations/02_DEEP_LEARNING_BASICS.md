# ğŸ§  DEEP LEARNING CÆ  Báº¢N
## Neural Networks tá»« Ä‘áº§u - Hiá»ƒu Ä‘á»ƒ lÃ m

---

## ğŸ“š Má»¤C Lá»¤C

1. [Neural Network lÃ  gÃ¬?](#1-neural-network-lÃ -gÃ¬)
2. [Perceptron - Neuron Ä‘Æ¡n giáº£n](#2-perceptron---neuron-Ä‘Æ¡n-giáº£n)
3. [Activation Functions](#3-activation-functions)
4. [Forward Propagation](#4-forward-propagation)
5. [Loss Functions](#5-loss-functions)
6. [Backpropagation](#6-backpropagation)
7. [Gradient Descent](#7-gradient-descent)
8. [Overfitting & Regularization](#8-overfitting--regularization)
9. [BÃ i táº­p thá»±c hÃ nh](#9-bÃ i-táº­p-thá»±c-hÃ nh)

---

## 1. NEURAL NETWORK LÃ€ GÃŒ?

### ğŸ§  Láº¥y cáº£m há»©ng tá»« nÃ£o bá»™

**NÃ£o ngÆ°á»i:**
```
Neuron (táº¿ bÃ o tháº§n kinh):
- Nháº­n tÃ­n hiá»‡u tá»« nhiá»u neurons khÃ¡c
- Xá»­ lÃ½ tÃ­n hiá»‡u
- Gá»­i tÃ­n hiá»‡u Ä‘áº¿n neurons tiáº¿p theo

VÃ­ dá»¥: Nháº­n diá»‡n máº·t ngÆ°á»i
Input â†’ Neurons nháº­n diá»‡n cáº¡nh â†’ Neurons nháº­n diá»‡n hÃ¬nh dáº¡ng â†’ Neurons nháº­n diá»‡n khuÃ´n máº·t â†’ Output
```

**Neural Network (mÃ´ phá»ng):**
```
Artificial Neuron:
- Nháº­n inputs (x1, x2, x3, ...)
- TÃ­nh tá»•ng cÃ³ trá»ng sá»‘: w1*x1 + w2*x2 + w3*x3 + b
- Ãp dá»¥ng activation function
- Gá»­i output Ä‘áº¿n layer tiáº¿p theo
```

### ğŸ“Š Kiáº¿n trÃºc cÆ¡ báº£n

```
Input Layer â†’ Hidden Layer(s) â†’ Output Layer

VÃ­ dá»¥ dá»± Ä‘oÃ¡n giÃ¡ FPT:

Input:          Hidden:         Output:
close â—‹         â—‹               
ma20  â—‹    â†’    â—‹     â†’         â—‹ price_tomorrow
rsi   â—‹         â—‹               
macd  â—‹         â—‹               
```

### ğŸ¯ Táº¡i sao gá»i lÃ  "Deep" Learning?

**Shallow (NÃ´ng):**
```
Input â†’ 1 Hidden Layer â†’ Output
â†’ Há»c Ä‘Æ°á»£c patterns Ä‘Æ¡n giáº£n
```

**Deep (SÃ¢u):**
```
Input â†’ Hidden 1 â†’ Hidden 2 â†’ Hidden 3 â†’ ... â†’ Output
â†’ Há»c Ä‘Æ°á»£c patterns phá»©c táº¡p, hierarchical
```

**VÃ­ dá»¥:**
- Layer 1: Há»c low-level features (giÃ¡ tÄƒng/giáº£m)
- Layer 2: Há»c mid-level features (xu hÆ°á»›ng ngáº¯n háº¡n)
- Layer 3: Há»c high-level features (regime, patterns phá»©c táº¡p)

---

## 2. PERCEPTRON - NEURON ÄÆ N GIáº¢N

### ğŸ”¬ Perceptron lÃ  gÃ¬?

> **Perceptron = Neural network Ä‘Æ¡n giáº£n nháº¥t (1 neuron)**

### ğŸ“ CÃ´ng thá»©c

```
y = f(w1*x1 + w2*x2 + ... + wn*xn + b)
    â†‘  â†‘                              â†‘
    f  weights                        bias
```

**Giáº£i thÃ­ch:**
- `x1, x2, ..., xn`: Inputs (features)
- `w1, w2, ..., wn`: Weights (trá»ng sá»‘)
- `b`: Bias (Ä‘á»™ lá»‡ch)
- `f`: Activation function
- `y`: Output

### ğŸ¯ VÃ­ dá»¥ cá»¥ thá»ƒ

**BÃ i toÃ¡n:** Dá»± Ä‘oÃ¡n FPT tÄƒng (1) hay giáº£m (0)

**Inputs:**
```
x1 = close = 100
x2 = ma20 = 95
x3 = rsi = 65
```

**Weights (ban Ä‘áº§u random):**
```
w1 = 0.5
w2 = -0.3
w3 = 0.8
b = -10
```

**TÃ­nh toÃ¡n:**
```
Step 1: Weighted sum
z = w1*x1 + w2*x2 + w3*x3 + b
  = 0.5*100 + (-0.3)*95 + 0.8*65 + (-10)
  = 50 - 28.5 + 52 - 10
  = 63.5

Step 2: Activation (Sigmoid)
y = sigmoid(z) = 1 / (1 + e^(-63.5))
  â‰ˆ 1.0

Step 3: Káº¿t luáº­n
y â‰ˆ 1 â†’ Dá»± Ä‘oÃ¡n TÄ‚NG
```

### ğŸ’¡ Ã nghÄ©a cá»§a Weights

**Weight lá»›n (|w| cao):**
- Feature quan trá»ng
- áº¢nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n output

**Weight nhá» (|w| tháº¥p):**
- Feature Ã­t quan trá»ng
- áº¢nh hÆ°á»Ÿng yáº¿u Ä‘áº¿n output

**Weight dÆ°Æ¡ng (+):**
- Feature tÄƒng â†’ Output tÄƒng

**Weight Ã¢m (-):**
- Feature tÄƒng â†’ Output giáº£m

**VÃ­ dá»¥:**
```
w_rsi = 0.8 (lá»›n, dÆ°Æ¡ng)
â†’ RSI tÄƒng â†’ XÃ¡c suáº¥t tÄƒng giÃ¡ cao

w_ma20 = -0.3 (nhá», Ã¢m)
â†’ MA20 tÄƒng â†’ XÃ¡c suáº¥t tÄƒng giÃ¡ giáº£m nháº¹
```

---

## 3. ACTIVATION FUNCTIONS

### ğŸ¤” Táº¡i sao cáº§n Activation Function?

**KhÃ´ng cÃ³ activation:**
```
y = w1*x1 + w2*x2 + b
â†’ Chá»‰ lÃ  linear function
â†’ KhÃ´ng há»c Ä‘Æ°á»£c patterns phá»©c táº¡p
```

**CÃ³ activation:**
```
y = f(w1*x1 + w2*x2 + b)
â†’ Non-linear function
â†’ Há»c Ä‘Æ°á»£c patterns phá»©c táº¡p
```

### ğŸ“Š CÃ¡c loáº¡i Activation Functions

#### **1. Sigmoid**

**CÃ´ng thá»©c:**
```
Ïƒ(x) = 1 / (1 + e^(-x))
```

**Äá»“ thá»‹:**
```
  1.0 â”¤        â•­â”€â”€â”€â”€
      â”‚      â•±
  0.5 â”¤    â•±
      â”‚  â•±
  0.0 â”¤â•±
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     -âˆ    0    +âˆ
```

**Äáº·c Ä‘iá»ƒm:**
- Output: (0, 1)
- DÃ¹ng cho: Binary classification, output layer
- Æ¯u: Dá»… interpret (xÃ¡c suáº¥t)
- NhÆ°á»£c: Vanishing gradient problem

#### **2. Tanh (Hyperbolic Tangent)**

**CÃ´ng thá»©c:**
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

**Äá»“ thá»‹:**
```
  1.0 â”¤       â•­â”€â”€â”€â”€
      â”‚     â•±
  0.0 â”¤   â•±
      â”‚ â•±
 -1.0 â”¤â•±
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     -âˆ    0    +âˆ
```

**Äáº·c Ä‘iá»ƒm:**
- Output: (-1, 1)
- DÃ¹ng cho: Hidden layers
- Æ¯u: Zero-centered (tá»‘t hÆ¡n Sigmoid)
- NhÆ°á»£c: Váº«n cÃ³ vanishing gradient

#### **3. ReLU (Rectified Linear Unit)**

**CÃ´ng thá»©c:**
```
ReLU(x) = max(0, x)
```

**Äá»“ thá»‹:**
```
      â”‚      â•±
      â”‚    â•±
      â”‚  â•±
  0.0 â”¤â•±
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     -âˆ    0    +âˆ
```

**Äáº·c Ä‘iá»ƒm:**
- Output: [0, +âˆ)
- DÃ¹ng cho: Hidden layers (phá»• biáº¿n nháº¥t)
- Æ¯u: Nhanh, khÃ´ng vanishing gradient
- NhÆ°á»£c: Dying ReLU problem

#### **4. Leaky ReLU**

**CÃ´ng thá»©c:**
```
LeakyReLU(x) = max(0.01*x, x)
```

**Äá»“ thá»‹:**
```
      â”‚      â•±
      â”‚    â•±
      â”‚  â•±
  0.0 â”¤â•±
      â•±
     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
    -âˆ    0    +âˆ
```

**Äáº·c Ä‘iá»ƒm:**
- Output: (-âˆ, +âˆ)
- DÃ¹ng cho: Hidden layers
- Æ¯u: Giáº£i quyáº¿t dying ReLU
- NhÆ°á»£c: ThÃªm hyperparameter (alpha)

### ğŸ’¡ Chá»n Activation nÃ o?

| Layer | Activation | LÃ½ do |
|-------|-----------|-------|
| **Hidden layers** | ReLU hoáº·c Leaky ReLU | Nhanh, hiá»‡u quáº£ |
| **Output (Regression)** | Linear (khÃ´ng activation) | Output khÃ´ng bá»‹ giá»›i háº¡n |
| **Output (Binary)** | Sigmoid | Output lÃ  xÃ¡c suáº¥t (0-1) |
| **Output (Multi-class)** | Softmax | Output lÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t |

---

## 4. FORWARD PROPAGATION

### ğŸ”„ Forward Propagation lÃ  gÃ¬?

> **Forward Propagation = TÃ­nh toÃ¡n tá»« input â†’ output**

### ğŸ“Š VÃ­ dá»¥ cá»¥ thá»ƒ

**Network:**
```
Input (2 features) â†’ Hidden (3 neurons) â†’ Output (1 neuron)

x1 â—‹     â—‹ h1
     â†’   â—‹ h2  â†’  â—‹ y
x2 â—‹     â—‹ h3
```

**Step-by-step:**

**Step 1: Input â†’ Hidden**
```python
# Inputs
x = [100, 95]  # [close, ma20]

# Weights (Input â†’ Hidden)
W1 = [[0.5, -0.3, 0.8],   # weights cho x1
      [0.2,  0.6, -0.4]]  # weights cho x2
b1 = [-10, 5, 2]          # biases

# TÃ­nh z1 (weighted sum)
z1 = x @ W1 + b1
   = [100, 95] @ [[0.5, -0.3, 0.8],
                  [0.2,  0.6, -0.4]] + [-10, 5, 2]
   = [50+19, -30+57, 80-38] + [-10, 5, 2]
   = [69, 27, 42] + [-10, 5, 2]
   = [59, 32, 44]

# Ãp dá»¥ng activation (ReLU)
h = ReLU(z1)
  = [59, 32, 44]  # Táº¥t cáº£ > 0 nÃªn giá»¯ nguyÃªn
```

**Step 2: Hidden â†’ Output**
```python
# Weights (Hidden â†’ Output)
W2 = [[0.7],
      [-0.5],
      [0.9]]
b2 = [3]

# TÃ­nh z2
z2 = h @ W2 + b2
   = [59, 32, 44] @ [[0.7], [-0.5], [0.9]] + [3]
   = [59*0.7 + 32*(-0.5) + 44*0.9] + [3]
   = [41.3 - 16 + 39.6] + [3]
   = [64.9] + [3]
   = [67.9]

# Ãp dá»¥ng activation (Linear cho regression)
y = z2
  = 67.9

â†’ Dá»± Ä‘oÃ¡n giÃ¡ ngÃ y mai: 67.9 (nghÃ¬n Ä‘á»“ng)
```

### ğŸ”§ Code Implementation

```python
import numpy as np

def relu(x):
    return np.maximum(0, x)

def forward_propagation(x, W1, b1, W2, b2):
    """
    Forward pass through network
    
    Args:
        x: Input (shape: [batch_size, input_dim])
        W1: Weights layer 1 (shape: [input_dim, hidden_dim])
        b1: Bias layer 1 (shape: [hidden_dim])
        W2: Weights layer 2 (shape: [hidden_dim, output_dim])
        b2: Bias layer 2 (shape: [output_dim])
    
    Returns:
        y: Output predictions
        cache: Intermediate values for backprop
    """
    # Layer 1: Input â†’ Hidden
    z1 = x @ W1 + b1
    h = relu(z1)
    
    # Layer 2: Hidden â†’ Output
    z2 = h @ W2 + b2
    y = z2  # Linear activation for regression
    
    # Cache for backprop
    cache = {'x': x, 'z1': z1, 'h': h, 'z2': z2}
    
    return y, cache
```

---

## 5. LOSS FUNCTIONS

### ğŸ¯ Loss Function lÃ  gÃ¬?

> **Loss Function = Äo lÆ°á»ng "sai" bao nhiÃªu**

**Má»¥c Ä‘Ã­ch:**
- Äo khoáº£ng cÃ¡ch giá»¯a prediction vÃ  actual
- CÃ ng nhá» cÃ ng tá»‘t
- DÃ¹ng Ä‘á»ƒ update weights

### ğŸ“Š CÃ¡c loáº¡i Loss Functions

#### **1. MSE (Mean Squared Error)**

**CÃ´ng thá»©c:**
```
MSE = (1/n) Ã— Î£(y_true - y_pred)Â²
```

**Khi nÃ o dÃ¹ng:**
- Regression problems
- Muá»‘n pháº¡t náº·ng outliers

**Code:**
```python
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)
```

#### **2. MAE (Mean Absolute Error)**

**CÃ´ng thá»©c:**
```
MAE = (1/n) Ã— Î£|y_true - y_pred|
```

**Khi nÃ o dÃ¹ng:**
- Regression problems
- KhÃ´ng muá»‘n pháº¡t náº·ng outliers

**Code:**
```python
def mae_loss(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))
```

#### **3. Binary Cross-Entropy**

**CÃ´ng thá»©c:**
```
BCE = -(1/n) Ã— Î£[y_true Ã— log(y_pred) + (1-y_true) Ã— log(1-y_pred)]
```

**Khi nÃ o dÃ¹ng:**
- Binary classification (tÄƒng/giáº£m)

**Code:**
```python
def binary_crossentropy(y_true, y_pred):
    epsilon = 1e-7  # TrÃ¡nh log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
```

### ğŸ’¡ Chá»n Loss nÃ o?

| Task | Loss Function |
|------|---------------|
| **Regression (giÃ¡ cá»• phiáº¿u)** | MSE hoáº·c MAE |
| **Binary Classification (tÄƒng/giáº£m)** | Binary Cross-Entropy |
| **Multi-class Classification** | Categorical Cross-Entropy |

---

## 6. BACKPROPAGATION

### ğŸ”„ Backpropagation lÃ  gÃ¬?

> **Backpropagation = TÃ­nh gradient cá»§a loss theo tá»«ng weight**

**Má»¥c Ä‘Ã­ch:**
- Biáº¿t Ä‘Æ°á»£c weight nÃ o cáº§n tÄƒng/giáº£m
- Biáº¿t Ä‘Æ°á»£c tÄƒng/giáº£m bao nhiÃªu
- DÃ¹ng Ä‘á»ƒ update weights

### ğŸ“ Chain Rule

**Ã tÆ°á»Ÿng:**
```
âˆ‚Loss/âˆ‚W1 = âˆ‚Loss/âˆ‚y Ã— âˆ‚y/âˆ‚z2 Ã— âˆ‚z2/âˆ‚h Ã— âˆ‚h/âˆ‚z1 Ã— âˆ‚z1/âˆ‚W1
            â†‘         â†‘         â†‘         â†‘         â†‘
         Output    Linear    ReLU     Linear    Input
```

### ğŸ¯ VÃ­ dá»¥ Ä‘Æ¡n giáº£n

**Network:**
```
x â†’ [W, b] â†’ z â†’ ReLU â†’ h â†’ [W2, b2] â†’ y
```

**Forward:**
```
x = 2
W = 0.5
b = 1
z = W*x + b = 0.5*2 + 1 = 2
h = ReLU(z) = 2
W2 = 0.8
b2 = 0.5
y = W2*h + b2 = 0.8*2 + 0.5 = 2.1

y_true = 3
Loss = (y_true - y)Â² = (3 - 2.1)Â² = 0.81
```

**Backward:**
```
âˆ‚Loss/âˆ‚y = -2(y_true - y) = -2(3 - 2.1) = -1.8

âˆ‚y/âˆ‚W2 = h = 2
âˆ‚Loss/âˆ‚W2 = âˆ‚Loss/âˆ‚y Ã— âˆ‚y/âˆ‚W2 = -1.8 Ã— 2 = -3.6

âˆ‚y/âˆ‚h = W2 = 0.8
âˆ‚h/âˆ‚z = 1 (vÃ¬ z > 0, ReLU derivative = 1)
âˆ‚z/âˆ‚W = x = 2
âˆ‚Loss/âˆ‚W = âˆ‚Loss/âˆ‚y Ã— âˆ‚y/âˆ‚h Ã— âˆ‚h/âˆ‚z Ã— âˆ‚z/âˆ‚W
         = -1.8 Ã— 0.8 Ã— 1 Ã— 2
         = -2.88
```

### ğŸ”§ Code Implementation

```python
def backward_propagation(cache, y_true, y_pred, W2):
    """
    Backward pass to compute gradients
    
    Args:
        cache: Intermediate values from forward pass
        y_true: True labels
        y_pred: Predictions
        W2: Weights layer 2
    
    Returns:
        grads: Dictionary of gradients
    """
    x = cache['x']
    z1 = cache['z1']
    h = cache['h']
    
    # Gradient of loss w.r.t. output
    dL_dy = 2 * (y_pred - y_true) / len(y_true)
    
    # Gradient w.r.t. W2, b2
    dL_dW2 = h.T @ dL_dy
    dL_db2 = np.sum(dL_dy, axis=0)
    
    # Gradient w.r.t. h
    dL_dh = dL_dy @ W2.T
    
    # Gradient w.r.t. z1 (ReLU derivative)
    dL_dz1 = dL_dh * (z1 > 0)
    
    # Gradient w.r.t. W1, b1
    dL_dW1 = x.T @ dL_dz1
    dL_db1 = np.sum(dL_dz1, axis=0)
    
    grads = {
        'dW1': dL_dW1,
        'db1': dL_db1,
        'dW2': dL_dW2,
        'db2': dL_db2
    }
    
    return grads
```

---

## 7. GRADIENT DESCENT

### ğŸ¯ Gradient Descent lÃ  gÃ¬?

> **Gradient Descent = Thuáº­t toÃ¡n update weights Ä‘á»ƒ giáº£m loss**

**Ã tÆ°á»Ÿng:**
```
1. TÃ­nh gradient (hÆ°á»›ng tÄƒng loss)
2. Äi ngÆ°á»£c hÆ°á»›ng gradient (Ä‘á»ƒ giáº£m loss)
3. Láº·p láº¡i cho Ä‘áº¿n khi loss khÃ´ng giáº£m ná»¯a
```

### ğŸ“Š Visualize

```
Loss
  â†‘
  â”‚     â•±â•²
  â”‚    â•±  â•²
  â”‚   â•±    â•²
  â”‚  â•±      â•²
  â”‚ â•±        â•²___
  â”‚â•±             â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight
  
  Start here â—
  â†“ (gradient descent)
  â†“
  â†“
  End here (minimum) â—
```

### ğŸ”§ CÃ´ng thá»©c

```
W_new = W_old - learning_rate Ã— gradient

VÃ­ dá»¥:
W_old = 0.5
gradient = -2.88
learning_rate = 0.01

W_new = 0.5 - 0.01 Ã— (-2.88)
      = 0.5 + 0.0288
      = 0.5288
```

### ğŸ’¡ Learning Rate

**Learning rate quÃ¡ lá»›n:**
```
Loss
  â†‘
  â”‚     â•±â•²
  â”‚    â•±  â•²
  â”‚   â—â”€â”€â”€â”€â—  â† Nháº£y qua láº¡i, khÃ´ng há»™i tá»¥
  â”‚  â•±      â•²
  â”‚ â•±        â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight
```

**Learning rate quÃ¡ nhá»:**
```
Loss
  â†‘
  â”‚     â•±â•²
  â”‚    â•±  â•²
  â”‚   â—â†’â†’â†’â†’  â† Cháº­m, máº¥t nhiá»u thá»i gian
  â”‚  â•±      â•²
  â”‚ â•±        â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight
```

**Learning rate vá»«a pháº£i:**
```
Loss
  â†‘
  â”‚     â•±â•²
  â”‚    â•±  â•²
  â”‚   â—â†’â†’â—  â† Nhanh vÃ  há»™i tá»¥
  â”‚  â•±      â•²
  â”‚ â•±        â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight
```

### ğŸ”§ Code Implementation

```python
def gradient_descent_step(W1, b1, W2, b2, grads, learning_rate):
    """
    Update weights using gradient descent
    
    Args:
        W1, b1, W2, b2: Current weights
        grads: Gradients from backprop
        learning_rate: Step size
    
    Returns:
        Updated weights
    """
    W1 = W1 - learning_rate * grads['dW1']
    b1 = b1 - learning_rate * grads['db1']
    W2 = W2 - learning_rate * grads['dW2']
    b2 = b2 - learning_rate * grads['db2']
    
    return W1, b1, W2, b2
```

### ğŸ“Š Training Loop

```python
def train(X_train, y_train, epochs=100, learning_rate=0.01):
    """
    Full training loop
    """
    # Initialize weights randomly
    W1 = np.random.randn(X_train.shape[1], 10) * 0.01
    b1 = np.zeros(10)
    W2 = np.random.randn(10, 1) * 0.01
    b2 = np.zeros(1)
    
    losses = []
    
    for epoch in range(epochs):
        # Forward pass
        y_pred, cache = forward_propagation(X_train, W1, b1, W2, b2)
        
        # Compute loss
        loss = mse_loss(y_train, y_pred)
        losses.append(loss)
        
        # Backward pass
        grads = backward_propagation(cache, y_train, y_pred, W2)
        
        # Update weights
        W1, b1, W2, b2 = gradient_descent_step(W1, b1, W2, b2, grads, learning_rate)
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    return W1, b1, W2, b2, losses
```

---

## 8. OVERFITTING & REGULARIZATION

### ğŸ¯ Overfitting trong Neural Networks

**Dáº¥u hiá»‡u:**
- Training loss ráº¥t tháº¥p
- Validation loss cao
- Model "nhá»›" training data thay vÃ¬ "há»c" pattern

### ğŸ’¡ CÃ¡c ká»¹ thuáº­t chá»‘ng Overfitting

#### **1. L2 Regularization (Weight Decay)**

**Ã tÆ°á»Ÿng:**
- Pháº¡t weights lá»›n
- Ã‰p weights nhá» láº¡i

**CÃ´ng thá»©c:**
```
Loss_total = Loss_data + Î» Ã— Î£(WÂ²)
                         â†‘
                    Regularization term
```

**Code:**
```python
def mse_loss_with_l2(y_true, y_pred, W1, W2, lambda_reg=0.01):
    data_loss = np.mean((y_true - y_pred) ** 2)
    reg_loss = lambda_reg * (np.sum(W1**2) + np.sum(W2**2))
    return data_loss + reg_loss
```

#### **2. Dropout**

**Ã tÆ°á»Ÿng:**
- Randomly "táº¯t" má»™t sá»‘ neurons trong training
- Ã‰p network há»c robust features

**Visualize:**
```
Training:
x â—‹     â—‹ h1 (active)
     â†’  âœ— h2 (dropped)  â†’  â—‹ y
x â—‹     â—‹ h3 (active)

Testing:
x â—‹     â—‹ h1
     â†’  â—‹ h2  â†’  â—‹ y
x â—‹     â—‹ h3
```

**Code:**
```python
def dropout(h, dropout_rate=0.5, training=True):
    if training:
        mask = np.random.rand(*h.shape) > dropout_rate
        return h * mask / (1 - dropout_rate)
    else:
        return h
```

#### **3. Early Stopping**

**Ã tÆ°á»Ÿng:**
- Dá»«ng training khi validation loss khÃ´ng giáº£m ná»¯a

**Code:**
```python
best_val_loss = float('inf')
patience = 10
patience_counter = 0

for epoch in range(epochs):
    # Training...
    val_loss = evaluate(X_val, y_val)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # Save best model
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping!")
            break
```

---

## 9. BÃ€I Táº¬P THá»°C HÃ€NH

### ğŸ¯ BÃ i táº­p 1: Implement Perceptron

**Äá» bÃ i:**
Implement perceptron tá»« Ä‘áº§u Ä‘á»ƒ dá»± Ä‘oÃ¡n FPT tÄƒng/giáº£m

**Gá»£i Ã½:**
```python
class Perceptron:
    def __init__(self, input_dim):
        self.W = np.random.randn(input_dim) * 0.01
        self.b = 0
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def forward(self, X):
        z = X @ self.W + self.b
        return self.sigmoid(z)
    
    def train(self, X, y, epochs=100, lr=0.01):
        # TODO: Implement training loop
        pass
```

**Kiá»ƒm tra:**
- [ ] Implement Ä‘Æ°á»£c forward pass
- [ ] TÃ­nh Ä‘Æ°á»£c loss
- [ ] Implement Ä‘Æ°á»£c backward pass
- [ ] Train Ä‘Æ°á»£c model

---

### ğŸ¯ BÃ i táº­p 2: Build 2-Layer Network

**Äá» bÃ i:**
Build neural network 2 layers Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ FPT

**Gá»£i Ã½:**
```python
class TwoLayerNet:
    def __init__(self, input_dim, hidden_dim, output_dim):
        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01
        self.b1 = np.zeros(hidden_dim)
        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01
        self.b2 = np.zeros(output_dim)
    
    def forward(self, X):
        # TODO: Implement
        pass
    
    def backward(self, X, y):
        # TODO: Implement
        pass
    
    def train(self, X, y, epochs=100, lr=0.01):
        # TODO: Implement
        pass
```

**Kiá»ƒm tra:**
- [ ] Implement Ä‘Æ°á»£c forward pass
- [ ] Implement Ä‘Æ°á»£c backward pass
- [ ] Train Ä‘Æ°á»£c model
- [ ] So sÃ¡nh vá»›i Linear Regression

---

## âœ… KIá»‚M TRA HIá»‚U BÃ€I

TrÆ°á»›c khi sang bÃ i tiáº¿p theo, hÃ£y Ä‘áº£m báº£o báº¡n:

- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c neural network lÃ  gÃ¬
- [ ] Hiá»ƒu Ä‘Æ°á»£c perceptron vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng
- [ ] Liá»‡t kÃª Ä‘Æ°á»£c cÃ¡c activation functions vÃ  khi nÃ o dÃ¹ng
- [ ] Hiá»ƒu Ä‘Æ°á»£c forward propagation
- [ ] Hiá»ƒu Ä‘Æ°á»£c backpropagation vÃ  chain rule
- [ ] Implement Ä‘Æ°á»£c gradient descent
- [ ] Hiá»ƒu Ä‘Æ°á»£c overfitting vÃ  cÃ¡ch kháº¯c phá»¥c
- [ ] LÃ m Ä‘Æ°á»£c 2 bÃ i táº­p thá»±c hÃ nh

**Náº¿u chÆ°a pass háº¿t checklist, Ä‘á»c láº¡i pháº§n tÆ°Æ¡ng á»©ng!**

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

**Videos:**
- 3Blue1Brown: Neural Networks series
- Andrew Ng: Deep Learning Specialization

**Books:**
- "Deep Learning" - Goodfellow, Bengio, Courville
- "Neural Networks and Deep Learning" - Michael Nielsen

---

## ğŸš€ BÆ¯á»šC TIáº¾P THEO

Sau khi hoÃ n thÃ nh bÃ i nÃ y, sang:
- `02_modeling/03_LSTM_GRU.md` - LSTM cho time series

**ChÃºc báº¡n há»c tá»‘t! ğŸ“**
