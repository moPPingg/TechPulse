<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Deep Learning CÆ¡ Báº£n</title>
  <style>
    :root {
      --bg: #fafafa;
      --text: #1a1a1a;
      --muted: #555;
      --code-bg: #f0f0f0;
      --border: #e0e0e0;
      --accent: #2563eb;
      --blockquote: #e8eef7;
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #1a1a1a;
        --text: #e4e4e4;
        --muted: #a0a0a0;
        --code-bg: #2d2d2d;
        --border: #333;
        --accent: #60a5fa;
        --blockquote: #1e293b;
      }
    }
    * { box-sizing: border-box; }
    body {
      font-family: "Segoe UI", "Source Sans 3", system-ui, sans-serif;
      line-height: 1.65;
      color: var(--text);
      background: var(--bg);
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem 1.5rem;
    }
    h1 { font-size: 1.85rem; margin-top: 0; border-bottom: 2px solid var(--border); padding-bottom: 0.5rem; }
    h2 { font-size: 1.4rem; margin-top: 2rem; color: var(--accent); }
    h3 { font-size: 1.15rem; margin-top: 1.5rem; }
    h1, h2, h3 { scroll-margin-top: 1rem; }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    pre, code {
      font-family: "Consolas", "Fira Code", monospace;
      background: var(--code-bg);
      border-radius: 6px;
    }
    code { padding: 0.2em 0.4em; font-size: 0.9em; }
    pre { padding: 1rem; overflow-x: auto; margin: 1rem 0; }
    pre code { padding: 0; background: none; }
    blockquote {
      border-left: 4px solid var(--accent);
      margin: 1rem 0;
      padding: 0.5rem 1rem;
      background: var(--blockquote);
      color: var(--muted);
    }
    hr { border: none; border-top: 1px solid var(--border); margin: 2rem 0; }
    ul, ol { padding-left: 1.5rem; }
    li { margin: 0.35rem 0; }
    .back-link { display: inline-block; margin-bottom: 1rem; color: var(--muted); font-size: 0.9rem; }
  </style>
</head>
<body>
  <a class="back-link" href=".">â† Trá»Ÿ láº¡i thÆ° má»¥c</a>
  <main>
<h1 id="deep-learning-co-ban">Deep Learning CÆ¡ Báº£n</h1>
<h2 id="neural-networks-tu-au-hieu-e-lam">Neural Networks tá»« Ä‘áº§u - Hiá»ƒu Ä‘á»ƒ lÃ m</h2>
<hr />
<h2 id="muc-luc">Má»¥c lá»¥c</h2>
<ol>
<li><a href="#1-neural-network-lÃ -gÃ¬">Neural Network lÃ  gÃ¬?</a></li>
<li><a href="#2-perceptron---neuron-Ä‘Æ¡n-giáº£n">Perceptron - Neuron Ä‘Æ¡n giáº£n</a></li>
<li><a href="#3-activation-functions">Activation Functions</a></li>
<li><a href="#4-tensor--shape-reasoning">Tensor &amp; Shape Reasoning</a></li>
<li><a href="#5-forward-propagation">Forward Propagation</a></li>
<li><a href="#6-loss-functions">Loss Functions</a></li>
<li><a href="#7-backpropagation">Backpropagation</a></li>
<li><a href="#8-gradient-descent">Gradient Descent</a></li>
<li><a href="#9-overfitting--regularization">Overfitting &amp; Regularization</a></li>
<li><a href="#10-training-pipeline-thá»±c-táº¿">Training Pipeline Thá»±c Táº¿</a></li>
<li><a href="#11-debug-deep-learning">Debug Deep Learning</a></li>
<li><a href="#12-optimization-algorithms">Optimization Algorithms</a></li>
<li><a href="#13-vanishing--exploding-gradients">Vanishing &amp; Exploding Gradients</a></li>
<li><a href="#14-weight-initialization">Weight Initialization</a></li>
<li><a href="#15-hyperparameters-learning-rate-batch-size-epochs">Hyperparameters: Learning Rate, Batch Size, Epochs</a></li>
<li><a href="#16-batch-normalization--dropout">Batch Normalization &amp; Dropout</a></li>
<li><a href="#17-Ä‘á»c-training-curves--cháº©n-Ä‘oÃ¡n-váº¥n-Ä‘á»">Äá»c Training Curves &amp; Cháº©n Ä‘oÃ¡n váº¥n Ä‘á»</a></li>
<li><a href="#18-pytorch-training-loop">PyTorch Training Loop</a></li>
<li><a href="#19-bÃ i-táº­p-thá»±c-hÃ nh">BÃ i táº­p thá»±c hÃ nh</a></li>
</ol>
<hr />
<h2 id="1-neural-network-la-gi">1. NEURAL NETWORK LÃ€ GÃŒ?</h2>
<h3 id="lay-cam-hung-tu-nao-bo">ğŸ§  Láº¥y cáº£m há»©ng tá»« nÃ£o bá»™</h3>
<p><strong>NÃ£o ngÆ°á»i:</strong></p>
<pre><code>Neuron (táº¿ bÃ o tháº§n kinh):
- Nháº­n tÃ­n hiá»‡u tá»« nhiá»u neurons khÃ¡c
- Xá»­ lÃ½ tÃ­n hiá»‡u
- Gá»­i tÃ­n hiá»‡u Ä‘áº¿n neurons tiáº¿p theo

VÃ­ dá»¥: Nháº­n diá»‡n máº·t ngÆ°á»i
Input â†’ Neurons nháº­n diá»‡n cáº¡nh â†’ Neurons nháº­n diá»‡n hÃ¬nh dáº¡ng â†’ Neurons nháº­n diá»‡n khuÃ´n máº·t â†’ Output
</code></pre>
<p><strong>Neural Network (mÃ´ phá»ng):</strong></p>
<pre><code>Artificial Neuron:
- Nháº­n inputs (x1, x2, x3, ...)
- TÃ­nh tá»•ng cÃ³ trá»ng sá»‘: w1*x1 + w2*x2 + w3*x3 + b
- Ãp dá»¥ng activation function
- Gá»­i output Ä‘áº¿n layer tiáº¿p theo
</code></pre>
<h3 id="kien-truc-co-ban">ğŸ“Š Kiáº¿n trÃºc cÆ¡ báº£n</h3>
<pre><code>Input Layer â†’ Hidden Layer(s) â†’ Output Layer

VÃ­ dá»¥ dá»± Ä‘oÃ¡n giÃ¡ FPT:

Input:          Hidden:         Output:
close â—‹         â—‹               
ma20  â—‹    â†’    â—‹     â†’         â—‹ price_tomorrow
rsi   â—‹         â—‹               
macd  â—‹         â—‹               
</code></pre>
<h3 id="tai-sao-goi-la-deep-learning">ğŸ¯ Táº¡i sao gá»i lÃ  "Deep" Learning?</h3>
<p><strong>Shallow (NÃ´ng):</strong></p>
<pre><code>Input â†’ 1 Hidden Layer â†’ Output
â†’ Há»c Ä‘Æ°á»£c patterns Ä‘Æ¡n giáº£n
</code></pre>
<p><strong>Deep (SÃ¢u):</strong></p>
<pre><code>Input â†’ Hidden 1 â†’ Hidden 2 â†’ Hidden 3 â†’ ... â†’ Output
â†’ Há»c Ä‘Æ°á»£c patterns phá»©c táº¡p, hierarchical
</code></pre>
<p><strong>VÃ­ dá»¥:</strong>
- Layer 1: Há»c low-level features (giÃ¡ tÄƒng/giáº£m)
- Layer 2: Há»c mid-level features (xu hÆ°á»›ng ngáº¯n háº¡n)
- Layer 3: Há»c high-level features (regime, patterns phá»©c táº¡p)</p>
<hr />
<h2 id="2-perceptron-neuron-on-gian">2. PERCEPTRON - NEURON ÄÆ N GIáº¢N</h2>
<h3 id="perceptron-la-gi">ğŸ”¬ Perceptron lÃ  gÃ¬?</h3>
<blockquote>
<p><strong>Perceptron = Neural network Ä‘Æ¡n giáº£n nháº¥t (1 neuron)</strong></p>
</blockquote>
<h3 id="cong-thuc">ğŸ“ CÃ´ng thá»©c</h3>
<pre><code>y = f(w1*x1 + w2*x2 + ... + wn*xn + b)
    â†‘  â†‘                              â†‘
    f  weights                        bias
</code></pre>
<p><strong>Giáº£i thÃ­ch:</strong>
- <code>x1, x2, ..., xn</code>: Inputs (features)
- <code>w1, w2, ..., wn</code>: Weights (trá»ng sá»‘)
- <code>b</code>: Bias (Ä‘á»™ lá»‡ch)
- <code>f</code>: Activation function
- <code>y</code>: Output</p>
<h3 id="vi-du-cu-the">ğŸ¯ VÃ­ dá»¥ cá»¥ thá»ƒ</h3>
<p><strong>BÃ i toÃ¡n:</strong> Dá»± Ä‘oÃ¡n FPT tÄƒng (1) hay giáº£m (0)</p>
<p><strong>Inputs:</strong></p>
<pre><code>x1 = close = 100
x2 = ma20 = 95
x3 = rsi = 65
</code></pre>
<p><strong>Weights (ban Ä‘áº§u random):</strong></p>
<pre><code>w1 = 0.5
w2 = -0.3
w3 = 0.8
b = -10
</code></pre>
<p><strong>TÃ­nh toÃ¡n:</strong></p>
<pre><code>Step 1: Weighted sum
z = w1*x1 + w2*x2 + w3*x3 + b
  = 0.5*100 + (-0.3)*95 + 0.8*65 + (-10)
  = 50 - 28.5 + 52 - 10
  = 63.5

Step 2: Activation (Sigmoid)
y = sigmoid(z) = 1 / (1 + e^(-63.5))
  â‰ˆ 1.0

Step 3: Káº¿t luáº­n
y â‰ˆ 1 â†’ Dá»± Ä‘oÃ¡n TÄ‚NG
</code></pre>
<h3 id="y-nghia-cua-weights">ğŸ’¡ Ã nghÄ©a cá»§a Weights</h3>
<p><strong>Weight lá»›n (|w| cao):</strong>
- Feature quan trá»ng
- áº¢nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n output</p>
<p><strong>Weight nhá» (|w| tháº¥p):</strong>
- Feature Ã­t quan trá»ng
- áº¢nh hÆ°á»Ÿng yáº¿u Ä‘áº¿n output</p>
<p><strong>Weight dÆ°Æ¡ng (+):</strong>
- Feature tÄƒng â†’ Output tÄƒng</p>
<p><strong>Weight Ã¢m (-):</strong>
- Feature tÄƒng â†’ Output giáº£m</p>
<p><strong>VÃ­ dá»¥:</strong></p>
<pre><code>w_rsi = 0.8 (lá»›n, dÆ°Æ¡ng)
â†’ RSI tÄƒng â†’ XÃ¡c suáº¥t tÄƒng giÃ¡ cao

w_ma20 = -0.3 (nhá», Ã¢m)
â†’ MA20 tÄƒng â†’ XÃ¡c suáº¥t tÄƒng giÃ¡ giáº£m nháº¹
</code></pre>
<hr />
<h2 id="3-activation-functions">3. ACTIVATION FUNCTIONS</h2>
<h3 id="tai-sao-can-activation-function">ğŸ¤” Táº¡i sao cáº§n Activation Function?</h3>
<p><strong>KhÃ´ng cÃ³ activation:</strong></p>
<pre><code>y = w1*x1 + w2*x2 + b
â†’ Chá»‰ lÃ  linear function
â†’ KhÃ´ng há»c Ä‘Æ°á»£c patterns phá»©c táº¡p
</code></pre>
<p><strong>CÃ³ activation:</strong></p>
<pre><code>y = f(w1*x1 + w2*x2 + b)
â†’ Non-linear function
â†’ Há»c Ä‘Æ°á»£c patterns phá»©c táº¡p
</code></pre>
<h3 id="cac-loai-activation-functions">ğŸ“Š CÃ¡c loáº¡i Activation Functions</h3>
<h4 id="1-sigmoid"><strong>1. Sigmoid</strong></h4>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>Ïƒ(x) = 1 / (1 + e^(-x))
</code></pre>
<p><strong>Äá»“ thá»‹:</strong></p>
<pre><code>  1.0 â”¤        â•­â”€â”€â”€â”€
      â”‚      â•±
  0.5 â”¤    â•±
      â”‚  â•±
  0.0 â”¤â•±
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     -âˆ    0    +âˆ
</code></pre>
<p><strong>Äáº·c Ä‘iá»ƒm:</strong>
- Output: (0, 1)
- DÃ¹ng cho: Binary classification, output layer
- Æ¯u: Dá»… interpret (xÃ¡c suáº¥t)
- NhÆ°á»£c: Vanishing gradient problem</p>
<h4 id="2-tanh-hyperbolic-tangent"><strong>2. Tanh (Hyperbolic Tangent)</strong></h4>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
</code></pre>
<p><strong>Äá»“ thá»‹:</strong></p>
<pre><code>  1.0 â”¤       â•­â”€â”€â”€â”€
      â”‚     â•±
  0.0 â”¤   â•±
      â”‚ â•±
 -1.0 â”¤â•±
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     -âˆ    0    +âˆ
</code></pre>
<p><strong>Äáº·c Ä‘iá»ƒm:</strong>
- Output: (-1, 1)
- DÃ¹ng cho: Hidden layers
- Æ¯u: Zero-centered (tá»‘t hÆ¡n Sigmoid)
- NhÆ°á»£c: Váº«n cÃ³ vanishing gradient</p>
<h4 id="3-relu-rectified-linear-unit"><strong>3. ReLU (Rectified Linear Unit)</strong></h4>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>ReLU(x) = max(0, x)
</code></pre>
<p><strong>Äá»“ thá»‹:</strong></p>
<pre><code>      â”‚      â•±
      â”‚    â•±
      â”‚  â•±
  0.0 â”¤â•±
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     -âˆ    0    +âˆ
</code></pre>
<p><strong>Äáº·c Ä‘iá»ƒm:</strong>
- Output: [0, +âˆ)
- DÃ¹ng cho: Hidden layers (phá»• biáº¿n nháº¥t)
- Æ¯u: Nhanh, khÃ´ng vanishing gradient
- NhÆ°á»£c: Dying ReLU problem</p>
<h4 id="4-leaky-relu"><strong>4. Leaky ReLU</strong></h4>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>LeakyReLU(x) = max(0.01*x, x)
</code></pre>
<p><strong>Äá»“ thá»‹:</strong></p>
<pre><code>      â”‚      â•±
      â”‚    â•±
      â”‚  â•±
  0.0 â”¤â•±
      â•±
     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
    -âˆ    0    +âˆ
</code></pre>
<p><strong>Äáº·c Ä‘iá»ƒm:</strong>
- Output: (-âˆ, +âˆ)
- DÃ¹ng cho: Hidden layers
- Æ¯u: Giáº£i quyáº¿t dying ReLU
- NhÆ°á»£c: ThÃªm hyperparameter (alpha)</p>
<h3 id="chon-activation-nao">ğŸ’¡ Chá»n Activation nÃ o?</h3>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Activation</th>
<th>LÃ½ do</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hidden layers</strong></td>
<td>ReLU hoáº·c Leaky ReLU</td>
<td>Nhanh, hiá»‡u quáº£</td>
</tr>
<tr>
<td><strong>Output (Regression)</strong></td>
<td>Linear (khÃ´ng activation)</td>
<td>Output khÃ´ng bá»‹ giá»›i háº¡n</td>
</tr>
<tr>
<td><strong>Output (Binary)</strong></td>
<td>Sigmoid</td>
<td>Output lÃ  xÃ¡c suáº¥t (0-1)</td>
</tr>
<tr>
<td><strong>Output (Multi-class)</strong></td>
<td>Softmax</td>
<td>Output lÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="4-tensor-shape-reasoning">4. TENSOR &amp; SHAPE REASONING</h2>
<h3 id="tai-sao-shape-quan-trong">ğŸ¯ Táº¡i sao Shape quan trá»ng?</h3>
<blockquote>
<p><strong>90% bugs trong Deep Learning = Shape mismatch!</strong></p>
</blockquote>
<p><strong>Thá»±c táº¿:</strong></p>
<pre><code class="language-python"># Error phá»• biáº¿n nháº¥t:
RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x10 and 20x5)
ValueError: operands could not be broadcast together with shapes (32,10) (20,)

â†’ ÄÃ¢y lÃ  NIGHTMARE #1 cá»§a sinh viÃªn ML!
</code></pre>
<h3 id="41-tensor-la-gi">ğŸ“Š 4.1. Tensor lÃ  gÃ¬?</h3>
<p><strong>Tensor = Multi-dimensional array</strong></p>
<pre><code>Scalar (0D):  5
             â†‘
           Chá»‰ 1 sá»‘

Vector (1D):  [1, 2, 3, 4, 5]
              â†‘
            Shape: (5,)

Matrix (2D):  [[1, 2, 3],
               [4, 5, 6]]
              â†‘
            Shape: (2, 3)
            2 rows, 3 columns

Tensor (3D):  [[[1, 2], [3, 4]],
               [[5, 6], [7, 8]]]
              â†‘
            Shape: (2, 2, 2)
</code></pre>
<p><strong>Code:</strong></p>
<pre><code class="language-python">import numpy as np

# Scalar
a = 5
print(f&quot;Shape: {np.array(a).shape}&quot;)  # ()

# Vector
b = np.array([1, 2, 3, 4, 5])
print(f&quot;Shape: {b.shape}&quot;)  # (5,)

# Matrix
c = np.array([[1, 2, 3], [4, 5, 6]])
print(f&quot;Shape: {c.shape}&quot;)  # (2, 3)

# 3D Tensor
d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print(f&quot;Shape: {d.shape}&quot;)  # (2, 2, 2)
</code></pre>
<h3 id="42-batch-dimension">ğŸ“Š 4.2. Batch Dimension</h3>
<p><strong>Táº¡i sao cáº§n Batch?</strong></p>
<pre><code>Thay vÃ¬ train tá»«ng sample:
Sample 1: [100, 95, 65] â†’ Forward â†’ Loss â†’ Backward â†’ Update weights
Sample 2: [102, 97, 68] â†’ Forward â†’ Loss â†’ Backward â†’ Update weights
...
â†’ CHáº¬M! 1000 samples = 1000 láº§n update

Train theo batch:
Batch: [[100, 95, 65],
        [102, 97, 68],
        [105, 99, 70],
        ...(32 samples)]
â†’ Forward 32 samples cÃ¹ng lÃºc â†’ Average loss â†’ Backward â†’ Update
â†’ NHANH! 1000 samples / 32 = 31 láº§n update
</code></pre>
<p><strong>Batch dimension = Dimension Ä‘áº§u tiÃªn</strong></p>
<pre><code>Single sample:
X = [100, 95, 65]
Shape: (3,)
     â†‘
   3 features

Batch of 32 samples:
X = [[100, 95, 65],
     [102, 97, 68],
     ...(32 samples)]
Shape: (32, 3)
       â†‘   â†‘
    batch features
</code></pre>
<p><strong>Code:</strong></p>
<pre><code class="language-python"># Single sample
x_single = np.array([100, 95, 65])
print(f&quot;Single: {x_single.shape}&quot;)  # (3,)

# Batch of 32
x_batch = np.random.randn(32, 3)
print(f&quot;Batch: {x_batch.shape}&quot;)  # (32, 3)

# Chuyá»ƒn single â†’ batch (thÃªm batch dimension)
x_single_batched = x_single[np.newaxis, :]  # hoáº·c x_single.reshape(1, -1)
print(f&quot;Single as batch: {x_single_batched.shape}&quot;)  # (1, 3)
</code></pre>
<h3 id="43-shape-flow-qua-layers">ğŸ“Š 4.3. Shape Flow qua Layers</h3>
<p><strong>VÃ­ dá»¥: 2-Layer Network</strong></p>
<pre><code>Input: X (batch_size=32, features=3)
       â†“
W1 (3, 10), b1 (10,)
       â†“
Z1 = X @ W1 + b1
Shape: (32, 3) @ (3, 10) + (10,) = (32, 10)
                                     â†‘
                            Broadcasting b1
       â†“
H = ReLU(Z1)
Shape: (32, 10)  # Shape khÃ´ng Ä‘á»•i
       â†“
W2 (10, 1), b2 (1,)
       â†“
Z2 = H @ W2 + b2
Shape: (32, 10) @ (10, 1) + (1,) = (32, 1)
       â†“
Y (32, 1)  # Predictions cho 32 samples
</code></pre>
<p><strong>Quy táº¯c Matrix Multiplication:</strong></p>
<pre><code>(A, B) @ (B, C) = (A, C)
 â†‘        â†‘
Pháº£i khá»›p!

VÃ­ dá»¥:
(32, 3) @ (3, 10) = (32, 10) âœ…
(32, 3) @ (10, 3) = ERROR! âŒ
     â†‘       â†‘
  3 â‰  10 â†’ KhÃ´ng khá»›p!
</code></pre>
<p><strong>Code kiá»ƒm tra shape:</strong></p>
<pre><code class="language-python">def forward_with_shape_check(X, W1, b1, W2, b2):
    print(f&quot;Input X: {X.shape}&quot;)  # (32, 3)

    # Layer 1
    print(f&quot;W1: {W1.shape}, b1: {b1.shape}&quot;)  # (3, 10), (10,)

    Z1 = X @ W1 + b1
    print(f&quot;Z1 = X @ W1 + b1: {Z1.shape}&quot;)  # (32, 10)

    H = np.maximum(0, Z1)  # ReLU
    print(f&quot;H = ReLU(Z1): {H.shape}&quot;)  # (32, 10)

    # Layer 2
    print(f&quot;W2: {W2.shape}, b2: {b2.shape}&quot;)  # (10, 1), (1,)

    Z2 = H @ W2 + b2
    print(f&quot;Z2 = H @ W2 + b2: {Z2.shape}&quot;)  # (32, 1)

    return Z2

# Test
X = np.random.randn(32, 3)
W1 = np.random.randn(3, 10) * 0.01
b1 = np.zeros(10)
W2 = np.random.randn(10, 1) * 0.01
b2 = np.zeros(1)

Y = forward_with_shape_check(X, W1, b1, W2, b2)
</code></pre>
<h3 id="44-broadcasting">ğŸ“Š 4.4. Broadcasting</h3>
<p><strong>Broadcasting = Tá»± Ä‘á»™ng expand shape Ä‘á»ƒ phÃ©p toÃ¡n hoáº¡t Ä‘á»™ng</strong></p>
<p><strong>Quy táº¯c Broadcasting:</strong></p>
<pre><code>Numpy tá»± Ä‘á»™ng expand dimensions khi:
1. Má»™t trong 2 arrays cÃ³ dimension = 1
2. Má»™t trong 2 arrays thiáº¿u dimension

VÃ­ dá»¥:
(32, 10) + (10,)
         â†“ Broadcasting
(32, 10) + (1, 10)  # ThÃªm dimension
         â†“
(32, 10) + (32, 10)  # Repeat 32 láº§n
         â†“
(32, 10)  # Káº¿t quáº£
</code></pre>
<p><strong>VÃ­ dá»¥ cá»¥ thá»ƒ:</strong></p>
<p><strong>Case 1: Vector + Scalar</strong></p>
<pre><code class="language-python">a = np.array([1, 2, 3])  # (3,)
b = 10                    # scalar

result = a + b  # [11, 12, 13]
# Broadcasting: 10 â†’ [10, 10, 10]
</code></pre>
<p><strong>Case 2: Matrix + Vector</strong></p>
<pre><code class="language-python">A = np.array([[1, 2, 3],
              [4, 5, 6]])  # (2, 3)
b = np.array([10, 20, 30])  # (3,)

result = A + b
# Broadcasting:
# b (3,) â†’ (1, 3) â†’ (2, 3)
# [[11, 22, 33],
#  [14, 25, 36]]
</code></pre>
<p><strong>Case 3: Batch + Bias</strong></p>
<pre><code class="language-python">Z = np.random.randn(32, 10)  # (32, 10)
b = np.zeros(10)              # (10,)

result = Z + b  # (32, 10)
# Broadcasting:
# b (10,) â†’ (1, 10) â†’ (32, 10)
</code></pre>
<p><strong>Case 4: KhÃ´ng broadcast Ä‘Æ°á»£c</strong></p>
<pre><code class="language-python">A = np.random.randn(32, 10)  # (32, 10)
B = np.random.randn(20)       # (20,)

# A + B â†’ ERROR!
# (32, 10) vs (20,)
#       10 â‰  20 â†’ KhÃ´ng khá»›p!
</code></pre>
<p><strong>Visualize Broadcasting:</strong></p>
<pre><code>Original:
Z (32, 10)  +  b (10,)

Step 1: ThÃªm dimension
Z (32, 10)  +  b (1, 10)

Step 2: Repeat
Z (32, 10)  +  b (32, 10)
                 â†‘
          Repeat 32 láº§n

Result: (32, 10)
</code></pre>
<h3 id="45-debug-shape-mismatch">ğŸ’¡ 4.5. Debug Shape Mismatch</h3>
<p><strong>Common Errors vÃ  CÃ¡ch Fix:</strong></p>
<p><strong>Error 1: Dimension mismatch</strong></p>
<pre><code class="language-python"># âŒ ERROR
X = np.random.randn(32, 3)  # (32, 3)
W = np.random.randn(10, 5)  # (10, 5)
Z = X @ W  # ERROR: 3 â‰  10

# âœ… FIX
W = np.random.randn(3, 5)   # (3, 5)
Z = X @ W  # (32, 3) @ (3, 5) = (32, 5) âœ…
</code></pre>
<p><strong>Error 2: Forgotten batch dimension</strong></p>
<pre><code class="language-python"># âŒ ERROR
x = np.array([100, 95, 65])  # (3,) - single sample
W = np.random.randn(3, 10)   # (3, 10)
z = x @ W  # OK: (3,) @ (3, 10) = (10,)

# NhÆ°ng khi train batch:
X = np.random.randn(32, 3)  # (32, 3)
# Náº¿u code khÃ´ng handle batch â†’ ERROR!

# âœ… FIX: Always design cho batch
def forward(X):  # X: (batch, features)
    # Code sáº½ work cho cáº£ single (batch=1) vÃ  batch
    pass
</code></pre>
<p><strong>Error 3: Squeeze/Unsqueeze</strong></p>
<pre><code class="language-python"># âŒ ERROR
Y = np.random.randn(32, 1, 1)  # (32, 1, 1) - thá»«a dimensions
loss = mse(Y, Y_true)  # CÃ³ thá»ƒ bá»‹ lá»—i

# âœ… FIX
Y = Y.squeeze()  # (32,) hoáº·c (32, 1)
</code></pre>
<p><strong>Trick Debug Shape:</strong></p>
<pre><code class="language-python">def debug_shapes(X, W1, b1, W2, b2):
    &quot;&quot;&quot;In ra táº¥t cáº£ shapes Ä‘á»ƒ debug&quot;&quot;&quot;
    print(&quot;=&quot;*50)
    print(f&quot;X: {X.shape}&quot;)
    print(f&quot;W1: {W1.shape}, b1: {b1.shape}&quot;)

    Z1 = X @ W1 + b1
    print(f&quot;Z1: {Z1.shape}&quot;)

    H = np.maximum(0, Z1)
    print(f&quot;H: {H.shape}&quot;)

    print(f&quot;W2: {W2.shape}, b2: {b2.shape}&quot;)

    Z2 = H @ W2 + b2
    print(f&quot;Z2: {Z2.shape}&quot;)
    print(&quot;=&quot;*50)

    return Z2

# LUÃ”N gá»i hÃ m nÃ y khi debug!
</code></pre>
<h3 id="shape-reasoning-checklist">ğŸ¯ Shape Reasoning Checklist</h3>
<p>Khi viáº¿t code Deep Learning, LUÃ”N tá»± há»i:</p>
<ul>
<li>[ ] Input shape lÃ  gÃ¬? (batch_size, features)</li>
<li>[ ] Weight shape pháº£i lÃ  gÃ¬ Ä‘á»ƒ khá»›p?</li>
<li>[ ] Output shape sáº½ lÃ  gÃ¬?</li>
<li>[ ] Bias cÃ³ broadcast Ä‘Ãºng khÃ´ng?</li>
<li>[ ] Activation function cÃ³ thay Ä‘á»•i shape khÃ´ng?</li>
<li>[ ] Loss function expect shape gÃ¬?</li>
</ul>
<hr />
<h2 id="5-forward-propagation">5. FORWARD PROPAGATION</h2>
<h3 id="forward-propagation-la-gi">ğŸ”„ Forward Propagation lÃ  gÃ¬?</h3>
<blockquote>
<p><strong>Forward Propagation = TÃ­nh toÃ¡n tá»« input â†’ output</strong></p>
</blockquote>
<h3 id="vi-du-cu-the_1">ğŸ“Š VÃ­ dá»¥ cá»¥ thá»ƒ</h3>
<p><strong>Network:</strong></p>
<pre><code>Input (2 features) â†’ Hidden (3 neurons) â†’ Output (1 neuron)

x1 â—‹     â—‹ h1
     â†’   â—‹ h2  â†’  â—‹ y
x2 â—‹     â—‹ h3
</code></pre>
<p><strong>Step-by-step:</strong></p>
<p><strong>Step 1: Input â†’ Hidden</strong></p>
<pre><code class="language-python"># Inputs
x = [100, 95]  # [close, ma20]

# Weights (Input â†’ Hidden)
W1 = [[0.5, -0.3, 0.8],   # weights cho x1
      [0.2,  0.6, -0.4]]  # weights cho x2
b1 = [-10, 5, 2]          # biases

# TÃ­nh z1 (weighted sum)
z1 = x @ W1 + b1
   = [100, 95] @ [[0.5, -0.3, 0.8],
                  [0.2,  0.6, -0.4]] + [-10, 5, 2]
   = [50+19, -30+57, 80-38] + [-10, 5, 2]
   = [69, 27, 42] + [-10, 5, 2]
   = [59, 32, 44]

# Ãp dá»¥ng activation (ReLU)
h = ReLU(z1)
  = [59, 32, 44]  # Táº¥t cáº£ &gt; 0 nÃªn giá»¯ nguyÃªn
</code></pre>
<p><strong>Step 2: Hidden â†’ Output</strong></p>
<pre><code class="language-python"># Weights (Hidden â†’ Output)
W2 = [[0.7],
      [-0.5],
      [0.9]]
b2 = [3]

# TÃ­nh z2
z2 = h @ W2 + b2
   = [59, 32, 44] @ [[0.7], [-0.5], [0.9]] + [3]
   = [59*0.7 + 32*(-0.5) + 44*0.9] + [3]
   = [41.3 - 16 + 39.6] + [3]
   = [64.9] + [3]
   = [67.9]

# Ãp dá»¥ng activation (Linear cho regression)
y = z2
  = 67.9

â†’ Dá»± Ä‘oÃ¡n giÃ¡ ngÃ y mai: 67.9 (nghÃ¬n Ä‘á»“ng)
</code></pre>
<h3 id="code-implementation">ğŸ”§ Code Implementation</h3>
<pre><code class="language-python">import numpy as np

def relu(x):
    return np.maximum(0, x)

def forward_propagation(x, W1, b1, W2, b2):
    &quot;&quot;&quot;
    Forward pass through network

    Args:
        x: Input (shape: [batch_size, input_dim])
        W1: Weights layer 1 (shape: [input_dim, hidden_dim])
        b1: Bias layer 1 (shape: [hidden_dim])
        W2: Weights layer 2 (shape: [hidden_dim, output_dim])
        b2: Bias layer 2 (shape: [output_dim])

    Returns:
        y: Output predictions
        cache: Intermediate values for backprop
    &quot;&quot;&quot;
    # Layer 1: Input â†’ Hidden
    z1 = x @ W1 + b1
    h = relu(z1)

    # Layer 2: Hidden â†’ Output
    z2 = h @ W2 + b2
    y = z2  # Linear activation for regression

    # Cache for backprop
    cache = {'x': x, 'z1': z1, 'h': h, 'z2': z2}

    return y, cache
</code></pre>
<hr />
<h2 id="6-loss-functions">6. LOSS FUNCTIONS</h2>
<h3 id="loss-function-la-gi">ğŸ¯ Loss Function lÃ  gÃ¬?</h3>
<blockquote>
<p><strong>Loss Function = Äo lÆ°á»ng "sai" bao nhiÃªu</strong></p>
</blockquote>
<p><strong>Má»¥c Ä‘Ã­ch:</strong>
- Äo khoáº£ng cÃ¡ch giá»¯a prediction vÃ  actual
- CÃ ng nhá» cÃ ng tá»‘t
- DÃ¹ng Ä‘á»ƒ update weights</p>
<h3 id="cac-loai-loss-functions">ğŸ“Š CÃ¡c loáº¡i Loss Functions</h3>
<h4 id="1-mse-mean-squared-error"><strong>1. MSE (Mean Squared Error)</strong></h4>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>MSE = (1/n) Ã— Î£(y_true - y_pred)Â²
</code></pre>
<p><strong>Khi nÃ o dÃ¹ng:</strong>
- Regression problems
- Muá»‘n pháº¡t náº·ng outliers</p>
<p><strong>Code:</strong></p>
<pre><code class="language-python">def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)
</code></pre>
<h4 id="2-mae-mean-absolute-error"><strong>2. MAE (Mean Absolute Error)</strong></h4>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>MAE = (1/n) Ã— Î£|y_true - y_pred|
</code></pre>
<p><strong>Khi nÃ o dÃ¹ng:</strong>
- Regression problems
- KhÃ´ng muá»‘n pháº¡t náº·ng outliers</p>
<p><strong>Code:</strong></p>
<pre><code class="language-python">def mae_loss(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))
</code></pre>
<h4 id="3-binary-cross-entropy"><strong>3. Binary Cross-Entropy</strong></h4>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>BCE = -(1/n) Ã— Î£[y_true Ã— log(y_pred) + (1-y_true) Ã— log(1-y_pred)]
</code></pre>
<p><strong>Khi nÃ o dÃ¹ng:</strong>
- Binary classification (tÄƒng/giáº£m)</p>
<p><strong>Code:</strong></p>
<pre><code class="language-python">def binary_crossentropy(y_true, y_pred):
    epsilon = 1e-7  # TrÃ¡nh log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
</code></pre>
<h3 id="chon-loss-nao">ğŸ’¡ Chá»n Loss nÃ o?</h3>
<table>
<thead>
<tr>
<th>Task</th>
<th>Loss Function</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Regression (giÃ¡ cá»• phiáº¿u)</strong></td>
<td>MSE hoáº·c MAE</td>
</tr>
<tr>
<td><strong>Binary Classification (tÄƒng/giáº£m)</strong></td>
<td>Binary Cross-Entropy</td>
</tr>
<tr>
<td><strong>Multi-class Classification</strong></td>
<td>Categorical Cross-Entropy</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="7-backpropagation">7. BACKPROPAGATION</h2>
<h3 id="backpropagation-la-gi">ğŸ”„ Backpropagation lÃ  gÃ¬?</h3>
<blockquote>
<p><strong>Backpropagation = TÃ­nh gradient cá»§a loss theo tá»«ng weight</strong></p>
</blockquote>
<p><strong>Má»¥c Ä‘Ã­ch:</strong>
- Biáº¿t Ä‘Æ°á»£c weight nÃ o cáº§n tÄƒng/giáº£m
- Biáº¿t Ä‘Æ°á»£c tÄƒng/giáº£m bao nhiÃªu
- DÃ¹ng Ä‘á»ƒ update weights</p>
<h3 id="chain-rule">ğŸ“ Chain Rule</h3>
<p><strong>Ã tÆ°á»Ÿng:</strong></p>
<pre><code>âˆ‚Loss/âˆ‚W1 = âˆ‚Loss/âˆ‚y Ã— âˆ‚y/âˆ‚z2 Ã— âˆ‚z2/âˆ‚h Ã— âˆ‚h/âˆ‚z1 Ã— âˆ‚z1/âˆ‚W1
            â†‘         â†‘         â†‘         â†‘         â†‘
         Output    Linear    ReLU     Linear    Input
</code></pre>
<h3 id="vi-du-on-gian">ğŸ¯ VÃ­ dá»¥ Ä‘Æ¡n giáº£n</h3>
<p><strong>Network:</strong></p>
<pre><code>x â†’ [W, b] â†’ z â†’ ReLU â†’ h â†’ [W2, b2] â†’ y
</code></pre>
<p><strong>Forward:</strong></p>
<pre><code>x = 2
W = 0.5
b = 1
z = W*x + b = 0.5*2 + 1 = 2
h = ReLU(z) = 2
W2 = 0.8
b2 = 0.5
y = W2*h + b2 = 0.8*2 + 0.5 = 2.1

y_true = 3
Loss = (y_true - y)Â² = (3 - 2.1)Â² = 0.81
</code></pre>
<p><strong>Backward:</strong></p>
<pre><code>âˆ‚Loss/âˆ‚y = -2(y_true - y) = -2(3 - 2.1) = -1.8

âˆ‚y/âˆ‚W2 = h = 2
âˆ‚Loss/âˆ‚W2 = âˆ‚Loss/âˆ‚y Ã— âˆ‚y/âˆ‚W2 = -1.8 Ã— 2 = -3.6

âˆ‚y/âˆ‚h = W2 = 0.8
âˆ‚h/âˆ‚z = 1 (vÃ¬ z &gt; 0, ReLU derivative = 1)
âˆ‚z/âˆ‚W = x = 2
âˆ‚Loss/âˆ‚W = âˆ‚Loss/âˆ‚y Ã— âˆ‚y/âˆ‚h Ã— âˆ‚h/âˆ‚z Ã— âˆ‚z/âˆ‚W
         = -1.8 Ã— 0.8 Ã— 1 Ã— 2
         = -2.88
</code></pre>
<h3 id="code-implementation_1">ğŸ”§ Code Implementation</h3>
<pre><code class="language-python">def backward_propagation(cache, y_true, y_pred, W2):
    &quot;&quot;&quot;
    Backward pass to compute gradients

    Args:
        cache: Intermediate values from forward pass
        y_true: True labels
        y_pred: Predictions
        W2: Weights layer 2

    Returns:
        grads: Dictionary of gradients
    &quot;&quot;&quot;
    x = cache['x']
    z1 = cache['z1']
    h = cache['h']

    # Gradient of loss w.r.t. output
    dL_dy = 2 * (y_pred - y_true) / len(y_true)

    # Gradient w.r.t. W2, b2
    dL_dW2 = h.T @ dL_dy
    dL_db2 = np.sum(dL_dy, axis=0)

    # Gradient w.r.t. h
    dL_dh = dL_dy @ W2.T

    # Gradient w.r.t. z1 (ReLU derivative)
    dL_dz1 = dL_dh * (z1 &gt; 0)

    # Gradient w.r.t. W1, b1
    dL_dW1 = x.T @ dL_dz1
    dL_db1 = np.sum(dL_dz1, axis=0)

    grads = {
        'dW1': dL_dW1,
        'db1': dL_db1,
        'dW2': dL_dW2,
        'db2': dL_db2
    }

    return grads
</code></pre>
<hr />
<h2 id="8-gradient-descent">8. GRADIENT DESCENT</h2>
<h3 id="gradient-descent-la-gi">ğŸ¯ Gradient Descent lÃ  gÃ¬?</h3>
<blockquote>
<p><strong>Gradient Descent = Thuáº­t toÃ¡n update weights Ä‘á»ƒ giáº£m loss</strong></p>
</blockquote>
<p><strong>Ã tÆ°á»Ÿng:</strong></p>
<pre><code>1. TÃ­nh gradient (hÆ°á»›ng tÄƒng loss)
2. Äi ngÆ°á»£c hÆ°á»›ng gradient (Ä‘á»ƒ giáº£m loss)
3. Láº·p láº¡i cho Ä‘áº¿n khi loss khÃ´ng giáº£m ná»¯a
</code></pre>
<h3 id="visualize">ğŸ“Š Visualize</h3>
<pre><code>Loss
  â†‘
  â”‚     â•±â•²
  â”‚    â•±  â•²
  â”‚   â•±    â•²
  â”‚  â•±      â•²
  â”‚ â•±        â•²___
  â”‚â•±             â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight

  Start here â—
  â†“ (gradient descent)
  â†“
  â†“
  End here (minimum) â—
</code></pre>
<h3 id="cong-thuc_1">ğŸ”§ CÃ´ng thá»©c</h3>
<pre><code>W_new = W_old - learning_rate Ã— gradient

VÃ­ dá»¥:
W_old = 0.5
gradient = -2.88
learning_rate = 0.01

W_new = 0.5 - 0.01 Ã— (-2.88)
      = 0.5 + 0.0288
      = 0.5288
</code></pre>
<h3 id="learning-rate">ğŸ’¡ Learning Rate</h3>
<p><strong>Learning rate quÃ¡ lá»›n:</strong></p>
<pre><code>Loss
  â†‘
  â”‚     â•±â•²
  â”‚    â•±  â•²
  â”‚   â—â”€â”€â”€â”€â—  â† Nháº£y qua láº¡i, khÃ´ng há»™i tá»¥
  â”‚  â•±      â•²
  â”‚ â•±        â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight
</code></pre>
<p><strong>Learning rate quÃ¡ nhá»:</strong></p>
<pre><code>Loss
  â†‘
  â”‚     â•±â•²
  â”‚    â•±  â•²
  â”‚   â—â†’â†’â†’â†’  â† Cháº­m, máº¥t nhiá»u thá»i gian
  â”‚  â•±      â•²
  â”‚ â•±        â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight
</code></pre>
<p><strong>Learning rate vá»«a pháº£i:</strong></p>
<pre><code>Loss
  â†‘
  â”‚     â•±â•²
  â”‚    â•±  â•²
  â”‚   â—â†’â†’â—  â† Nhanh vÃ  há»™i tá»¥
  â”‚  â•±      â•²
  â”‚ â•±        â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight
</code></pre>
<h3 id="code-implementation_2">ğŸ”§ Code Implementation</h3>
<pre><code class="language-python">def gradient_descent_step(W1, b1, W2, b2, grads, learning_rate):
    &quot;&quot;&quot;
    Update weights using gradient descent

    Args:
        W1, b1, W2, b2: Current weights
        grads: Gradients from backprop
        learning_rate: Step size

    Returns:
        Updated weights
    &quot;&quot;&quot;
    W1 = W1 - learning_rate * grads['dW1']
    b1 = b1 - learning_rate * grads['db1']
    W2 = W2 - learning_rate * grads['dW2']
    b2 = b2 - learning_rate * grads['db2']

    return W1, b1, W2, b2
</code></pre>
<h3 id="training-loop">ğŸ“Š Training Loop</h3>
<pre><code class="language-python">def train(X_train, y_train, epochs=100, learning_rate=0.01):
    &quot;&quot;&quot;
    Full training loop
    &quot;&quot;&quot;
    # Initialize weights randomly
    W1 = np.random.randn(X_train.shape[1], 10) * 0.01
    b1 = np.zeros(10)
    W2 = np.random.randn(10, 1) * 0.01
    b2 = np.zeros(1)

    losses = []

    for epoch in range(epochs):
        # Forward pass
        y_pred, cache = forward_propagation(X_train, W1, b1, W2, b2)

        # Compute loss
        loss = mse_loss(y_train, y_pred)
        losses.append(loss)

        # Backward pass
        grads = backward_propagation(cache, y_train, y_pred, W2)

        # Update weights
        W1, b1, W2, b2 = gradient_descent_step(W1, b1, W2, b2, grads, learning_rate)

        if epoch % 10 == 0:
            print(f&quot;Epoch {epoch}, Loss: {loss:.4f}&quot;)

    return W1, b1, W2, b2, losses
</code></pre>
<hr />
<h2 id="9-overfitting-regularization">9. OVERFITTING &amp; REGULARIZATION</h2>
<h3 id="overfitting-trong-neural-networks">ğŸ¯ Overfitting trong Neural Networks</h3>
<p><strong>Dáº¥u hiá»‡u:</strong>
- Training loss ráº¥t tháº¥p
- Validation loss cao
- Model "nhá»›" training data thay vÃ¬ "há»c" pattern</p>
<h3 id="cac-ky-thuat-chong-overfitting">ğŸ’¡ CÃ¡c ká»¹ thuáº­t chá»‘ng Overfitting</h3>
<h4 id="1-l2-regularization-weight-decay"><strong>1. L2 Regularization (Weight Decay)</strong></h4>
<p><strong>Ã tÆ°á»Ÿng:</strong>
- Pháº¡t weights lá»›n
- Ã‰p weights nhá» láº¡i</p>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>Loss_total = Loss_data + Î» Ã— Î£(WÂ²)
                         â†‘
                    Regularization term
</code></pre>
<p><strong>Code:</strong></p>
<pre><code class="language-python">def mse_loss_with_l2(y_true, y_pred, W1, W2, lambda_reg=0.01):
    data_loss = np.mean((y_true - y_pred) ** 2)
    reg_loss = lambda_reg * (np.sum(W1**2) + np.sum(W2**2))
    return data_loss + reg_loss
</code></pre>
<h4 id="2-dropout"><strong>2. Dropout</strong></h4>
<p><strong>Ã tÆ°á»Ÿng:</strong>
- Randomly "táº¯t" má»™t sá»‘ neurons trong training
- Ã‰p network há»c robust features</p>
<p><strong>Visualize:</strong></p>
<pre><code>Training:
x â—‹     â—‹ h1 (active)
     â†’  âœ— h2 (dropped)  â†’  â—‹ y
x â—‹     â—‹ h3 (active)

Testing:
x â—‹     â—‹ h1
     â†’  â—‹ h2  â†’  â—‹ y
x â—‹     â—‹ h3
</code></pre>
<p><strong>Code:</strong></p>
<pre><code class="language-python">def dropout(h, dropout_rate=0.5, training=True):
    if training:
        mask = np.random.rand(*h.shape) &gt; dropout_rate
        return h * mask / (1 - dropout_rate)
    else:
        return h
</code></pre>
<h4 id="3-early-stopping"><strong>3. Early Stopping</strong></h4>
<p><strong>Ã tÆ°á»Ÿng:</strong>
- Dá»«ng training khi validation loss khÃ´ng giáº£m ná»¯a</p>
<p><strong>Code:</strong></p>
<pre><code class="language-python">best_val_loss = float('inf')
patience = 10
patience_counter = 0

for epoch in range(epochs):
    # Training...
    val_loss = evaluate(X_val, y_val)

    if val_loss &lt; best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # Save best model
    else:
        patience_counter += 1
        if patience_counter &gt;= patience:
            print(&quot;Early stopping!&quot;)
            break
</code></pre>
<hr />
<h2 id="10-training-pipeline-thuc-te">10. TRAINING PIPELINE THá»°C Táº¾</h2>
<h3 id="training-pipeline-la-gi">ğŸ¯ Training Pipeline lÃ  gÃ¬?</h3>
<blockquote>
<p><strong>Training Pipeline = Quy trÃ¬nh train model tá»« A-Z trong thá»±c táº¿</strong></p>
</blockquote>
<p><strong>Pipeline Ä‘áº§y Ä‘á»§:</strong></p>
<pre><code>Load Data â†’ Split (Train/Val/Test) â†’ Preprocessing â†’
Initialize Model â†’ Training Loop â†’ Validation â†’ 
Early Stopping â†’ Save Best Model â†’ Test Evaluation
</code></pre>
<h3 id="101-trainvaltest-split">ğŸ“Š 10.1. Train/Val/Test Split</h3>
<p><strong>Táº¡i sao cáº§n 3 sets?</strong></p>
<pre><code>Training Set (70%):   Train model
Validation Set (15%): Tune hyperparameters, early stopping
Test Set (15%):       ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng (KHÃ”NG CHáº M Ä‘áº¿n trong quÃ¡ trÃ¬nh train!)

Táº¡i sao khÃ´ng chá»‰ Train/Test?
â†’ Náº¿u tune hyperparameters trÃªn test set â†’ Data leakage!
â†’ Test set pháº£i &quot;unseen&quot; hoÃ n toÃ n
</code></pre>
<p><strong>Code:</strong></p>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

# Load data
X = ...  # Features
y = ...  # Target

# Split 1: Train + (Val+Test)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, shuffle=False  # shuffle=False cho time series!
)

# Split 2: Val + Test
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False
)

print(f&quot;Train: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)&quot;)
print(f&quot;Val:   {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)&quot;)
print(f&quot;Test:  {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)&quot;)
</code></pre>
<h3 id="102-training-loop-voi-validation">ğŸ“Š 10.2. Training Loop vá»›i Validation</h3>
<p><strong>Full Training Loop:</strong></p>
<pre><code class="language-python">def train_model(model, X_train, y_train, X_val, y_val, 
                epochs=100, batch_size=32, learning_rate=0.01):
    &quot;&quot;&quot;
    Training loop vá»›i validation
    &quot;&quot;&quot;
    # History Ä‘á»ƒ plot sau
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_mae': [],
        'val_mae': []
    }

    # Best model tracking
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0

    for epoch in range(epochs):
        # ==================== TRAINING ====================
        model.train()  # Set model to training mode

        train_losses = []
        train_maes = []

        # Mini-batch training
        n_batches = len(X_train) // batch_size

        for i in range(n_batches):
            # Get batch
            start_idx = i * batch_size
            end_idx = start_idx + batch_size

            X_batch = X_train[start_idx:end_idx]
            y_batch = y_train[start_idx:end_idx]

            # Forward pass
            y_pred, cache = model.forward(X_batch)

            # Compute loss
            loss = mse_loss(y_batch, y_pred)
            mae = np.mean(np.abs(y_batch - y_pred))

            train_losses.append(loss)
            train_maes.append(mae)

            # Backward pass
            grads = model.backward(cache, y_batch, y_pred)

            # Update weights
            model.update_weights(grads, learning_rate)

        # Average training metrics
        train_loss = np.mean(train_losses)
        train_mae = np.mean(train_maes)

        # ==================== VALIDATION ====================
        model.eval()  # Set model to evaluation mode

        y_val_pred, _ = model.forward(X_val)
        val_loss = mse_loss(y_val, y_val_pred)
        val_mae = np.mean(np.abs(y_val - y_val_pred))

        # Save history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_mae'].append(train_mae)
        history['val_mae'].append(val_mae)

        # Print progress
        if epoch % 10 == 0:
            print(f&quot;Epoch {epoch}/{epochs}&quot;)
            print(f&quot;  Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}&quot;)
            print(f&quot;  Val Loss:   {val_loss:.4f}, Val MAE:   {val_mae:.4f}&quot;)

        # ==================== EARLY STOPPING ====================
        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save best model
            save_model(model, 'best_model.pkl')
            print(f&quot;  â†’ Best model saved! Val Loss: {val_loss:.4f}&quot;)
        else:
            patience_counter += 1
            if patience_counter &gt;= patience:
                print(f&quot;\nEarly stopping at epoch {epoch}!&quot;)
                print(f&quot;Best Val Loss: {best_val_loss:.4f}&quot;)
                break

    return history
</code></pre>
<h3 id="103-early-stopping-chi-tiet">ğŸ“Š 10.3. Early Stopping (Chi tiáº¿t)</h3>
<p><strong>Táº¡i sao cáº§n Early Stopping?</strong></p>
<pre><code>Epochs â†’  0 â”€â”€â”€â”€â”€â”€â”€â”€ 50 â”€â”€â”€â”€â”€â”€â”€â”€ 100 â”€â”€â”€â”€â”€â”€â”€â”€ 150
Train:    High â”€â”€â”€â”€â†’ Low â”€â”€â”€â”€â”€â†’ Very Low â”€â”€â†’ Extremely Low
Val:      High â”€â”€â”€â”€â†’ Low â”€â”€â”€â”€â”€â†’ Low â”€â”€â”€â”€â”€â”€â”€â”€â†’ High (Overfitting!)
                              â†‘
                         Stop here!
</code></pre>
<p><strong>Implementation Ä‘áº§y Ä‘á»§:</strong></p>
<pre><code class="language-python">class EarlyStopping:
    &quot;&quot;&quot;
    Early Stopping Ä‘á»ƒ trÃ¡nh overfitting
    &quot;&quot;&quot;
    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):
        &quot;&quot;&quot;
        Args:
            patience: Sá»‘ epochs chá» trÆ°á»›c khi stop
            min_delta: Minimum change Ä‘á»ƒ coi lÃ  improvement
            restore_best_weights: CÃ³ restore weights tá»‘t nháº¥t khÃ´ng
        &quot;&quot;&quot;
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights

        self.best_loss = float('inf')
        self.best_weights = None
        self.counter = 0
        self.should_stop = False

    def __call__(self, val_loss, model):
        &quot;&quot;&quot;
        Check xem cÃ³ nÃªn stop khÃ´ng
        &quot;&quot;&quot;
        # Check improvement
        if val_loss &lt; self.best_loss - self.min_delta:
            # Improved!
            self.best_loss = val_loss
            self.counter = 0

            if self.restore_best_weights:
                self.best_weights = model.get_weights()  # Save weights

            return False  # Continue training

        else:
            # No improvement
            self.counter += 1

            if self.counter &gt;= self.patience:
                self.should_stop = True

                if self.restore_best_weights and self.best_weights is not None:
                    model.set_weights(self.best_weights)  # Restore best weights

                return True  # Stop training

        return False

# Usage
early_stopping = EarlyStopping(patience=10, min_delta=0.001)

for epoch in range(epochs):
    # Train...
    val_loss = evaluate(model, X_val, y_val)

    if early_stopping(val_loss, model):
        print(f&quot;Early stopping at epoch {epoch}!&quot;)
        break
</code></pre>
<h3 id="104-learning-rate-scheduling">ğŸ“Š 10.4. Learning Rate Scheduling</h3>
<p><strong>Táº¡i sao cáº§n LR Scheduling?</strong></p>
<pre><code>Learning rate cao (0.1):  Há»c nhanh nhÆ°ng khÃ´ng há»™i tá»¥
Learning rate tháº¥p (0.0001): Há»c cháº­m, máº¥t thá»i gian

â†’ Giáº£i phÃ¡p: Báº¯t Ä‘áº§u cao, giáº£m dáº§n theo thá»i gian
</code></pre>
<p><strong>CÃ¡c loáº¡i LR Schedules:</strong></p>
<p><strong>1. Step Decay</strong></p>
<pre><code class="language-python">def step_decay(epoch, initial_lr=0.01, drop=0.5, epochs_drop=10):
    &quot;&quot;&quot;
    Giáº£m LR theo steps

    Epoch 0-9:   lr = 0.01
    Epoch 10-19: lr = 0.005
    Epoch 20-29: lr = 0.0025
    &quot;&quot;&quot;
    return initial_lr * (drop ** (epoch // epochs_drop))
</code></pre>
<p><strong>2. Exponential Decay</strong></p>
<pre><code class="language-python">def exponential_decay(epoch, initial_lr=0.01, decay_rate=0.95):
    &quot;&quot;&quot;
    Giáº£m LR theo hÃ m mÅ©

    lr(t) = initial_lr * (decay_rate ^ epoch)
    &quot;&quot;&quot;
    return initial_lr * (decay_rate ** epoch)
</code></pre>
<p><strong>3. Cosine Annealing</strong></p>
<pre><code class="language-python">import numpy as np

def cosine_annealing(epoch, initial_lr=0.01, T_max=100, eta_min=0.0001):
    &quot;&quot;&quot;
    Giáº£m LR theo cosine

    lr(t) = eta_min + (initial_lr - eta_min) * (1 + cos(Ï€t/T_max)) / 2
    &quot;&quot;&quot;
    return eta_min + (initial_lr - eta_min) * (1 + np.cos(np.pi * epoch / T_max)) / 2
</code></pre>
<p><strong>4. ReduceLROnPlateau (ThÃ´ng minh nháº¥t)</strong></p>
<pre><code class="language-python">class ReduceLROnPlateau:
    &quot;&quot;&quot;
    Giáº£m LR khi val loss khÃ´ng giáº£m
    &quot;&quot;&quot;
    def __init__(self, factor=0.5, patience=5, min_lr=1e-6):
        self.factor = factor
        self.patience = patience
        self.min_lr = min_lr

        self.best_loss = float('inf')
        self.counter = 0

    def __call__(self, val_loss, current_lr):
        if val_loss &lt; self.best_loss:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1

            if self.counter &gt;= self.patience:
                new_lr = max(current_lr * self.factor, self.min_lr)
                print(f&quot;Reducing LR: {current_lr:.6f} â†’ {new_lr:.6f}&quot;)
                self.counter = 0
                return new_lr

        return current_lr

# Usage
lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=5)
learning_rate = 0.01

for epoch in range(epochs):
    # Train...
    val_loss = evaluate(model, X_val, y_val)

    learning_rate = lr_scheduler(val_loss, learning_rate)
</code></pre>
<p><strong>Visualize LR Schedules:</strong></p>
<pre><code class="language-python">import matplotlib.pyplot as plt

epochs = 100
lrs_step = [step_decay(e) for e in range(epochs)]
lrs_exp = [exponential_decay(e) for e in range(epochs)]
lrs_cos = [cosine_annealing(e, T_max=epochs) for e in range(epochs)]

plt.figure(figsize=(12, 6))
plt.plot(lrs_step, label='Step Decay')
plt.plot(lrs_exp, label='Exponential Decay')
plt.plot(lrs_cos, label='Cosine Annealing')
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedules')
plt.legend()
plt.grid(True)
plt.show()
</code></pre>
<h3 id="105-model-checkpointing">ğŸ“Š 10.5. Model Checkpointing</h3>
<p><strong>Táº¡i sao cáº§n Checkpointing?</strong></p>
<pre><code>Training 100 epochs, epoch 95 crash â†’ Máº¥t háº¿t! ğŸ˜­

â†’ Save model Ä‘á»‹nh ká»³
</code></pre>
<p><strong>Implementation:</strong></p>
<pre><code class="language-python">import pickle
import os

class ModelCheckpoint:
    &quot;&quot;&quot;
    Save model Ä‘á»‹nh ká»³
    &quot;&quot;&quot;
    def __init__(self, filepath='checkpoints/model_{epoch:03d}.pkl', 
                 save_best_only=True, monitor='val_loss', mode='min'):
        self.filepath = filepath
        self.save_best_only = save_best_only
        self.monitor = monitor
        self.mode = mode

        self.best = float('inf') if mode == 'min' else float('-inf')

    def __call__(self, epoch, model, metrics):
        &quot;&quot;&quot;
        Save model náº¿u cáº§n
        &quot;&quot;&quot;
        current = metrics[self.monitor]

        # Check if should save
        should_save = False

        if self.save_best_only:
            if self.mode == 'min' and current &lt; self.best:
                should_save = True
                self.best = current
            elif self.mode == 'max' and current &gt; self.best:
                should_save = True
                self.best = current
        else:
            should_save = True

        if should_save:
            filepath = self.filepath.format(epoch=epoch)
            os.makedirs(os.path.dirname(filepath), exist_ok=True)

            with open(filepath, 'wb') as f:
                pickle.dump(model, f)

            print(f&quot;Model saved to {filepath}&quot;)

# Usage
checkpoint = ModelCheckpoint(
    filepath='checkpoints/model_epoch_{epoch:03d}_loss_{val_loss:.4f}.pkl',
    save_best_only=True,
    monitor='val_loss'
)

for epoch in range(epochs):
    # Train...
    metrics = {
        'train_loss': train_loss,
        'val_loss': val_loss
    }

    checkpoint(epoch, model, metrics)
</code></pre>
<h3 id="106-complete-training-pipeline">ğŸ“Š 10.6. Complete Training Pipeline</h3>
<p><strong>Full Pipeline Code:</strong></p>
<pre><code class="language-python">def full_training_pipeline(X_train, y_train, X_val, y_val, X_test, y_test,
                           model, epochs=100, batch_size=32, initial_lr=0.01):
    &quot;&quot;&quot;
    Complete training pipeline
    &quot;&quot;&quot;
    # Initialize components
    early_stopping = EarlyStopping(patience=10, min_delta=0.001)
    lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=5)
    checkpoint = ModelCheckpoint(save_best_only=True)

    # Training
    learning_rate = initial_lr
    history = {'train_loss': [], 'val_loss': []}

    for epoch in range(epochs):
        print(f&quot;\nEpoch {epoch+1}/{epochs}, LR: {learning_rate:.6f}&quot;)

        # Train
        train_loss = train_one_epoch(model, X_train, y_train, batch_size, learning_rate)

        # Validate
        val_loss = evaluate(model, X_val, y_val)

        # Save history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)

        print(f&quot;Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}&quot;)

        # LR Scheduling
        learning_rate = lr_scheduler(val_loss, learning_rate)

        # Checkpointing
        metrics = {'val_loss': val_loss}
        checkpoint(epoch, model, metrics)

        # Early Stopping
        if early_stopping(val_loss, model):
            print(f&quot;Early stopping at epoch {epoch+1}!&quot;)
            break

    # Final evaluation on test set
    print(&quot;\n&quot; + &quot;=&quot;*50)
    print(&quot;FINAL EVALUATION ON TEST SET&quot;)
    print(&quot;=&quot;*50)

    test_loss = evaluate(model, X_test, y_test)
    print(f&quot;Test Loss: {test_loss:.4f}&quot;)

    # Plot history
    plot_history(history)

    return model, history
</code></pre>
<hr />
<h2 id="11-debug-deep-learning">11. DEBUG DEEP LEARNING</h2>
<h3 id="debugging-la-ky-nang-1">ğŸ¯ Debugging lÃ  ká»¹ nÄƒng #1!</h3>
<blockquote>
<p><strong>90% thá»i gian Deep Learning = Debug, 10% = Train model</strong></p>
</blockquote>
<h3 id="111-vanishing-gradient-problem">ğŸ“Š 11.1. Vanishing Gradient Problem</h3>
<p><strong>Váº¥n Ä‘á»:</strong></p>
<pre><code>Gradient &quot;biáº¿n máº¥t&quot; khi backprop qua nhiá»u layers

Layer 10: gradient = 0.5
Layer 9:  gradient = 0.5 Ã— 0.5 = 0.25
Layer 8:  gradient = 0.25 Ã— 0.5 = 0.125
...
Layer 1:  gradient â‰ˆ 0.0001 (quÃ¡ nhá»!)

â†’ Layers Ä‘áº§u khÃ´ng há»c Ä‘Æ°á»£c gÃ¬!
</code></pre>
<p><strong>NguyÃªn nhÃ¢n:</strong></p>
<p><strong>1. Sigmoid/Tanh Activation</strong></p>
<pre><code class="language-python"># Sigmoid derivative
def sigmoid_derivative(x):
    s = 1 / (1 + np.exp(-x))
    return s * (1 - s)

# Maximum derivative = 0.25
x = np.linspace(-10, 10, 1000)
deriv = sigmoid_derivative(x)
print(f&quot;Max derivative: {deriv.max()}&quot;)  # 0.25

# Sau 10 layers:
grad = 0.25 ** 10  # = 9.5e-7 â†’ Vanished!
</code></pre>
<p><strong>2. Poor Weight Initialization</strong></p>
<pre><code class="language-python"># âŒ BAD: Weights quÃ¡ nhá»
W = np.random.randn(100, 100) * 0.0001
# â†’ Activations vÃ  gradients cÅ©ng quÃ¡ nhá»

# âœ… GOOD: Xavier/He initialization
W = np.random.randn(100, 100) * np.sqrt(2.0 / 100)  # He init cho ReLU
</code></pre>
<p><strong>CÃ¡ch phÃ¡t hiá»‡n:</strong></p>
<pre><code class="language-python">def check_vanishing_gradient(model, X, y):
    &quot;&quot;&quot;
    Kiá»ƒm tra vanishing gradient
    &quot;&quot;&quot;
    # Forward pass
    y_pred, cache = model.forward(X)
    loss = mse_loss(y, y_pred)

    # Backward pass
    grads = model.backward(cache, y, y_pred)

    # Check gradient magnitudes
    print(&quot;Gradient Magnitudes:&quot;)
    for layer_name, grad in grads.items():
        grad_norm = np.linalg.norm(grad)
        print(f&quot;  {layer_name}: {grad_norm:.6f}&quot;)

        if grad_norm &lt; 1e-5:
            print(f&quot;    âš ï¸ WARNING: Very small gradient! Vanishing?&quot;)
</code></pre>
<p><strong>Giáº£i phÃ¡p:</strong></p>
<p><strong>1. DÃ¹ng ReLU thay Sigmoid/Tanh</strong></p>
<pre><code class="language-python"># ReLU derivative = 1 (náº¿u x &gt; 0)
# KhÃ´ng bá»‹ vanishing!
</code></pre>
<p><strong>2. Batch Normalization</strong></p>
<pre><code class="language-python">def batch_norm(x, gamma, beta, eps=1e-5):
    &quot;&quot;&quot;
    Batch Normalization
    &quot;&quot;&quot;
    mean = np.mean(x, axis=0)
    var = np.var(x, axis=0)

    x_norm = (x - mean) / np.sqrt(var + eps)
    out = gamma * x_norm + beta

    return out
</code></pre>
<p><strong>3. Residual Connections (ResNet)</strong></p>
<pre><code class="language-python">def residual_block(x, W1, W2):
    &quot;&quot;&quot;
    Residual connection

    out = F(x) + x
         â†‘       â†‘
      learned  skip connection
    &quot;&quot;&quot;
    # Forward
    h = relu(x @ W1)
    out = h @ W2

    # Add skip connection
    out = out + x  # â† GiÃºp gradient flow tá»‘t hÆ¡n!

    return out
</code></pre>
<h3 id="112-exploding-gradient-problem">ğŸ“Š 11.2. Exploding Gradient Problem</h3>
<p><strong>Váº¥n Ä‘á»:</strong></p>
<pre><code>Gradient &quot;phÃ¡t ná»•&quot; khi backprop

Layer 1: gradient = 2.0
Layer 2: gradient = 2.0 Ã— 2.0 = 4.0
Layer 3: gradient = 4.0 Ã— 2.0 = 8.0
...
Layer 10: gradient = 1024 â†’ Explode!

â†’ Weights bá»‹ update quÃ¡ máº¡nh â†’ NaN/Inf!
</code></pre>
<p><strong>CÃ¡ch phÃ¡t hiá»‡n:</strong></p>
<pre><code class="language-python">def check_exploding_gradient(model, X, y):
    &quot;&quot;&quot;
    Kiá»ƒm tra exploding gradient
    &quot;&quot;&quot;
    grads = model.compute_gradients(X, y)

    for layer_name, grad in grads.items():
        grad_norm = np.linalg.norm(grad)
        print(f&quot;{layer_name}: {grad_norm:.6f}&quot;)

        if grad_norm &gt; 1000:
            print(f&quot;  âš ï¸ WARNING: Very large gradient! Exploding?&quot;)

        if np.isnan(grad).any() or np.isinf(grad).any():
            print(f&quot;  âŒ ERROR: NaN/Inf in gradient!&quot;)
</code></pre>
<p><strong>Giáº£i phÃ¡p:</strong></p>
<p><strong>1. Gradient Clipping</strong></p>
<pre><code class="language-python">def clip_gradients(grads, max_norm=5.0):
    &quot;&quot;&quot;
    Clip gradients Ä‘á»ƒ khÃ´ng vÆ°á»£t quÃ¡ max_norm
    &quot;&quot;&quot;
    total_norm = 0
    for grad in grads.values():
        total_norm += np.sum(grad ** 2)
    total_norm = np.sqrt(total_norm)

    if total_norm &gt; max_norm:
        clip_coef = max_norm / (total_norm + 1e-6)
        for key in grads:
            grads[key] = grads[key] * clip_coef

    return grads

# Usage
grads = model.backward(cache, y_true, y_pred)
grads = clip_gradients(grads, max_norm=5.0)
model.update_weights(grads, learning_rate)
</code></pre>
<p><strong>2. Lower Learning Rate</strong></p>
<pre><code class="language-python"># âŒ lr = 0.1 â†’ Exploding
# âœ… lr = 0.001 â†’ OK
</code></pre>
<p><strong>3. Weight Initialization</strong></p>
<pre><code class="language-python"># Xavier initialization
W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)

# He initialization (cho ReLU)
W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)
</code></pre>
<h3 id="113-loss-khong-giam-debugging-checklist">ğŸ“Š 11.3. Loss KhÃ´ng Giáº£m - Debugging Checklist</h3>
<p><strong>Triá»‡u chá»©ng:</strong></p>
<pre><code>Epoch 1: Loss = 5.234
Epoch 10: Loss = 5.231
Epoch 50: Loss = 5.229
Epoch 100: Loss = 5.228

â†’ Loss khÃ´ng giáº£m Ä‘Ã¡ng ká»ƒ!
</code></pre>
<p><strong>Checklist debug (theo thá»© tá»±):</strong></p>
<p><strong>1. Kiá»ƒm tra data</strong></p>
<pre><code class="language-python"># Check input
print(f&quot;X shape: {X.shape}&quot;)
print(f&quot;X min: {X.min()}, max: {X.max()}&quot;)
print(f&quot;X has NaN: {np.isnan(X).any()}&quot;)

# Check target
print(f&quot;y shape: {y.shape}&quot;)
print(f&quot;y min: {y.min()}, max: {y.max()}&quot;)
print(f&quot;y has NaN: {np.isnan(y).any()}&quot;)

# Check alignment
assert len(X) == len(y), &quot;X vÃ  y khÃ´ng cÃ¹ng length!&quot;
</code></pre>
<p><strong>2. Kiá»ƒm tra preprocessing</strong></p>
<pre><code class="language-python"># âŒ Forget scaling
X_raw = load_data()  # [1000, 50000, 0.5, ...]
# â†’ Scale khÃ¡c nhau quÃ¡ lá»›n!

# âœ… Scale data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)
</code></pre>
<p><strong>3. Kiá»ƒm tra model forward</strong></p>
<pre><code class="language-python"># Test forward pass vá»›i 1 sample
x_test = X_train[0:1]  # (1, features)
y_pred, cache = model.forward(x_test)

print(f&quot;Prediction: {y_pred}&quot;)
print(f&quot;Expected: {y_train[0]}&quot;)

# Prediction cÃ³ há»£p lÃ½ khÃ´ng?
# Náº¿u predict = [1e10] mÃ  y_true = [100] â†’ CÃ³ váº¥n Ä‘á»!
</code></pre>
<p><strong>4. Kiá»ƒm tra learning rate</strong></p>
<pre><code class="language-python"># Test vá»›i nhiá»u LRs
lrs = [0.001, 0.01, 0.1, 1.0]

for lr in lrs:
    model = initialize_model()
    losses = []

    for epoch in range(10):
        loss = train_one_epoch(model, X_train, y_train, lr=lr)
        losses.append(loss)

    print(f&quot;LR={lr}: Final loss={losses[-1]:.4f}&quot;)
    plt.plot(losses, label=f'LR={lr}')

plt.legend()
plt.show()

# LR tá»‘t nháº¥t: Loss giáº£m smooth vÃ  nhanh
</code></pre>
<p><strong>5. Kiá»ƒm tra gradient</strong></p>
<pre><code class="language-python"># Numerical gradient check
def numerical_gradient(model, X, y, epsilon=1e-5):
    &quot;&quot;&quot;
    TÃ­nh gradient báº±ng numerical differentiation
    &quot;&quot;&quot;
    numerical_grads = {}

    for param_name, param in model.params.items():
        numerical_grad = np.zeros_like(param)

        it = np.nditer(param, flags=['multi_index'])
        while not it.finished:
            idx = it.multi_index

            # f(x + epsilon)
            old_value = param[idx]
            param[idx] = old_value + epsilon
            loss_plus, _ = model.forward(X)
            loss_plus = mse_loss(y, loss_plus)

            # f(x - epsilon)
            param[idx] = old_value - epsilon
            loss_minus, _ = model.forward(X)
            loss_minus = mse_loss(y, loss_minus)

            # Gradient
            numerical_grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)

            # Restore
            param[idx] = old_value
            it.iternext()

        numerical_grads[param_name] = numerical_grad

    return numerical_grads

# So sÃ¡nh vá»›i backprop gradient
y_pred, cache = model.forward(X)
backprop_grads = model.backward(cache, y, y_pred)
numerical_grads = numerical_gradient(model, X, y)

for param_name in backprop_grads:
    diff = np.linalg.norm(backprop_grads[param_name] - numerical_grads[param_name])
    print(f&quot;{param_name}: difference = {diff:.8f}&quot;)

    if diff &gt; 1e-5:
        print(f&quot;  âš ï¸ WARNING: Gradient may be wrong!&quot;)
</code></pre>
<p><strong>6. Overfitting Check</strong></p>
<pre><code class="language-python"># Overfit 1 batch Ä‘á»ƒ check model capacity
X_tiny = X_train[:32]  # 1 batch
y_tiny = y_train[:32]

for epoch in range(1000):
    loss = train_one_epoch(model, X_tiny, y_tiny, lr=0.01)

    if epoch % 100 == 0:
        print(f&quot;Epoch {epoch}: Loss = {loss:.6f}&quot;)

# Náº¿u loss KHÃ”NG giáº£m vá» ~0 â†’ Model cÃ³ váº¥n Ä‘á»!
# (Model pháº£i overfit Ä‘Æ°á»£c Ã­t nháº¥t 1 batch)
</code></pre>
<h3 id="114-how-to-tune-learning-rate">ğŸ“Š 11.4. How to Tune Learning Rate</h3>
<p><strong>Strategy 1: Learning Rate Range Test</strong></p>
<pre><code class="language-python">def lr_range_test(model, X_train, y_train, min_lr=1e-5, max_lr=10, num_steps=100):
    &quot;&quot;&quot;
    Test nhiá»u LRs Ä‘á»ƒ tÃ¬m LR tá»‘t nháº¥t
    &quot;&quot;&quot;
    lrs = np.logspace(np.log10(min_lr), np.log10(max_lr), num_steps)
    losses = []

    for lr in lrs:
        # Train 1 batch
        loss = train_one_epoch(model, X_train, y_train, lr=lr, epochs=1)
        losses.append(loss)

        # Stop náº¿u loss explode
        if len(losses) &gt; 1 and losses[-1] &gt; losses[-2] * 3:
            break

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(lrs[:len(losses)], losses)
    plt.xscale('log')
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('Learning Rate Range Test')
    plt.grid(True)
    plt.show()

    # Best LR: Giáº£m nhanh nháº¥t
    best_idx = np.argmin(np.gradient(losses))
    best_lr = lrs[best_idx]
    print(f&quot;Suggested LR: {best_lr:.6f}&quot;)

    return best_lr

# Usage
best_lr = lr_range_test(model, X_train, y_train)
</code></pre>
<p><strong>Strategy 2: Grid Search</strong></p>
<pre><code class="language-python">lrs = [0.0001, 0.001, 0.01, 0.1]

best_lr = None
best_val_loss = float('inf')

for lr in lrs:
    print(f&quot;\nTesting LR = {lr}&quot;)

    model = initialize_model()
    history = train(model, X_train, y_train, X_val, y_val, 
                    epochs=50, lr=lr)

    val_loss = history['val_loss'][-1]
    print(f&quot;Final val loss: {val_loss:.4f}&quot;)

    if val_loss &lt; best_val_loss:
        best_val_loss = val_loss
        best_lr = lr

print(f&quot;\nBest LR: {best_lr}&quot;)
</code></pre>
<p><strong>Signs of Good/Bad LR:</strong></p>
<pre><code>LR quÃ¡ nhá» (0.00001):
Epoch 1: Loss = 5.234
Epoch 10: Loss = 5.220
Epoch 50: Loss = 5.180
â†’ Giáº£m quÃ¡ cháº­m!

LR vá»«a pháº£i (0.001):
Epoch 1: Loss = 5.234
Epoch 10: Loss = 4.123
Epoch 50: Loss = 2.456
â†’ Giáº£m smooth vÃ  nhanh âœ…

LR quÃ¡ lá»›n (0.1):
Epoch 1: Loss = 5.234
Epoch 2: Loss = 8.123
Epoch 3: Loss = 15.234
â†’ Loss tÄƒng / oscillate! âŒ

LR ráº¥t lá»›n (1.0):
Epoch 1: Loss = 5.234
Epoch 2: Loss = NaN
â†’ Explode ngay! âŒ
</code></pre>
<h3 id="debug-workflow">ğŸ’¡ Debug Workflow</h3>
<p><strong>Khi loss khÃ´ng giáº£m, lÃ m theo thá»© tá»±:</strong></p>
<ol>
<li>âœ… Check data (shape, NaN, range)</li>
<li>âœ… Overfit 1 batch (model cÃ³ capacity?)</li>
<li>âœ… Check gradient (numerical vs backprop)</li>
<li>âœ… Tune learning rate (LR range test)</li>
<li>âœ… Check preprocessing (scaling)</li>
<li>âœ… Simplify model (too complex?)</li>
<li>âœ… Add regularization (overfitting?)</li>
</ol>
<hr />
<h2 id="12-optimization-algorithms">12. OPTIMIZATION ALGORITHMS</h2>
<h3 id="tong-quan">Tá»•ng quan</h3>
<p>Gradient Descent cÆ¡ báº£n cÃ³ nhiá»u háº¡n cháº¿. CÃ¡c optimizer hiá»‡n Ä‘áº¡i cáº£i thiá»‡n tá»‘c Ä‘á»™ vÃ  kháº£ nÄƒng há»™i tá»¥.</p>
<h3 id="121-sgd-stochastic-gradient-descent">12.1. SGD (Stochastic Gradient Descent)</h3>
<p><strong>Ã tÆ°á»Ÿng:</strong> Update weights sau má»—i mini-batch thay vÃ¬ toÃ n bá»™ dataset.</p>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>Î¸ = Î¸ - Î· Ã— âˆ‡L(Î¸)

Trong Ä‘Ã³:
- Î¸: weights
- Î·: learning rate
- âˆ‡L(Î¸): gradient cá»§a loss
</code></pre>
<p><strong>Váº¥n Ä‘á»:</strong></p>
<pre><code>Loss surface
    â•±â•²
   â•±  â•²     â† SGD dao Ä‘á»™ng, cháº­m trong &quot;ravines&quot;
  â•±    â•²       (vÃ¹ng dá»‘c má»™t chiá»u, pháº³ng chiá»u khÃ¡c)
 â•±      â•²___â—
</code></pre>
<pre><code class="language-python"># SGD cÆ¡ báº£n
def sgd_update(params, grads, lr):
    for key in params:
        params[key] -= lr * grads[key]
    return params
</code></pre>
<h3 id="122-momentum">12.2. Momentum</h3>
<p><strong>Ã tÆ°á»Ÿng:</strong> ThÃªm "quÃ¡n tÃ­nh" - nhá»› hÆ°á»›ng Ä‘i trÆ°á»›c Ä‘Ã³.</p>
<p><strong>Trá»±c giÃ¡c:</strong></p>
<pre><code>TÆ°á»Ÿng tÆ°á»£ng quáº£ bÃ³ng lÄƒn xuá»‘ng dá»‘c:
- KhÃ´ng cÃ³ momentum: Dá»«ng ngay khi gradient = 0
- CÃ³ momentum: Tiáº¿p tá»¥c lÄƒn theo Ä‘Ã  â†’ VÆ°á»£t qua local minima

v = Î² Ã— v + âˆ‡L(Î¸)        â† Cá»™ng dá»“n momentum
Î¸ = Î¸ - Î· Ã— v            â† Update vá»›i velocity
</code></pre>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>v_t = Î² Ã— v_{t-1} + Î· Ã— âˆ‡L(Î¸)
Î¸ = Î¸ - v_t

ThÃ´ng thÆ°á»ng: Î² = 0.9
</code></pre>
<p><strong>Lá»£i Ã­ch:</strong></p>
<pre><code>Ravine problem:
                    SGD          Momentum
                   â•±â•²â•±â•²          â”€â”€â”€â”€â”€â”€â†’
                  â•±    â•²        (Smooth path)
                 â•±      â•²___â—
</code></pre>
<pre><code class="language-python">def sgd_momentum_update(params, grads, velocity, lr, momentum=0.9):
    for key in params:
        velocity[key] = momentum * velocity[key] + lr * grads[key]
        params[key] -= velocity[key]
    return params, velocity
</code></pre>
<h3 id="123-rmsprop">12.3. RMSProp</h3>
<p><strong>Ã tÆ°á»Ÿng:</strong> Tá»± Ä‘iá»u chá»‰nh learning rate cho tá»«ng parameter dá»±a trÃªn lá»‹ch sá»­ gradient.</p>
<p><strong>Trá»±c giÃ¡c:</strong></p>
<pre><code>- Parameter cÃ³ gradient lá»›n liÃªn tá»¥c â†’ Giáº£m LR
- Parameter cÃ³ gradient nhá» â†’ TÄƒng LR
â†’ CÃ¢n báº±ng tá»‘c Ä‘á»™ há»c giá»¯a cÃ¡c dimensions
</code></pre>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>s_t = Î² Ã— s_{t-1} + (1-Î²) Ã— (âˆ‡L)Â²     â† Exponential moving average cá»§a gradientÂ²
Î¸ = Î¸ - Î· Ã— âˆ‡L / (âˆšs_t + Îµ)           â† Chia cho âˆšs Ä‘á»ƒ normalize

ThÃ´ng thÆ°á»ng: Î² = 0.9, Îµ = 1e-8
</code></pre>
<pre><code class="language-python">def rmsprop_update(params, grads, cache, lr, decay_rate=0.9, eps=1e-8):
    for key in params:
        cache[key] = decay_rate * cache[key] + (1 - decay_rate) * grads[key]**2
        params[key] -= lr * grads[key] / (np.sqrt(cache[key]) + eps)
    return params, cache
</code></pre>
<h3 id="124-adam-adaptive-moment-estimation">12.4. Adam (Adaptive Moment Estimation)</h3>
<p><strong>Ã tÆ°á»Ÿng:</strong> Káº¿t há»£p Momentum + RMSProp + Bias correction.</p>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>m_t = Î²1 Ã— m_{t-1} + (1-Î²1) Ã— âˆ‡L       â† First moment (momentum)
v_t = Î²2 Ã— v_{t-1} + (1-Î²2) Ã— (âˆ‡L)Â²    â† Second moment (RMSProp)

# Bias correction (quan trá»ng á»Ÿ Ä‘áº§u training)
mÌ‚_t = m_t / (1 - Î²1^t)
vÌ‚_t = v_t / (1 - Î²2^t)

Î¸ = Î¸ - Î· Ã— mÌ‚_t / (âˆšvÌ‚_t + Îµ)

ThÃ´ng thÆ°á»ng: Î²1 = 0.9, Î²2 = 0.999, Îµ = 1e-8
</code></pre>
<p><strong>Táº¡i sao cáº§n Bias correction?</strong></p>
<pre><code>Ban Ä‘áº§u: m_0 = 0, v_0 = 0
â†’ m_1 = 0.9 Ã— 0 + 0.1 Ã— grad = 0.1 Ã— grad  â† QuÃ¡ nhá»!
â†’ Bias correction: mÌ‚_1 = 0.1 Ã— grad / (1 - 0.9) = grad â† ÄÃºng
</code></pre>
<pre><code class="language-python">def adam_update(params, grads, m, v, t, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
    for key in params:
        # Update biased first moment estimate
        m[key] = beta1 * m[key] + (1 - beta1) * grads[key]
        # Update biased second moment estimate
        v[key] = beta2 * v[key] + (1 - beta2) * grads[key]**2

        # Bias correction
        m_hat = m[key] / (1 - beta1**t)
        v_hat = v[key] / (1 - beta2**t)

        # Update parameters
        params[key] -= lr * m_hat / (np.sqrt(v_hat) + eps)

    return params, m, v
</code></pre>
<h3 id="so-sanh-optimizers">So sÃ¡nh Optimizers</h3>
<table>
<thead>
<tr>
<th>Optimizer</th>
<th>Æ¯u Ä‘iá»ƒm</th>
<th>NhÆ°á»£c Ä‘iá»ƒm</th>
<th>Khi nÃ o dÃ¹ng</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SGD</strong></td>
<td>ÄÆ¡n giáº£n, generalize tá»‘t</td>
<td>Cháº­m, cáº§n tune LR ká»¹</td>
<td>Khi cáº§n model generalize tá»‘t</td>
</tr>
<tr>
<td><strong>SGD+Momentum</strong></td>
<td>Nhanh hÆ¡n SGD, smooth</td>
<td>Cáº§n tune momentum</td>
<td>Default cho vision tasks</td>
</tr>
<tr>
<td><strong>RMSProp</strong></td>
<td>Adaptive LR, tá»‘t cho RNN</td>
<td>CÃ³ thá»ƒ khÃ´ng há»™i tá»¥</td>
<td>RNNs, non-stationary</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>Nhanh, Ã­t cáº§n tune, robust</td>
<td>CÃ³ thá»ƒ generalize kÃ©m</td>
<td>Default choice, prototype</td>
</tr>
</tbody>
</table>
<p><strong>Khuyáº¿n nghá»‹ thá»±c táº¿:</strong></p>
<pre><code>1. Báº¯t Ä‘áº§u vá»›i Adam (lr=0.001) â†’ Nhanh, dá»…
2. Náº¿u cáº§n performance tá»‘t hÆ¡n â†’ SGD+Momentum (tune LR ká»¹)
3. RNN/LSTM â†’ Adam hoáº·c RMSProp
</code></pre>
<hr />
<h2 id="13-vanishing-exploding-gradients">13. VANISHING &amp; EXPLODING GRADIENTS</h2>
<h3 id="131-vanishing-gradient-gradient-bien-mat">13.1. Vanishing Gradient - Gradient biáº¿n máº¥t</h3>
<p><strong>Váº¥n Ä‘á»:</strong></p>
<pre><code>Network sÃ¢u 10 layers, gradient nhÃ¢n qua má»—i layer:

Layer 10 â†’ Layer 9:  grad = 0.5 Ã— grad
Layer 9 â†’ Layer 8:   grad = 0.5 Ã— 0.5 Ã— grad
...
Layer 2 â†’ Layer 1:   grad = 0.5^9 Ã— grad = 0.002 Ã— grad

â†’ Gradient á»Ÿ layers Ä‘áº§u gáº§n nhÆ° = 0
â†’ Layers Ä‘áº§u khÃ´ng há»c Ä‘Æ°á»£c!
</code></pre>
<p><strong>NguyÃªn nhÃ¢n chÃ­nh:</strong></p>
<p><strong>1. Sigmoid/Tanh cÃ³ derivative nhá»:</strong></p>
<pre><code class="language-python"># Sigmoid: Ïƒ'(x) = Ïƒ(x) Ã— (1 - Ïƒ(x))
# Maximum cá»§a Ïƒ' = 0.25 (khi x=0)

import numpy as np
x = np.linspace(-5, 5, 100)
sigmoid = 1 / (1 + np.exp(-x))
sigmoid_deriv = sigmoid * (1 - sigmoid)
print(f&quot;Max derivative: {sigmoid_deriv.max():.4f}&quot;)  # 0.25

# Qua 10 layers: 0.25^10 = 9.5e-7 â†’ Vanished!
</code></pre>
<p><strong>2. Weights khá»Ÿi táº¡o quÃ¡ nhá»:</strong></p>
<pre><code class="language-python"># Náº¿u weights nhá» â†’ activations nhá» â†’ gradients nhá»
W = np.random.randn(100, 100) * 0.001  # QuÃ¡ nhá»!
</code></pre>
<p><strong>Dáº¥u hiá»‡u phÃ¡t hiá»‡n:</strong></p>
<pre><code>Epoch 1: Loss = 2.5
Epoch 10: Loss = 2.5
Epoch 100: Loss = 2.49

# Hoáº·c kiá»ƒm tra gradient magnitude
for name, grad in grads.items():
    print(f&quot;{name}: grad norm = {np.linalg.norm(grad):.8f}&quot;)

# Output:
# W1: grad norm = 0.00000012  â† Vanishing!
# W2: grad norm = 0.00000089
# W10: grad norm = 0.45321234  â† OK
</code></pre>
<h3 id="132-exploding-gradient-gradient-bung-no">13.2. Exploding Gradient - Gradient bÃ¹ng ná»•</h3>
<p><strong>Váº¥n Ä‘á»:</strong></p>
<pre><code>NgÆ°á»£c vá»›i vanishing, gradient nhÃ¢n lÃªn qua má»—i layer:

grad = 2.0^10 = 1024 â†’ Weights bá»‹ update quÃ¡ máº¡nh â†’ NaN
</code></pre>
<p><strong>Dáº¥u hiá»‡u:</strong></p>
<pre><code>Epoch 1: Loss = 2.5
Epoch 2: Loss = 15.3
Epoch 3: Loss = 1e+15
Epoch 4: Loss = NaN

# Hoáº·c
RuntimeWarning: overflow encountered in exp
</code></pre>
<h3 id="133-giai-phap">13.3. Giáº£i phÃ¡p</h3>
<p><strong>1. ReLU thay Sigmoid/Tanh:</strong></p>
<pre><code class="language-python"># ReLU derivative = 1 (náº¿u x &gt; 0)
# â†’ Gradient khÃ´ng bá»‹ nhÃ¢n vá»›i sá»‘ &lt; 1
# â†’ KhÃ´ng vanish!

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x &gt; 0).astype(float)  # 0 hoáº·c 1
</code></pre>
<p><strong>2. Weight Initialization Ä‘Ãºng cÃ¡ch (Xavier, He):</strong></p>
<pre><code class="language-python"># Xavier (cho Sigmoid/Tanh)
W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)

# He (cho ReLU)
W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)
</code></pre>
<p><strong>3. Batch Normalization:</strong></p>
<pre><code class="language-python"># Normalize activations vá» mean=0, var=1
# â†’ Gradient stable qua cÃ¡c layers
def batch_norm(x, gamma, beta, eps=1e-5):
    mean = np.mean(x, axis=0)
    var = np.var(x, axis=0)
    x_norm = (x - mean) / np.sqrt(var + eps)
    return gamma * x_norm + beta
</code></pre>
<p><strong>4. Gradient Clipping (cho Exploding):</strong></p>
<pre><code class="language-python">def clip_gradients(grads, max_norm=5.0):
    total_norm = 0
    for g in grads.values():
        total_norm += np.sum(g ** 2)
    total_norm = np.sqrt(total_norm)

    if total_norm &gt; max_norm:
        scale = max_norm / total_norm
        for key in grads:
            grads[key] *= scale
    return grads
</code></pre>
<p><strong>5. Residual Connections (Skip connections):</strong></p>
<pre><code class="language-python"># ResNet: out = F(x) + x
# Gradient cÃ³ &quot;Ä‘Æ°á»ng táº¯t&quot; qua skip connection
# â†’ KhÃ´ng bá»‹ vanish qua nhiá»u layers

def residual_block(x, W1, W2):
    h = relu(x @ W1)
    out = h @ W2
    return out + x  # Skip connection
</code></pre>
<hr />
<h2 id="14-weight-initialization">14. WEIGHT INITIALIZATION</h2>
<h3 id="tai-sao-initialization-quan-trong">Táº¡i sao Initialization quan trá»ng?</h3>
<p><strong>Khá»Ÿi táº¡o sai:</strong></p>
<pre><code>Weights = 0:
â†’ Táº¥t cáº£ neurons output giá»‘ng nhau
â†’ Gradients giá»‘ng nhau
â†’ Weights update giá»‘ng nhau
â†’ Neurons khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c! (Symmetry problem)

Weights quÃ¡ lá»›n:
â†’ Activations bÃ£o hÃ²a (Sigmoid â†’ 0 hoáº·c 1)
â†’ Gradients â†’ 0 (Vanishing)

Weights quÃ¡ nhá»:
â†’ Activations â†’ 0
â†’ Gradients â†’ 0 (Vanishing)
</code></pre>
<h3 id="141-xavier-initialization-glorot-2010">14.1. Xavier Initialization (Glorot, 2010)</h3>
<p><strong>Má»¥c tiÃªu:</strong> Giá»¯ variance cá»§a activations á»•n Ä‘á»‹nh qua cÃ¡c layers.</p>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>W ~ N(0, ÏƒÂ²)
Ïƒ = âˆš(2 / (n_in + n_out))

Hoáº·c Uniform:
W ~ U(-âˆš(6/(n_in + n_out)), âˆš(6/(n_in + n_out)))
</code></pre>
<p><strong>Trá»±c giÃ¡c:</strong></p>
<pre><code>n_in = 100, n_out = 50

Ïƒ = âˆš(2 / (100 + 50)) = âˆš(2/150) = 0.115

â†’ Weights phÃ¢n phá»‘i N(0, 0.115Â²)
â†’ KhÃ´ng quÃ¡ lá»›n, khÃ´ng quÃ¡ nhá»
</code></pre>
<p><strong>Khi nÃ o dÃ¹ng:</strong> Sigmoid, Tanh activations.</p>
<pre><code class="language-python">def xavier_init(n_in, n_out):
    return np.random.randn(n_in, n_out) * np.sqrt(2.0 / (n_in + n_out))

# Hoáº·c
def xavier_uniform_init(n_in, n_out):
    limit = np.sqrt(6.0 / (n_in + n_out))
    return np.random.uniform(-limit, limit, (n_in, n_out))
</code></pre>
<h3 id="142-he-initialization-kaiming-2015">14.2. He Initialization (Kaiming, 2015)</h3>
<p><strong>Má»¥c tiÃªu:</strong> Tá»‘i Æ°u cho ReLU (account cho viá»‡c ReLU "kill" 50% neurons).</p>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>W ~ N(0, ÏƒÂ²)
Ïƒ = âˆš(2 / n_in)

# LÃ½ do: ReLU zero-out 50% activations
# â†’ Cáº§n variance lá»›n hÆ¡n Xavier 2x
</code></pre>
<p><strong>Khi nÃ o dÃ¹ng:</strong> ReLU, Leaky ReLU activations.</p>
<pre><code class="language-python">def he_init(n_in, n_out):
    return np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)
</code></pre>
<h3 id="so-sanh-xavier-vs-he">So sÃ¡nh Xavier vs He</h3>
<table>
<thead>
<tr>
<th>Initialization</th>
<th>CÃ´ng thá»©c</th>
<th>Activation</th>
<th>LÃ½ do</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Xavier</strong></td>
<td>âˆš(2/(n_in + n_out))</td>
<td>Sigmoid, Tanh</td>
<td>Symmetric activations</td>
</tr>
<tr>
<td><strong>He</strong></td>
<td>âˆš(2/n_in)</td>
<td>ReLU, Leaky ReLU</td>
<td>ReLU kills 50% neurons</td>
</tr>
</tbody>
</table>
<p><strong>Thá»±c nghiá»‡m:</strong></p>
<pre><code class="language-python">import numpy as np

def test_initialization(init_func, n_layers=10, n_neurons=256, activation='relu'):
    &quot;&quot;&quot;Test xem activations cÃ³ stable khÃ´ng qua nhiá»u layers&quot;&quot;&quot;
    x = np.random.randn(32, n_neurons)  # Batch of 32

    for i in range(n_layers):
        W = init_func(n_neurons, n_neurons)
        x = x @ W

        if activation == 'relu':
            x = np.maximum(0, x)
        elif activation == 'tanh':
            x = np.tanh(x)

        print(f&quot;Layer {i+1}: mean={x.mean():.4f}, std={x.std():.4f}&quot;)

print(&quot;=== Xavier with Tanh ===&quot;)
test_initialization(xavier_init, activation='tanh')

print(&quot;\n=== He with ReLU ===&quot;)
test_initialization(he_init, activation='relu')
</code></pre>
<p><strong>Output tá»‘t:</strong></p>
<pre><code>Layer 1: mean=0.01, std=0.98
Layer 5: mean=0.02, std=0.95
Layer 10: mean=0.01, std=0.92
â†’ Stable! âœ“
</code></pre>
<p><strong>Output xáº¥u:</strong></p>
<pre><code>Layer 1: mean=0.01, std=0.98
Layer 5: mean=0.00, std=0.12
Layer 10: mean=0.00, std=0.001
â†’ Vanishing! âœ—
</code></pre>
<hr />
<h2 id="15-hyperparameters-learning-rate-batch-size-epochs">15. HYPERPARAMETERS: LEARNING RATE, BATCH SIZE, EPOCHS</h2>
<h3 id="151-learning-rate">15.1. Learning Rate</h3>
<p><strong>Learning rate quÃ¡ nhá»:</strong></p>
<pre><code>Epoch 1: Loss = 2.5
Epoch 100: Loss = 2.3
Epoch 1000: Loss = 2.1
â†’ Há»™i tá»¥ quÃ¡ cháº­m, tá»‘n thá»i gian
</code></pre>
<p><strong>Learning rate quÃ¡ lá»›n:</strong></p>
<pre><code>Epoch 1: Loss = 2.5
Epoch 2: Loss = 5.3
Epoch 3: Loss = NaN
â†’ Diverge, khÃ´ng há»™i tá»¥
</code></pre>
<p><strong>Learning rate vá»«a pháº£i:</strong></p>
<pre><code>Epoch 1: Loss = 2.5
Epoch 10: Loss = 1.2
Epoch 50: Loss = 0.3
â†’ Há»™i tá»¥ smooth vÃ  nhanh âœ“
</code></pre>
<p><strong>GiÃ¡ trá»‹ thÆ°á»ng dÃ¹ng:</strong></p>
<pre><code>Adam:     0.001 (default)
SGD:      0.01 - 0.1
RMSProp:  0.001

Rule of thumb: Báº¯t Ä‘áº§u vá»›i 0.001, Ä‘iá»u chá»‰nh dá»±a trÃªn loss curve
</code></pre>
<h3 id="152-batch-size">15.2. Batch Size</h3>
<p><strong>Batch size nhá» (16, 32):</strong></p>
<pre><code>Æ¯u Ä‘iá»ƒm:
- Regularization tá»± nhiÃªn (noise trong gradient)
- Ãt memory
- CÃ³ thá»ƒ escape local minima

NhÆ°á»£c Ä‘iá»ƒm:
- Gradient noisy, khÃ´ng stable
- Cháº­m (khÃ´ng táº­n dá»¥ng GPU parallelism)
</code></pre>
<p><strong>Batch size lá»›n (256, 512, 1024):</strong></p>
<pre><code>Æ¯u Ä‘iá»ƒm:
- Gradient stable, accurate
- Nhanh (GPU parallelism)
- Training smooth

NhÆ°á»£c Ä‘iá»ƒm:
- Cáº§n nhiá»u memory
- CÃ³ thá»ƒ overfit
- Generalize kÃ©m hÆ¡n
</code></pre>
<p><strong>Má»‘i quan há»‡ Batch Size vÃ  Learning Rate:</strong></p>
<pre><code>Quy táº¯c Linear Scaling:
- Batch size Ã— 2 â†’ Learning rate Ã— 2

VÃ­ dá»¥:
- Batch=32, LR=0.001 â†’ Batch=64, LR=0.002
</code></pre>
<p><strong>GiÃ¡ trá»‹ thÆ°á»ng dÃ¹ng:</strong></p>
<pre><code>GPU memory 8GB:   32-64
GPU memory 16GB:  64-128
GPU memory 32GB+: 128-512

Default: 32 (balance giá»¯a speed vÃ  regularization)
</code></pre>
<h3 id="153-epochs">15.3. Epochs</h3>
<p><strong>Epochs lÃ  gÃ¬?</strong></p>
<pre><code>1 epoch = Model Ä‘Ã£ &quot;xem&quot; toÃ n bá»™ training data 1 láº§n

Training data: 10,000 samples
Batch size: 100
â†’ 1 epoch = 100 iterations (batches)
</code></pre>
<p><strong>Bao nhiÃªu epochs?</strong></p>
<pre><code>KhÃ´ng cÃ³ con sá»‘ cá»‘ Ä‘á»‹nh!
â†’ DÃ¹ng Early Stopping Ä‘á»ƒ tá»± Ä‘á»™ng dá»«ng

ThÆ°á»ng tháº¥y:
- Simple models: 10-50 epochs
- Deep networks: 100-300 epochs
- Large datasets: Ãt epochs hÆ¡n
</code></pre>
<p><strong>Early Stopping:</strong></p>
<pre><code>Epochs:   1 â”€â”€â”€â”€â”€â”€â”€ 50 â”€â”€â”€â”€â”€â”€â”€ 100 â”€â”€â”€â”€â”€â”€â”€ 150
Train:    High â”€â”€â”€â†’ Low â”€â”€â”€â”€â†’ Very Low â”€â”€â†’ Extremely Low
Val:      High â”€â”€â”€â†’ Low â”€â”€â”€â”€â†’ Low â”€â”€â”€â”€â”€â”€â”€â”€â†’ High (Overfitting!)
                            â†‘
                       Stop here!
</code></pre>
<h3 id="154-tuong-tac-giua-cac-hyperparameters">15.4. TÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c Hyperparameters</h3>
<pre><code>                    Learning Rate
                   /             \
                  /               \
            Batch Size â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs
                  \               /
                   \             /
                    Convergence
</code></pre>
<p><strong>Practical Guidelines:</strong></p>
<pre><code class="language-python"># Starting point
config = {
    'learning_rate': 0.001,  # Adam default
    'batch_size': 32,        # Good balance
    'epochs': 100,           # With early stopping
    'early_stopping_patience': 10
}

# Tuning order:
# 1. Batch size (dá»±a vÃ o GPU memory)
# 2. Learning rate (LR range test)
# 3. Epochs (vá»›i early stopping)
</code></pre>
<hr />
<h2 id="16-batch-normalization-dropout">16. BATCH NORMALIZATION &amp; DROPOUT</h2>
<h3 id="161-batch-normalization">16.1. Batch Normalization</h3>
<p><strong>Váº¥n Ä‘á» Internal Covariate Shift:</strong></p>
<pre><code>Layer 1 output thay Ä‘á»•i â†’ Layer 2 input thay Ä‘á»•i
â†’ Layer 2 pháº£i liÃªn tá»¥c &quot;thÃ­ch nghi&quot; vá»›i input má»›i
â†’ Training cháº­m vÃ  khÃ´ng stable
</code></pre>
<p><strong>Batch Norm lÃ m gÃ¬?</strong></p>
<pre><code>Normalize activations vá» mean=0, variance=1 trong má»—i mini-batch
â†’ Input cá»§a má»—i layer á»•n Ä‘á»‹nh
â†’ Training nhanh vÃ  stable hÆ¡n
</code></pre>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>Input: x (batch_size, features)

Step 1: TÃ­nh mean vÃ  variance trong batch
Î¼ = (1/m) Ã— Î£ x_i           â† Batch mean
ÏƒÂ² = (1/m) Ã— Î£ (x_i - Î¼)Â²   â† Batch variance

Step 2: Normalize
xÌ‚ = (x - Î¼) / âˆš(ÏƒÂ² + Îµ)    â† Normalized (mean=0, var=1)

Step 3: Scale vÃ  Shift (learnable parameters)
y = Î³ Ã— xÌ‚ + Î²              â† Î³, Î² Ä‘Æ°á»£c há»c

Î³, Î² cho phÃ©p network &quot;undo&quot; normalization náº¿u cáº§n
</code></pre>
<p><strong>Vá»‹ trÃ­ Ä‘áº·t BatchNorm:</strong></p>
<pre><code>ThÆ°á»ng Ä‘áº·t SAU linear layer, TRÆ¯á»šC activation:

Linear â†’ BatchNorm â†’ ReLU â†’ Linear â†’ BatchNorm â†’ ReLU â†’ ...

Má»™t sá»‘ paper Ä‘áº·t SAU activation, cáº£ hai Ä‘á»u work
</code></pre>
<p><strong>Code implementation:</strong></p>
<pre><code class="language-python">class BatchNorm:
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        self.eps = eps
        self.momentum = momentum

        # Learnable parameters
        self.gamma = np.ones(num_features)
        self.beta = np.zeros(num_features)

        # Running statistics (cho inference)
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)

    def forward(self, x, training=True):
        if training:
            # TÃ­nh batch statistics
            batch_mean = x.mean(axis=0)
            batch_var = x.var(axis=0)

            # Update running statistics (cho inference sau nÃ y)
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var

            # Normalize
            x_norm = (x - batch_mean) / np.sqrt(batch_var + self.eps)
        else:
            # Inference: dÃ¹ng running statistics
            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)

        # Scale and shift
        return self.gamma * x_norm + self.beta
</code></pre>
<p><strong>Lá»£i Ã­ch cá»§a BatchNorm:</strong></p>
<pre><code>1. Cho phÃ©p dÃ¹ng LR cao hÆ¡n â†’ Training nhanh hÆ¡n
2. Giáº£m sensitivity vá»›i initialization
3. Regularization effect (noise tá»« batch statistics)
4. GiÃºp gradient flow tá»‘t hÆ¡n â†’ Train Ä‘Æ°á»£c networks sÃ¢u
</code></pre>
<h3 id="162-dropout">16.2. Dropout</h3>
<p><strong>Ã tÆ°á»Ÿng:</strong></p>
<pre><code>Training: Randomly &quot;táº¯t&quot; má»™t sá»‘ neurons vá»›i probability p
â†’ Network khÃ´ng thá»ƒ rely vÃ o báº¥t ká»³ neuron nÃ o
â†’ Há»c features robust hÆ¡n
â†’ Ensemble effect (má»—i forward pass lÃ  1 sub-network khÃ¡c)
</code></pre>
<p><strong>Visualize:</strong></p>
<pre><code>Training (p=0.5):
x â—‹     â—‹ h1 (active)
     â†’  âœ— h2 (dropped, p=0.5)  â†’  â—‹ y
x â—‹     â—‹ h3 (active)

Má»—i forward pass, neurons khÃ¡c bá»‹ drop
â†’ TÆ°Æ¡ng Ä‘Æ°Æ¡ng train ensemble cá»§a 2^n networks!
</code></pre>
<p><strong>CÃ´ng thá»©c:</strong></p>
<pre><code>Training:
mask ~ Bernoulli(1-p)      â† Random mask
h_dropped = h Ã— mask / (1-p)  â† Drop vÃ  scale

Inference:
h_out = h                   â† KhÃ´ng drop, khÃ´ng scale
</code></pre>
<p><strong>Táº¡i sao chia cho (1-p)?</strong></p>
<pre><code>Training: 50% neurons bá»‹ drop â†’ Output nhá» hÆ¡n
Inference: 100% neurons active â†’ Output lá»›n hÆ¡n

Scale by 1/(1-p) Ä‘á»ƒ Ä‘áº£m báº£o expected output giá»‘ng nhau:
E[h_dropped] = h Ã— (1-p) / (1-p) = h = E[h_inference]
</code></pre>
<p><strong>Code implementation:</strong></p>
<pre><code class="language-python">def dropout(x, p=0.5, training=True):
    &quot;&quot;&quot;
    Args:
        x: input tensor
        p: probability of dropping (0.5 = drop 50%)
        training: True for training, False for inference
    &quot;&quot;&quot;
    if not training or p == 0:
        return x

    # Create random mask
    mask = (np.random.rand(*x.shape) &gt; p).astype(float)

    # Apply mask and scale
    return x * mask / (1 - p)

# Usage
h = relu(x @ W1 + b1)
h = dropout(h, p=0.5, training=True)  # Training
# h = dropout(h, p=0.5, training=False)  # Inference
y = h @ W2 + b2
</code></pre>
<p><strong>Dropout rate thÆ°á»ng dÃ¹ng:</strong></p>
<pre><code>Input layer:    0.2 (drop 20%)
Hidden layers:  0.5 (drop 50%)
Output layer:   0 (khÃ´ng drop)

Large networks: CÃ³ thá»ƒ dÃ¹ng 0.5-0.8
Small networks: 0.2-0.3 hoáº·c khÃ´ng dÃ¹ng
</code></pre>
<h3 id="so-sanh-batchnorm-vs-dropout">So sÃ¡nh BatchNorm vs Dropout</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>BatchNorm</th>
<th>Dropout</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Má»¥c Ä‘Ã­ch chÃ­nh</strong></td>
<td>Stabilize training</td>
<td>Regularization</td>
</tr>
<tr>
<td><strong>CÆ¡ cháº¿</strong></td>
<td>Normalize activations</td>
<td>Random drop neurons</td>
</tr>
<tr>
<td><strong>Training vs Inference</strong></td>
<td>KhÃ¡c (batch vs running stats)</td>
<td>KhÃ¡c (drop vs no drop)</td>
</tr>
<tr>
<td><strong>Khi nÃ o dÃ¹ng</strong></td>
<td>Háº§u nhÆ° luÃ´n</td>
<td>Khi overfitting</td>
</tr>
<tr>
<td><strong>Vá»‹ trÃ­</strong></td>
<td>Sau Linear, trÆ°á»›c Activation</td>
<td>Sau Activation</td>
</tr>
</tbody>
</table>
<p><strong>Thá»±c táº¿:</strong></p>
<pre><code class="language-python"># Modern architecture thÆ°á»ng dÃ¹ng cáº£ hai
h = linear(x)
h = batch_norm(h)
h = relu(h)
h = dropout(h, p=0.5)
</code></pre>
<hr />
<h2 id="17-oc-training-curves-chan-oan-van-e">17. Äá»ŒC TRAINING CURVES &amp; CHáº¨N ÄOÃN Váº¤N Äá»€</h2>
<h3 id="171-training-curve-co-ban">17.1. Training Curve cÆ¡ báº£n</h3>
<pre><code>Loss
  â”‚
  â”‚â•²
  â”‚ â•²___Train Loss
  â”‚    â•²___________
  â”‚         â•²______
  â”‚               â•²___
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs
</code></pre>
<p><strong>Healthy training:</strong>
- Loss giáº£m smooth
- Train vÃ  Val loss gáº§n nhau
- Äáº¡t plateau vÃ  dá»«ng</p>
<h3 id="172-cac-pattern-va-chan-oan">17.2. CÃ¡c Pattern vÃ  Cháº©n Ä‘oÃ¡n</h3>
<p><strong>Pattern 1: Underfitting</strong></p>
<pre><code>Loss
  â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€Train
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€Val
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs

Dáº¥u hiá»‡u:
- Train loss cao
- Val loss cao
- Cáº£ hai khÃ´ng giáº£m nhiá»u

NguyÃªn nhÃ¢n:
- Model quÃ¡ Ä‘Æ¡n giáº£n
- Learning rate quÃ¡ nhá»
- Training chÆ°a Ä‘á»§ lÃ¢u

Giáº£i phÃ¡p:
- TÄƒng model capacity (thÃªm layers/neurons)
- TÄƒng learning rate
- Train lÃ¢u hÆ¡n
- ThÃªm features
</code></pre>
<p><strong>Pattern 2: Overfitting</strong></p>
<pre><code>Loss
  â”‚
  â”‚â•²
  â”‚ â•²____Val (tÄƒng láº¡i)
  â”‚  â•²   â•±
  â”‚   â•²_â•±
  â”‚    â•²______Train (tiáº¿p tá»¥c giáº£m)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs
           â†‘
      Stop here!

Dáº¥u hiá»‡u:
- Train loss tiáº¿p tá»¥c giáº£m
- Val loss giáº£m rá»“i TÄ‚NG
- Gap giá»¯a train vÃ  val ngÃ y cÃ ng lá»›n

NguyÃªn nhÃ¢n:
- Model quÃ¡ phá»©c táº¡p
- Training quÃ¡ lÃ¢u
- KhÃ´ng Ä‘á»§ data

Giáº£i phÃ¡p:
- Early stopping
- Dropout, L2 regularization
- Data augmentation
- Giáº£m model size
</code></pre>
<p><strong>Pattern 3: Learning Rate quÃ¡ cao</strong></p>
<pre><code>Loss
  â”‚    â•±â•²
  â”‚   â•±  â•²
  â”‚  â•±    â•²
  â”‚ â•±      â•²
  â”‚â•±        â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs

Hoáº·c:
Loss = NaN sau vÃ i epochs

Dáº¥u hiá»‡u:
- Loss oscillate (dao Ä‘á»™ng)
- Loss tÄƒng thay vÃ¬ giáº£m
- Loss = NaN/Inf

Giáº£i phÃ¡p:
- Giáº£m learning rate (Ã·10)
- Gradient clipping
</code></pre>
<p><strong>Pattern 4: Learning Rate quÃ¡ tháº¥p</strong></p>
<pre><code>Loss
  â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚                â•²
  â”‚                 â•²____
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs
  (Cáº§n 1000 epochs Ä‘á»ƒ tháº¥y káº¿t quáº£)

Dáº¥u hiá»‡u:
- Loss giáº£m ráº¥t cháº­m
- Cáº§n ráº¥t nhiá»u epochs

Giáº£i phÃ¡p:
- TÄƒng learning rate (Ã—10)
- DÃ¹ng LR scheduler
</code></pre>
<p><strong>Pattern 5: Saddle Point / Local Minimum</strong></p>
<pre><code>Loss
  â”‚
  â”‚â•²
  â”‚ â•²
  â”‚  â•²_______________  â† Stuck!
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs

Dáº¥u hiá»‡u:
- Loss giáº£m rá»“i plateau sá»›m
- KhÃ´ng cÃ³ dáº¥u hiá»‡u overfitting

Giáº£i phÃ¡p:
- DÃ¹ng momentum
- Learning rate scheduling
- Random restarts
</code></pre>
<h3 id="173-code-e-monitor-training">17.3. Code Ä‘á»ƒ Monitor Training</h3>
<pre><code class="language-python">import matplotlib.pyplot as plt

def plot_training_curves(history):
    &quot;&quot;&quot;Plot training vÃ  validation curves&quot;&quot;&quot;
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Loss curve
    axes[0].plot(history['train_loss'], label='Train Loss', color='blue')
    axes[0].plot(history['val_loss'], label='Val Loss', color='orange')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Loss Curves')
    axes[0].legend()
    axes[0].grid(True)

    # Gap analysis
    gap = np.array(history['val_loss']) - np.array(history['train_loss'])
    axes[1].plot(gap, label='Val - Train Gap', color='red')
    axes[1].axhline(y=0, color='black', linestyle='--')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss Gap')
    axes[1].set_title('Overfitting Indicator')
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.show()

    # Diagnosis
    print(&quot;\n=== DIAGNOSIS ===&quot;)
    final_train = history['train_loss'][-1]
    final_val = history['val_loss'][-1]
    min_val = min(history['val_loss'])
    min_val_epoch = history['val_loss'].index(min_val)

    print(f&quot;Final Train Loss: {final_train:.4f}&quot;)
    print(f&quot;Final Val Loss: {final_val:.4f}&quot;)
    print(f&quot;Best Val Loss: {min_val:.4f} at epoch {min_val_epoch}&quot;)
    print(f&quot;Gap: {final_val - final_train:.4f}&quot;)

    if final_train &gt; 1.0 and final_val &gt; 1.0:
        print(&quot;â†’ UNDERFITTING: Consider larger model or more training&quot;)
    elif final_val &gt; final_train * 1.5:
        print(&quot;â†’ OVERFITTING: Consider regularization or early stopping&quot;)
    elif min_val_epoch &lt; len(history['val_loss']) - 10:
        print(&quot;â†’ OVERFITTING: Should have stopped at epoch&quot;, min_val_epoch)
    else:
        print(&quot;â†’ LOOKS GOOD!&quot;)

# Usage
history = {
    'train_loss': [2.5, 1.8, 1.2, 0.8, 0.5, 0.3, 0.2, 0.15, 0.1, 0.08],
    'val_loss': [2.6, 1.9, 1.3, 0.9, 0.7, 0.65, 0.7, 0.75, 0.8, 0.85]
}
plot_training_curves(history)
</code></pre>
<hr />
<h2 id="18-pytorch-training-loop">18. PYTORCH TRAINING LOOP</h2>
<h3 id="181-setup-co-ban">18.1. Setup cÆ¡ báº£n</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# Check GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f&quot;Using device: {device}&quot;)
</code></pre>
<h3 id="182-inh-nghia-model">18.2. Äá»‹nh nghÄ©a Model</h3>
<pre><code class="language-python">class SimpleNN(nn.Module):
    &quot;&quot;&quot;
    Neural Network 2 layers cho regression
    &quot;&quot;&quot;
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.5):
        super(SimpleNN, self).__init__()

        # Layers
        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=dropout_rate)
        self.layer2 = nn.Linear(hidden_dim, output_dim)

        # He initialization (tá»‘t cho ReLU)
        nn.init.kaiming_normal_(self.layer1.weight, mode='fan_in', nonlinearity='relu')
        nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='relu')

    def forward(self, x):
        # Layer 1: Linear â†’ BatchNorm â†’ ReLU â†’ Dropout
        x = self.layer1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout(x)

        # Layer 2: Linear (output)
        x = self.layer2(x)
        return x
</code></pre>
<h3 id="183-training-loop-ay-u">18.3. Training Loop Ä‘áº§y Ä‘á»§</h3>
<pre><code class="language-python">def train_model(model, train_loader, val_loader, epochs=100, lr=0.001, patience=10):
    &quot;&quot;&quot;
    Training loop Ä‘áº§y Ä‘á»§ vá»›i:
    - Early stopping
    - Learning rate scheduling
    - Gradient clipping
    - Logging
    &quot;&quot;&quot;
    # Move model to device
    model = model.to(device)

    # Loss function vÃ  optimizer
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )

    # Early stopping variables
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0

    # History
    history = {'train_loss': [], 'val_loss': []}

    # Training loop
    for epoch in range(epochs):
        # ==================== TRAINING ====================
        model.train()  # Set training mode (enables dropout, batch norm training)
        train_losses = []

        for batch_X, batch_y in train_loader:
            # Move to device
            batch_X = batch_X.to(device)
            batch_y = batch_y.to(device)

            # Zero gradients
            optimizer.zero_grad()

            # Forward pass
            predictions = model(batch_X)
            loss = criterion(predictions, batch_y)

            # Backward pass
            loss.backward()

            # Gradient clipping (trÃ¡nh exploding gradients)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # Update weights
            optimizer.step()

            train_losses.append(loss.item())

        avg_train_loss = np.mean(train_losses)

        # ==================== VALIDATION ====================
        model.eval()  # Set evaluation mode (disables dropout, uses running stats for batch norm)
        val_losses = []

        with torch.no_grad():  # KhÃ´ng cáº§n tÃ­nh gradient khi validation
            for batch_X, batch_y in val_loader:
                batch_X = batch_X.to(device)
                batch_y = batch_y.to(device)

                predictions = model(batch_X)
                loss = criterion(predictions, batch_y)
                val_losses.append(loss.item())

        avg_val_loss = np.mean(val_losses)

        # Save history
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)

        # Learning rate scheduling
        scheduler.step(avg_val_loss)

        # Logging
        current_lr = optimizer.param_groups[0]['lr']
        print(f&quot;Epoch {epoch+1}/{epochs} | &quot;
              f&quot;Train Loss: {avg_train_loss:.4f} | &quot;
              f&quot;Val Loss: {avg_val_loss:.4f} | &quot;
              f&quot;LR: {current_lr:.6f}&quot;)

        # ==================== EARLY STOPPING ====================
        if avg_val_loss &lt; best_val_loss:
            best_val_loss = avg_val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            print(f&quot;  â†’ New best model! Val Loss: {best_val_loss:.4f}&quot;)
        else:
            patience_counter += 1
            if patience_counter &gt;= patience:
                print(f&quot;\nEarly stopping at epoch {epoch+1}!&quot;)
                break

    # Restore best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f&quot;\nRestored best model with Val Loss: {best_val_loss:.4f}&quot;)

    return model, history
</code></pre>
<h3 id="184-complete-example">18.4. Complete Example</h3>
<pre><code class="language-python"># =============================================================================
# COMPLETE PYTORCH TRAINING EXAMPLE
# =============================================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt

# 1. Generate sample data
np.random.seed(42)
torch.manual_seed(42)

n_samples = 1000
n_features = 10

X = np.random.randn(n_samples, n_features).astype(np.float32)
y = (X[:, 0] * 2 + X[:, 1] * 0.5 + np.random.randn(n_samples) * 0.1).astype(np.float32)
y = y.reshape(-1, 1)

# 2. Train/Val/Test split
train_size = int(0.7 * n_samples)
val_size = int(0.15 * n_samples)

X_train, y_train = X[:train_size], y[:train_size]
X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]

# 3. Create DataLoaders
train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))
val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))
test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# 4. Create model
model = SimpleNN(
    input_dim=n_features,
    hidden_dim=64,
    output_dim=1,
    dropout_rate=0.3
)

print(&quot;Model architecture:&quot;)
print(model)
print(f&quot;\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}&quot;)

# 5. Train
model, history = train_model(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=100,
    lr=0.001,
    patience=15
)

# 6. Evaluate on test set
model.eval()
test_losses = []

with torch.no_grad():
    for batch_X, batch_y in test_loader:
        batch_X = batch_X.to(device)
        batch_y = batch_y.to(device)

        predictions = model(batch_X)
        loss = nn.MSELoss()(predictions, batch_y)
        test_losses.append(loss.item())

test_loss = np.mean(test_losses)
print(f&quot;\n{'='*50}&quot;)
print(f&quot;TEST SET EVALUATION&quot;)
print(f&quot;{'='*50}&quot;)
print(f&quot;Test Loss (MSE): {test_loss:.4f}&quot;)
print(f&quot;Test RMSE: {np.sqrt(test_loss):.4f}&quot;)

# 7. Plot training curves
plt.figure(figsize=(10, 5))
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Curves')
plt.legend()
plt.grid(True)
plt.show()

# 8. Save model
torch.save({
    'model_state_dict': model.state_dict(),
    'history': history,
    'test_loss': test_loss
}, 'model_checkpoint.pt')
print(&quot;\nModel saved to 'model_checkpoint.pt'&quot;)
</code></pre>
<h3 id="185-key-points-trong-pytorch-training">18.5. Key Points trong PyTorch Training</h3>
<pre><code class="language-python"># 1. model.train() vs model.eval()
model.train()  # Enables dropout, batch norm uses batch stats
model.eval()   # Disables dropout, batch norm uses running stats

# 2. torch.no_grad() - KhÃ´ng tÃ­nh gradient khi inference
with torch.no_grad():
    predictions = model(X_test)

# 3. optimizer.zero_grad() - PHáº¢I gá»i trÆ°á»›c má»—i backward
optimizer.zero_grad()  # Reset gradients
loss.backward()        # Compute gradients
optimizer.step()       # Update weights

# 4. .to(device) - Move data/model to GPU
model = model.to(device)
batch_X = batch_X.to(device)

# 5. Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 6. Save/Load model
torch.save(model.state_dict(), 'model.pt')           # Save
model.load_state_dict(torch.load('model.pt'))        # Load
</code></pre>
<hr />
<h2 id="19-bai-tap-thuc-hanh">19. BÃ€I Táº¬P THá»°C HÃ€NH</h2>
<h3 id="bai-tap-1-implement-perceptron">BÃ i táº­p 1: Implement Perceptron</h3>
<p><strong>Äá» bÃ i:</strong>
Implement perceptron tá»« Ä‘áº§u Ä‘á»ƒ dá»± Ä‘oÃ¡n FPT tÄƒng/giáº£m</p>
<p><strong>Gá»£i Ã½:</strong></p>
<pre><code class="language-python">class Perceptron:
    def __init__(self, input_dim):
        self.W = np.random.randn(input_dim) * 0.01
        self.b = 0

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, X):
        z = X @ self.W + self.b
        return self.sigmoid(z)

    def train(self, X, y, epochs=100, lr=0.01):
        # TODO: Implement training loop
        pass
</code></pre>
<p><strong>Kiá»ƒm tra:</strong>
- [ ] Implement Ä‘Æ°á»£c forward pass
- [ ] TÃ­nh Ä‘Æ°á»£c loss
- [ ] Implement Ä‘Æ°á»£c backward pass
- [ ] Train Ä‘Æ°á»£c model</p>
<hr />
<h3 id="bai-tap-2-build-2-layer-network">BÃ i táº­p 2: Build 2-Layer Network</h3>
<p><strong>Äá» bÃ i:</strong>
Build neural network 2 layers Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ FPT</p>
<p><strong>Gá»£i Ã½:</strong></p>
<pre><code class="language-python">class TwoLayerNet:
    def __init__(self, input_dim, hidden_dim, output_dim):
        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01
        self.b1 = np.zeros(hidden_dim)
        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01
        self.b2 = np.zeros(output_dim)

    def forward(self, X):
        # TODO: Implement
        pass

    def backward(self, X, y):
        # TODO: Implement
        pass

    def train(self, X, y, epochs=100, lr=0.01):
        # TODO: Implement
        pass
</code></pre>
<p><strong>Kiá»ƒm tra:</strong>
- [ ] Implement Ä‘Æ°á»£c forward pass
- [ ] Implement Ä‘Æ°á»£c backward pass
- [ ] Train Ä‘Æ°á»£c model
- [ ] So sÃ¡nh vá»›i Linear Regression</p>
<hr />
<h3 id="bai-tap-3-shape-debugging">BÃ i táº­p 3: Shape Debugging</h3>
<p><strong>Äá» bÃ i:</strong>
Debug cÃ¡c shape errors sau</p>
<p><strong>Case 1:</strong></p>
<pre><code class="language-python">X = np.random.randn(32, 5)  # 32 samples, 5 features
W = np.random.randn(10, 20)
Z = X @ W  # ERROR!

# Fix: ???
</code></pre>
<p><strong>Case 2:</strong></p>
<pre><code class="language-python">X = np.random.randn(32, 10)
W = np.random.randn(10, 5)
b = np.random.randn(10)  # Wrong shape!
Z = X @ W + b  # ERROR!

# Fix: ???
</code></pre>
<p><strong>Kiá»ƒm tra:</strong>
- [ ] Fix Ä‘Æ°á»£c case 1
- [ ] Fix Ä‘Æ°á»£c case 2
- [ ] Giáº£i thÃ­ch táº¡i sao bá»‹ lá»—i</p>
<hr />
<h3 id="bai-tap-4-full-training-pipeline">BÃ i táº­p 4: Full Training Pipeline</h3>
<p><strong>Äá» bÃ i:</strong>
Implement full training pipeline vá»›i:
1. Train/Val/Test split
2. Early stopping
3. Learning rate scheduling
4. Checkpointing</p>
<p><strong>YÃªu cáº§u:</strong>
- Train trÃªn FPT data
- Plot training curve
- Save best model
- Report final test metrics</p>
<p><strong>Kiá»ƒm tra:</strong>
- [ ] Implement Ä‘Æ°á»£c full pipeline
- [ ] Early stopping hoáº¡t Ä‘á»™ng
- [ ] LR scheduling hoáº¡t Ä‘á»™ng
- [ ] Model Ä‘Æ°á»£c save Ä‘Ãºng</p>
<hr />
<h3 id="bai-tap-5-debug-gradients">BÃ i táº­p 5: Debug Gradients</h3>
<p><strong>Äá» bÃ i:</strong>
Kiá»ƒm tra gradients báº±ng numerical gradient</p>
<p><strong>YÃªu cáº§u:</strong>
1. Implement numerical gradient
2. So sÃ¡nh vá»›i backprop gradient
3. Verify difference &lt; 1e-5</p>
<p><strong>Kiá»ƒm tra:</strong>
- [ ] Implement Ä‘Æ°á»£c numerical gradient
- [ ] So sÃ¡nh Ä‘Æ°á»£c vá»›i backprop
- [ ] Gradient Ä‘Ãºng (diff &lt; 1e-5)</p>
<hr />
<h3 id="bai-tap-6-learning-rate-tuning">BÃ i táº­p 6: Learning Rate Tuning</h3>
<p><strong>Äá» bÃ i:</strong>
TÃ¬m learning rate tá»‘t nháº¥t cho FPT</p>
<p><strong>YÃªu cáº§u:</strong>
1. Implement LR range test
2. Test LRs tá»« 1e-5 Ä‘áº¿n 10
3. Plot loss vs LR
4. Chá»n LR tá»‘t nháº¥t</p>
<p><strong>Kiá»ƒm tra:</strong>
- [ ] LR range test hoáº¡t Ä‘á»™ng
- [ ] Plot Ä‘Æ°á»£c loss curve
- [ ] TÃ¬m Ä‘Æ°á»£c LR tá»‘t nháº¥t
- [ ] Train vá»›i LR Ä‘Ã³ vÃ  compare</p>
<hr />
<h2 id="kiem-tra-hieu-bai">Kiá»ƒm tra hiá»ƒu bÃ i</h2>
<p>TrÆ°á»›c khi sang bÃ i tiáº¿p theo, hÃ£y Ä‘áº£m báº£o báº¡n:</p>
<p><strong>Pháº§n CÆ¡ Báº£n:</strong>
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c neural network lÃ  gÃ¬
- [ ] Hiá»ƒu Ä‘Æ°á»£c perceptron vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng
- [ ] Liá»‡t kÃª Ä‘Æ°á»£c cÃ¡c activation functions vÃ  khi nÃ o dÃ¹ng
- [ ] Hiá»ƒu Ä‘Æ°á»£c forward propagation
- [ ] Hiá»ƒu Ä‘Æ°á»£c backpropagation vÃ  chain rule
- [ ] Implement Ä‘Æ°á»£c gradient descent</p>
<p><strong>Pháº§n Tensor &amp; Shape:</strong>
- [ ] Hiá»ƒu Ä‘Æ°á»£c tensor lÃ  gÃ¬ (scalar, vector, matrix, 3D)
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c batch dimension
- [ ] TÃ­nh Ä‘Æ°á»£c shape qua má»—i layer
- [ ] Hiá»ƒu Ä‘Æ°á»£c broadcasting
- [ ] Debug Ä‘Æ°á»£c shape mismatch errors</p>
<p><strong>Pháº§n Optimization:</strong>
- [ ] So sÃ¡nh Ä‘Æ°á»£c SGD, Momentum, RMSProp, Adam
- [ ] Biáº¿t khi nÃ o dÃ¹ng optimizer nÃ o
- [ ] Hiá»ƒu Ä‘Æ°á»£c bias correction trong Adam</p>
<p><strong>Pháº§n Vanishing/Exploding Gradients:</strong>
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c nguyÃªn nhÃ¢n vanishing gradient
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c nguyÃªn nhÃ¢n exploding gradient
- [ ] Biáº¿t cÃ¡ch kháº¯c phá»¥c (ReLU, BatchNorm, initialization, clipping)</p>
<p><strong>Pháº§n Weight Initialization:</strong>
- [ ] PhÃ¢n biá»‡t Ä‘Æ°á»£c Xavier vs He initialization
- [ ] Biáº¿t khi nÃ o dÃ¹ng loáº¡i nÃ o</p>
<p><strong>Pháº§n Hyperparameters:</strong>
- [ ] Hiá»ƒu áº£nh hÆ°á»Ÿng cá»§a learning rate
- [ ] Hiá»ƒu áº£nh hÆ°á»Ÿng cá»§a batch size
- [ ] Biáº¿t má»‘i quan há»‡ giá»¯a batch size vÃ  learning rate</p>
<p><strong>Pháº§n BatchNorm &amp; Dropout:</strong>
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c BatchNorm lÃ m gÃ¬ vÃ  táº¡i sao hiá»‡u quáº£
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c Dropout lÃ m gÃ¬ vÃ  táº¡i sao hiá»‡u quáº£
- [ ] Biáº¿t khi nÃ o dÃ¹ng train mode vs eval mode</p>
<p><strong>Pháº§n Training Curves:</strong>
- [ ] Äá»c Ä‘Æ°á»£c training curve vÃ  nháº­n biáº¿t underfitting
- [ ] Äá»c Ä‘Æ°á»£c training curve vÃ  nháº­n biáº¿t overfitting
- [ ] Nháº­n biáº¿t Ä‘Æ°á»£c LR quÃ¡ cao/tháº¥p tá»« loss curve</p>
<p><strong>Pháº§n PyTorch:</strong>
- [ ] Viáº¿t Ä‘Æ°á»£c training loop cÆ¡ báº£n
- [ ] Hiá»ƒu model.train() vs model.eval()
- [ ] Hiá»ƒu torch.no_grad()
- [ ] Implement Ä‘Æ°á»£c early stopping vÃ  LR scheduling</p>
<p><strong>Náº¿u chÆ°a pass háº¿t checklist, Ä‘á»c láº¡i pháº§n tÆ°Æ¡ng á»©ng!</strong></p>
<hr />
<h2 id="tai-lieu-tham-khao">TÃ i liá»‡u tham kháº£o</h2>
<p><strong>Videos (YouTube):</strong>
- 3Blue1Brown: Neural Networks series (visualization tá»‘t nháº¥t)
- Andrew Ng: Deep Learning Specialization (Coursera)
- StatQuest: Gradient Descent, Backpropagation
- Andrej Karpathy: Neural Networks: Zero to Hero</p>
<p><strong>Books:</strong>
- "Deep Learning" - Goodfellow, Bengio, Courville
- "Neural Networks and Deep Learning" - Michael Nielsen (Free online)
- "Deep Learning with Python" - FranÃ§ois Chollet</p>
<p><strong>Papers:</strong>
- "Understanding the difficulty of training deep feedforward neural networks" - Glorot &amp; Bengio (2010) - Xavier init
- "Delving Deep into Rectifiers" - He et al. (2015) - He initialization
- "Batch Normalization" - Ioffe &amp; Szegedy (2015)
- "Adam: A Method for Stochastic Optimization" - Kingma &amp; Ba (2014)
- "Dropout: A Simple Way to Prevent Neural Networks from Overfitting" - Srivastava et al. (2014)</p>
<p><strong>Documentation:</strong>
- PyTorch Tutorials: https://pytorch.org/tutorials/
- PyTorch Documentation: https://pytorch.org/docs/stable/</p>
<p><strong>Debug Resources:</strong>
- "A Recipe for Training Neural Networks" - Andrej Karpathy (Blog post)</p>
<hr />
<h2 id="buoc-tiep-theo">BÆ°á»›c tiáº¿p theo</h2>
<p>Sau khi hoÃ n thÃ nh bÃ i nÃ y, sang:</p>
<p><strong>Tiáº¿p theo:</strong>
- <code>03_TIME_SERIES_FUNDAMENTALS.md</code> - Hiá»ƒu Ä‘áº·c thÃ¹ cá»§a time series
- <code>02_modeling/03_LSTM_GRU.md</code> - LSTM/GRU cho time series</p>
<p><strong>Thá»© tá»± há»c Ä‘á» xuáº¥t:</strong>
1. <code>01_MACHINE_LEARNING_BASICS.md</code>
2. <code>02_DEEP_LEARNING_BASICS.md</code> (báº¡n Ä‘ang á»Ÿ Ä‘Ã¢y)
3. <code>03_TIME_SERIES_FUNDAMENTALS.md</code> - Time Series Ä‘áº·c thÃ¹
4. <code>02_modeling/01_BASELINE_MODELS.md</code> - ARIMA, GARCH
5. <code>02_modeling/02_ML_MODELS.md</code> - XGBoost, LightGBM
6. <code>02_modeling/03_LSTM_GRU.md</code> - Deep Learning cho Time Series</p>
  </main>
</body>
</html>
