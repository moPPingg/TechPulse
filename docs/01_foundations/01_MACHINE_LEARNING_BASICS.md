# ğŸ“ MACHINE LEARNING CÆ  Báº¢N CHO TIME SERIES
## Há»c Ä‘á»ƒ hiá»ƒu - KhÃ´ng pháº£i Ä‘á»ƒ nhá»›

---

## ğŸ“š Má»¤C Lá»¤C

1. [Machine Learning lÃ  gÃ¬?](#1-machine-learning-lÃ -gÃ¬)
2. [Supervised Learning](#2-supervised-learning)
3. [Regression vs Classification](#3-regression-vs-classification)
4. [Train/Test Split](#4-traintest-split)
5. [Overfitting vs Underfitting](#5-overfitting-vs-underfitting)
6. [Metrics Ä‘Ã¡nh giÃ¡](#6-metrics-Ä‘Ã¡nh-giÃ¡)
7. [BÃ i táº­p thá»±c hÃ nh](#7-bÃ i-táº­p-thá»±c-hÃ nh)

---

## 1. MACHINE LEARNING LÃ€ GÃŒ?

### ğŸ¤” TÃ¬nh huá»‘ng Ä‘á»i thÆ°á»ng

**Báº¡n muá»‘n dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u ngÃ y mai:**

**CÃ¡ch truyá»n thá»‘ng (láº­p trÃ¬nh thÃ´ng thÆ°á»ng):**
```
Náº¿u giÃ¡ hÃ´m nay > giÃ¡ hÃ´m qua:
    â†’ NgÃ y mai tÄƒng
Náº¿u khÃ´ng:
    â†’ NgÃ y mai giáº£m
```
âŒ **Váº¥n Ä‘á»:** QuÃ¡ Ä‘Æ¡n giáº£n, khÃ´ng chÃ­nh xÃ¡c

**CÃ¡ch Machine Learning:**
```
1. Cho mÃ¡y xem 10 nÄƒm dá»¯ liá»‡u lá»‹ch sá»­
2. MÃ¡y tá»± há»c pattern (quy luáº­t)
3. MÃ¡y dá»± Ä‘oÃ¡n dá»±a trÃªn pattern Ä‘Ã£ há»c
```
âœ… **Æ¯u Ä‘iá»ƒm:** MÃ¡y tá»± há»c, khÃ´ng cáº§n viáº¿t rules phá»©c táº¡p

### ğŸ“– Äá»‹nh nghÄ©a Ä‘Æ¡n giáº£n

> **Machine Learning = Dáº¡y mÃ¡y há»c tá»« dá»¯ liá»‡u**

Thay vÃ¬ báº¡n nÃ³i cho mÃ¡y "lÃ m tháº¿ nÃ y, lÃ m tháº¿ kia", báº¡n cho mÃ¡y xem nhiá»u vÃ­ dá»¥, mÃ¡y tá»± há»c cÃ¡ch lÃ m.

### ğŸ¯ VÃ­ dá»¥ cá»¥ thá»ƒ

**BÃ i toÃ¡n:** Dá»± Ä‘oÃ¡n giÃ¡ FPT ngÃ y mai

**Input (X):**
- GiÃ¡ hÃ´m nay: 100,000
- GiÃ¡ hÃ´m qua: 98,000
- Volume hÃ´m nay: 1,500,000
- RSI: 65
- MACD: 0.5

**Output (y):**
- GiÃ¡ ngÃ y mai: 102,000 (dá»± Ä‘oÃ¡n)

**ML lÃ m gÃ¬?**
```
ML model há»c tá»« 10 nÄƒm dá»¯ liá»‡u:
"Khi RSI > 60 vÃ  MACD > 0 vÃ  giÃ¡ tÄƒng 2 ngÃ y liÃªn tiáº¿p
 â†’ NgÃ y mai thÆ°á»ng tÄƒng thÃªm 1-2%"
```

---

## 2. SUPERVISED LEARNING

### ğŸ“ Há»c cÃ³ giÃ¡m sÃ¡t

**Giá»‘ng nhÆ° há»c á»Ÿ trÆ°á»ng:**
- Tháº§y cho bÃ i táº­p (X) vÃ  Ä‘Ã¡p Ã¡n (y)
- Há»c sinh lÃ m bÃ i, so sÃ¡nh vá»›i Ä‘Ã¡p Ã¡n
- Sai â†’ sá»­a, Ä‘Ãºng â†’ nhá»›
- Láº·p láº¡i cho Ä‘áº¿n khi há»c sinh lÃ m Ä‘Ãºng

**Trong ML:**
- Báº¡n cho mÃ¡y dá»¯ liá»‡u (X) vÃ  káº¿t quáº£ Ä‘Ãºng (y)
- MÃ¡y dá»± Ä‘oÃ¡n, so sÃ¡nh vá»›i káº¿t quáº£ Ä‘Ãºng
- Sai â†’ Ä‘iá»u chá»‰nh model
- ÄÃºng â†’ nhá»› pattern
- Láº·p láº¡i cho Ä‘áº¿n khi mÃ¡y dá»± Ä‘oÃ¡n tá»‘t

### ğŸ“Š VÃ­ dá»¥ vá»›i dá»¯ liá»‡u FPT

**Dá»¯ liá»‡u training (mÃ¡y há»c tá»« Ä‘Ã¢y):**

| NgÃ y | Close (X) | MA20 (X) | RSI (X) | Close ngÃ y mai (y) |
|------|-----------|----------|---------|-------------------|
| 1/1  | 100       | 95       | 50      | 102 âœ… (Ä‘Ã¡p Ã¡n)   |
| 1/2  | 102       | 96       | 55      | 105 âœ…            |
| 1/3  | 105       | 97       | 60      | 103 âœ…            |
| ...  | ...       | ...      | ...     | ...               |

**MÃ¡y há»c:**
```
Láº§n 1: Dá»± Ä‘oÃ¡n 1/1 â†’ 98 (sai, Ä‘Ã¡p Ã¡n lÃ  102)
       â†’ Äiá»u chá»‰nh model

Láº§n 2: Dá»± Ä‘oÃ¡n 1/1 â†’ 101 (gáº§n hÆ¡n!)
       â†’ Äiá»u chá»‰nh tiáº¿p

Láº§n 3: Dá»± Ä‘oÃ¡n 1/1 â†’ 102 (Ä‘Ãºng!)
       â†’ Nhá»› pattern nÃ y
```

### ğŸ”‘ CÃ´ng thá»©c tá»•ng quÃ¡t

```
Supervised Learning:
- Input: X (features)
- Output: y (target/label)
- Goal: Há»c hÃ m f sao cho f(X) â‰ˆ y

f(X) = y
â†‘      â†‘
Model  Káº¿t quáº£ thá»±c táº¿
```

---

## 3. REGRESSION VS CLASSIFICATION

### ğŸ¯ PhÃ¢n biá»‡t 2 loáº¡i bÃ i toÃ¡n

**REGRESSION (Há»“i quy):**
- Dá»± Ä‘oÃ¡n **Sá» LIÃŠN Tá»¤C**
- VÃ­ dá»¥: Dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u (100.5, 102.3, 98.7, ...)

**CLASSIFICATION (PhÃ¢n loáº¡i):**
- Dá»± Ä‘oÃ¡n **NHÃƒN Rá»œI Ráº C**
- VÃ­ dá»¥: Dá»± Ä‘oÃ¡n tÄƒng/giáº£m (TÄƒng, Giáº£m)

### ğŸ“Š VÃ­ dá»¥ cá»¥ thá»ƒ

**BÃ i toÃ¡n 1: Dá»± Ä‘oÃ¡n giÃ¡ FPT ngÃ y mai**
```
Input:  close=100, ma20=95, rsi=60
Output: 102.5 (sá»‘ liÃªn tá»¥c)
â†’ REGRESSION
```

**BÃ i toÃ¡n 2: Dá»± Ä‘oÃ¡n FPT tÄƒng hay giáº£m**
```
Input:  close=100, ma20=95, rsi=60
Output: "TÄƒng" (nhÃ£n rá»i ráº¡c)
â†’ CLASSIFICATION
```

### ğŸ” Dá»± Ã¡n TechPulse dÃ¹ng gÃ¬?

**Chá»§ yáº¿u: REGRESSION**
- Dá»± Ä‘oÃ¡n giÃ¡ cá»¥ thá»ƒ: 102,000 VNÄ
- Dá»± Ä‘oÃ¡n return: +2.5%

**CÃ³ thá»ƒ dÃ¹ng: CLASSIFICATION**
- Dá»± Ä‘oÃ¡n tÄƒng/giáº£m (binary)
- Dá»± Ä‘oÃ¡n má»©c Ä‘á»™: TÄƒng máº¡nh/TÄƒng nháº¹/Giáº£m nháº¹/Giáº£m máº¡nh

---

## 4. TRAIN/TEST SPLIT

### ğŸ“ Táº¡i sao cáº§n chia dá»¯ liá»‡u?

**VÃ­ dá»¥ há»c sinh:**
- Há»c tá»« sÃ¡ch giÃ¡o khoa (training data)
- Thi bÃ i má»›i chÆ°a tá»«ng tháº¥y (test data)
- Náº¿u chá»‰ há»c váº¹t sÃ¡ch â†’ thi bÃ i má»›i sáº½ kÃ©m

**Trong ML:**
- Train trÃªn dá»¯ liá»‡u cÅ© (2015-2023)
- Test trÃªn dá»¯ liá»‡u má»›i (2024)
- Náº¿u model chá»‰ nhá»› training data â†’ test kÃ©m (overfitting)

### ğŸ“Š CÃ¡ch chia dá»¯ liá»‡u

**Quy táº¯c chung:**
```
Training set: 70-80%  â†’ MÃ¡y há»c tá»« Ä‘Ã¢y
Test set:     20-30%  â†’ ÄÃ¡nh giÃ¡ model
```

**Vá»›i Time Series (QUAN TRá»ŒNG!):**
```
âŒ SAI: Chia ngáº«u nhiÃªn
   [2015][2020][2018][2023] â†’ Training
   [2019][2021][2017][2024] â†’ Test
   (LÃ½ do: KhÃ´ng thá»ƒ dÃ¹ng tÆ°Æ¡ng lai dá»± Ä‘oÃ¡n quÃ¡ khá»©!)

âœ… ÄÃšNG: Chia theo thá»i gian
   [2015][2016][2017][2018][2019][2020][2021][2022] â†’ Training
   [2023][2024] â†’ Test
   (LÃ½ do: Giá»‘ng thá»±c táº¿ - dÃ¹ng quÃ¡ khá»© dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai)
```

### ğŸ”§ CÃ¡ch implement

**BÆ°á»›c 1: Sáº¯p xáº¿p theo thá»i gian**
```python
# Giáº£ sá»­ df cÃ³ cá»™t 'date'
df = df.sort_values('date')
```

**BÆ°á»›c 2: Chia 80/20**
```python
# TÃ­nh Ä‘iá»ƒm chia
split_idx = int(len(df) * 0.8)

# Chia data
train_df = df[:split_idx]   # 80% Ä‘áº§u
test_df = df[split_idx:]     # 20% cuá»‘i
```

**BÆ°á»›c 3: TÃ¡ch X vÃ  y**
```python
# Features (X)
X_train = train_df[['close', 'ma20', 'rsi', 'macd']]
X_test = test_df[['close', 'ma20', 'rsi', 'macd']]

# Target (y) - giÃ¡ ngÃ y mai
y_train = train_df['close'].shift(-1)  # Shift Ä‘á»ƒ láº¥y giÃ¡ ngÃ y mai
y_test = test_df['close'].shift(-1)
```

---

## 5. OVERFITTING VS UNDERFITTING

### ğŸ¯ Hiá»ƒu qua vÃ­ dá»¥ há»c sinh

**UNDERFITTING (Há»c kÃ©m):**
```
Há»c sinh chá»‰ há»c: "Náº¿u giÃ¡ tÄƒng â†’ ngÃ y mai tÄƒng"
â†’ QuÃ¡ Ä‘Æ¡n giáº£n, khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c pattern phá»©c táº¡p
â†’ Äiá»ƒm tháº¥p cáº£ training láº«n test
```

**OVERFITTING (Há»c váº¹t):**
```
Há»c sinh nhá»› tá»«ng cÃ¢u trong sÃ¡ch:
"NgÃ y 1/1/2020 giÃ¡ 100 â†’ ngÃ y 2/1 giÃ¡ 102"
"NgÃ y 2/1/2020 giÃ¡ 102 â†’ ngÃ y 3/1 giÃ¡ 105"
â†’ Nhá»› chi tiáº¿t quÃ¡, khÃ´ng tá»•ng quÃ¡t
â†’ Äiá»ƒm cao training, Ä‘iá»ƒm tháº¥p test
```

**GOOD FIT (Há»c tá»‘t):**
```
Há»c sinh hiá»ƒu pattern:
"Khi RSI > 70 vÃ  volume tÄƒng Ä‘á»™t biáº¿n â†’ thÆ°á»ng giáº£m"
â†’ Tá»•ng quÃ¡t hÃ³a tá»‘t
â†’ Äiá»ƒm cao cáº£ training láº«n test
```

### ğŸ“Š Biá»ƒu Ä‘á»“ minh há»a

```
Error
  â†‘
  â”‚     Underfitting        Good Fit      Overfitting
  â”‚         â•±â•²                 â•±â•²             â•±â•²
  â”‚        â•±  â•²               â•±  â•²           â•±  â•²
  â”‚       â•±    â•²             â•±    â•²         â•±    â•²
  â”‚      â•±      â•²           â•±      â•²       â•±      â•²
  â”‚     â•±        â•²         â•±        â•²     â•±        â•²___
  â”‚    â•±          â•²       â•±          â•²   â•±            Test Error
  â”‚___â•±____________â•²_____â•±____________â•²_â•±_____________
  â”‚                                    â•²
  â”‚                                     â•²___Training Error
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
                Model Complexity
```

### ğŸ” CÃ¡ch phÃ¡t hiá»‡n

**Dáº¥u hiá»‡u Underfitting:**
- Training error cao
- Test error cao
- Model quÃ¡ Ä‘Æ¡n giáº£n

**Dáº¥u hiá»‡u Overfitting:**
- Training error ráº¥t tháº¥p (~0)
- Test error cao
- ChÃªnh lá»‡ch lá»›n giá»¯a train vÃ  test

**Dáº¥u hiá»‡u Good Fit:**
- Training error tháº¥p
- Test error tháº¥p
- ChÃªnh lá»‡ch nhá» giá»¯a train vÃ  test

### ğŸ’¡ CÃ¡ch kháº¯c phá»¥c

**Underfitting â†’ TÄƒng Ä‘á»™ phá»©c táº¡p:**
- ThÃªm features
- DÃ¹ng model phá»©c táº¡p hÆ¡n (LSTM thay vÃ¬ Linear)
- TÄƒng sá»‘ epochs training

**Overfitting â†’ Giáº£m Ä‘á»™ phá»©c táº¡p:**
- Regularization (L1, L2)
- Dropout (vá»›i Neural Networks)
- Early stopping
- ThÃªm dá»¯ liá»‡u training
- Giáº£m sá»‘ features

---

## 6. METRICS ÄÃNH GIÃ

### ğŸ“Š Táº¡i sao cáº§n metrics?

**KhÃ´ng cÃ³ metrics:**
```
Báº¡n: "Model cá»§a tÃ´i tá»‘t!"
Reviewer: "Tá»‘t nhÆ° tháº¿ nÃ o? Báº±ng chá»©ng?"
Báº¡n: "á»ªm... nhÃ¬n cÃ³ váº» tá»‘t..."
â†’ KhÃ´ng thuyáº¿t phá»¥c!
```

**CÃ³ metrics:**
```
Báº¡n: "Model cá»§a tÃ´i cÃ³ MSE = 0.5, MAE = 0.3"
Reviewer: "So vá»›i baseline?"
Báº¡n: "Baseline MSE = 1.2, tÃ´i giáº£m Ä‘Æ°á»£c 58%"
â†’ Thuyáº¿t phá»¥c!
```

### ğŸ¯ CÃ¡c metrics quan trá»ng

#### **1. MSE (Mean Squared Error)**

**CÃ´ng thá»©c:**
```
MSE = (1/n) Ã— Î£(y_true - y_pred)Â²

VÃ­ dá»¥:
y_true = [100, 102, 105]
y_pred = [98,  103, 104]
error  = [2,   -1,  1]
squared= [4,   1,   1]
MSE    = (4 + 1 + 1) / 3 = 2.0
```

**Ã nghÄ©a:**
- Äo "sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh"
- Pháº¡t náº·ng cÃ¡c lá»—i lá»›n (vÃ¬ bÃ¬nh phÆ°Æ¡ng)
- ÄÆ¡n vá»‹: (Ä‘Æ¡n vá»‹ cá»§a y)Â²

**Khi nÃ o dÃ¹ng:**
- Khi muá»‘n pháº¡t náº·ng outliers
- Khi sai sá»‘ lá»›n quan trá»ng hÆ¡n sai sá»‘ nhá»

#### **2. MAE (Mean Absolute Error)**

**CÃ´ng thá»©c:**
```
MAE = (1/n) Ã— Î£|y_true - y_pred|

VÃ­ dá»¥:
y_true = [100, 102, 105]
y_pred = [98,  103, 104]
error  = [2,   -1,  1]
abs    = [2,   1,   1]
MAE    = (2 + 1 + 1) / 3 = 1.33
```

**Ã nghÄ©a:**
- Äo "sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh"
- Pháº¡t Ä‘á»u cÃ¡c lá»—i (khÃ´ng bÃ¬nh phÆ°Æ¡ng)
- ÄÆ¡n vá»‹: Ä‘Æ¡n vá»‹ cá»§a y

**Khi nÃ o dÃ¹ng:**
- Khi muá»‘n Ä‘á»‘i xá»­ cÃ´ng báº±ng vá»›i má»i lá»—i
- Dá»… interpret hÆ¡n MSE

#### **3. RMSE (Root Mean Squared Error)**

**CÃ´ng thá»©c:**
```
RMSE = âˆšMSE

VÃ­ dá»¥:
MSE = 2.0
RMSE = âˆš2.0 = 1.41
```

**Ã nghÄ©a:**
- Giá»‘ng MSE nhÆ°ng Ä‘Æ¡n vá»‹ giá»‘ng y
- Dá»… interpret hÆ¡n MSE

#### **4. MAPE (Mean Absolute Percentage Error)**

**CÃ´ng thá»©c:**
```
MAPE = (1/n) Ã— Î£|((y_true - y_pred) / y_true)| Ã— 100%

VÃ­ dá»¥:
y_true = [100, 102, 105]
y_pred = [98,  103, 104]
error% = [2%,  -0.98%, 0.95%]
abs%   = [2%,  0.98%,  0.95%]
MAPE   = (2 + 0.98 + 0.95) / 3 = 1.31%
```

**Ã nghÄ©a:**
- Äo "sai sá»‘ pháº§n trÄƒm trung bÃ¬nh"
- KhÃ´ng phá»¥ thuá»™c vÃ o scale cá»§a y
- ÄÆ¡n vá»‹: %

**Khi nÃ o dÃ¹ng:**
- Khi muá»‘n so sÃ¡nh models trÃªn datasets khÃ¡c nhau
- Khi muá»‘n metric dá»… hiá»ƒu (%)

### ğŸ“Š So sÃ¡nh cÃ¡c metrics

| Metric | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Khi nÃ o dÃ¹ng |
|--------|---------|------------|--------------|
| **MSE** | Pháº¡t náº·ng outliers | KhÃ³ interpret, Ä‘Æ¡n vá»‹ láº¡ | Khi outliers quan trá»ng |
| **MAE** | Dá»… hiá»ƒu, Ä‘Æ¡n vá»‹ rÃµ | KhÃ´ng pháº¡t náº·ng outliers | Khi muá»‘n metric Ä‘Æ¡n giáº£n |
| **RMSE** | Dá»… hiá»ƒu hÆ¡n MSE | Váº«n pháº¡t náº·ng outliers | Khi muá»‘n MSE nhÆ°ng dá»… Ä‘á»c |
| **MAPE** | Scale-free, dá»… so sÃ¡nh | Lá»—i khi y_true = 0 | Khi so sÃ¡nh nhiá»u datasets |

### ğŸ’¡ Metrics nÃ o cho TechPulse?

**Khuyáº¿n nghá»‹:**
1. **MAE** - Metric chÃ­nh (dá»… hiá»ƒu, á»•n Ä‘á»‹nh)
2. **RMSE** - Metric phá»¥ (pháº¡t outliers)
3. **MAPE** - So sÃ¡nh giá»¯a cÃ¡c mÃ£ cá»• phiáº¿u

---

## 7. BÃ€I Táº¬P THá»°C HÃ€NH

### ğŸ¯ BÃ i táº­p 1: Hiá»ƒu Train/Test Split

**Äá» bÃ i:**
Báº¡n cÃ³ dá»¯ liá»‡u FPT tá»« 2020-2024 (1,250 dÃ²ng). HÃ£y:
1. Chia 80/20 train/test
2. TÃ­nh sá»‘ dÃ²ng má»—i set
3. XÃ¡c Ä‘á»‹nh khoáº£ng thá»i gian má»—i set

**Gá»£i Ã½:**
```python
# BÆ°á»›c 1: Load data
df = pd.read_csv('data/features/vn30/FPT.csv')
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date')

# BÆ°á»›c 2: Chia
split_idx = int(len(df) * 0.8)
train_df = df[:split_idx]
test_df = df[split_idx:]

# BÆ°á»›c 3: In thÃ´ng tin
print(f"Training: {len(train_df)} dÃ²ng, tá»« {train_df['date'].min()} Ä‘áº¿n {train_df['date'].max()}")
print(f"Test: {len(test_df)} dÃ²ng, tá»« {test_df['date'].min()} Ä‘áº¿n {test_df['date'].max()}")
```

**Kiá»ƒm tra:**
- [ ] Training set cÃ³ ~1,000 dÃ²ng (80%)
- [ ] Test set cÃ³ ~250 dÃ²ng (20%)
- [ ] Training set Ä‘áº¿n trÆ°á»›c test set theo thá»i gian

---

### ğŸ¯ BÃ i táº­p 2: TÃ­nh Metrics

**Äá» bÃ i:**
Cho predictions vÃ  actual values, tÃ­nh MSE, MAE, RMSE, MAPE

```python
y_true = [100, 105, 102, 108, 110]
y_pred = [98,  107, 101, 110, 108]
```

**Gá»£i Ã½:**
```python
import numpy as np

# MSE
mse = np.mean((y_true - y_pred) ** 2)

# MAE
mae = np.mean(np.abs(y_true - y_pred))

# RMSE
rmse = np.sqrt(mse)

# MAPE
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

print(f"MSE: {mse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAPE: {mape:.2f}%")
```

**ÄÃ¡p Ã¡n:**
- MSE: 4.4
- MAE: 2.0
- RMSE: 2.1
- MAPE: 1.94%

**Kiá»ƒm tra:**
- [ ] TÃ­nh Ä‘Æ°á»£c MSE Ä‘Ãºng
- [ ] TÃ­nh Ä‘Æ°á»£c MAE Ä‘Ãºng
- [ ] Hiá»ƒu táº¡i sao MSE > MAE
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c MAPE = 1.94% nghÄ©a lÃ  gÃ¬

---

### ğŸ¯ BÃ i táº­p 3: Implement Linear Regression

**Äá» bÃ i:**
DÃ¹ng Linear Regression dá»± Ä‘oÃ¡n giÃ¡ FPT ngÃ y mai

**BÆ°á»›c 1: Chuáº©n bá»‹ data**
```python
# Load features data
df = pd.read_csv('data/features/vn30/FPT.csv')

# Chá»n features
features = ['close', 'ma_20', 'rsi_14', 'macd']
X = df[features]

# Target: giÃ¡ ngÃ y mai
y = df['close'].shift(-1)

# Drop NaN
df_clean = pd.concat([X, y], axis=1).dropna()
X = df_clean[features]
y = df_clean['close']

# Train/test split
split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]
```

**BÆ°á»›c 2: Train model**
```python
from sklearn.linear_model import LinearRegression

# Táº¡o model
model = LinearRegression()

# Train
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
```

**BÆ°á»›c 3: Evaluate**
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"MSE: {mse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
```

**BÆ°á»›c 4: Visualize**
```python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(y_test.values, label='Actual', alpha=0.7)
plt.plot(y_pred, label='Predicted', alpha=0.7)
plt.legend()
plt.title('FPT Price Prediction - Linear Regression')
plt.xlabel('Days')
plt.ylabel('Price')
plt.show()
```

**Kiá»ƒm tra:**
- [ ] Model train thÃ nh cÃ´ng
- [ ] TÃ­nh Ä‘Æ°á»£c metrics
- [ ] Váº½ Ä‘Æ°á»£c biá»ƒu Ä‘á»“
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c káº¿t quáº£

---

## âœ… KIá»‚M TRA HIá»‚U BÃ€I

TrÆ°á»›c khi sang bÃ i tiáº¿p theo, hÃ£y Ä‘áº£m báº£o báº¡n:

- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c Machine Learning lÃ  gÃ¬ báº±ng lá»i cá»§a mÃ¬nh
- [ ] PhÃ¢n biá»‡t Ä‘Æ°á»£c Supervised vs Unsupervised Learning
- [ ] PhÃ¢n biá»‡t Ä‘Æ°á»£c Regression vs Classification
- [ ] Hiá»ƒu táº¡i sao pháº£i chia train/test vá»›i time series
- [ ] PhÃ¢n biá»‡t Ä‘Æ°á»£c Overfitting vs Underfitting
- [ ] TÃ­nh Ä‘Æ°á»£c MSE, MAE, RMSE, MAPE báº±ng tay
- [ ] Implement Ä‘Æ°á»£c Linear Regression cho FPT
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c káº¿t quáº£ dá»± bÃ¡o

**Náº¿u chÆ°a pass háº¿t checklist, Ä‘á»c láº¡i pháº§n tÆ°Æ¡ng á»©ng!**

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

**Videos (YouTube):**
- StatQuest: Machine Learning Fundamentals
- 3Blue1Brown: Neural Networks series
- Krish Naik: Machine Learning Playlist

**Courses:**
- Andrew Ng - Machine Learning (Coursera)
- Fast.ai - Practical Deep Learning

**Books:**
- "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
- "Introduction to Statistical Learning" - James et al.

---

## ğŸš€ BÆ¯á»šC TIáº¾P THEO

Sau khi hoÃ n thÃ nh bÃ i nÃ y, sang:
- `03_TIME_SERIES_FUNDAMENTALS.md` - Hiá»ƒu Ä‘áº·c thÃ¹ cá»§a time series

**ChÃºc báº¡n há»c tá»‘t! ğŸ“**
