# ğŸ“° CRAWL TIN Tá»¨C VIá»†T NAM
## CafeF & VnExpress - Nguá»“n tin chá»©ng khoÃ¡n Viá»‡t

---

## ğŸ“š Má»¤C Lá»¤C

1. [Táº¡i sao chá»n CafeF & VnExpress?](#1-táº¡i-sao-chá»n-cafef--vnexpress)
2. [Kiáº¿n trÃºc Crawler](#2-kiáº¿n-trÃºc-crawler)
3. [Crawl CafeF News](#3-crawl-cafef-news)
4. [Crawl VnExpress News](#4-crawl-vnexpress-news)
5. [Data Schema](#5-data-schema)
6. [Best Practices](#6-best-practices)
7. [BÃ i táº­p thá»±c hÃ nh](#7-bÃ i-táº­p-thá»±c-hÃ nh)

---

## 1. Táº I SAO CHá»ŒN CAFEF & VNEXPRESS?

### ğŸ¯ So sÃ¡nh cÃ¡c nguá»“n tin Viá»‡t Nam

| Nguá»“n | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | ÄÃ¡nh giÃ¡ |
|-------|---------|------------|----------|
| **CafeF** | ChuyÃªn chá»©ng khoÃ¡n, cÃ³ API, data sáº¡ch | Ãt tin tá»•ng há»£p | â­â­â­â­â­ |
| **VnExpress** | Nhiá»u tin, uy tÃ­n, dá»… crawl | Nhiá»u noise, cáº§n filter | â­â­â­â­ |
| **Vneconomy** | ChuyÃªn kinh táº¿ | Ãt tin vá» cá»• phiáº¿u cá»¥ thá»ƒ | â­â­â­ |
| **Äáº§u tÆ°** | ChuyÃªn Ä‘áº§u tÆ° | Website phá»©c táº¡p | â­â­â­ |
| **Bloomberg VN** | Cháº¥t lÆ°á»£ng cao | Ãt tin, paywall | â­â­ |

### âœ… LÃ½ do chá»n CafeF + VnExpress

**CafeF:**
- âœ… ChuyÃªn vá» chá»©ng khoÃ¡n VN
- âœ… CÃ³ API/RSS feed
- âœ… Tin tá»©c real-time
- âœ… PhÃ¢n loáº¡i rÃµ rÃ ng (cÃ´ng ty, ngÃ nh)
- âœ… ÄÃ£ cÃ³ sáºµn price crawler

**VnExpress:**
- âœ… Nguá»“n tin uy tÃ­n nháº¥t VN
- âœ… Coverage rá»™ng (kinh táº¿, chÃ­nh trá»‹, xÃ£ há»™i)
- âœ… Dá»… crawl (HTML structure á»•n Ä‘á»‹nh)
- âœ… Nhiá»u tin tÃ¡c Ä‘á»™ng giÃ¡n tiáº¿p Ä‘áº¿n thá»‹ trÆ°á»ng
- âœ… SEO tá»‘t â†’ tin Ä‘Æ°á»£c Ä‘á»c nhiá»u

### ğŸ¯ Chiáº¿n lÆ°á»£c káº¿t há»£p

```
CafeF (60%):
- Tin chá»©ng khoÃ¡n trá»±c tiáº¿p
- BÃ¡o cÃ¡o tÃ i chÃ­nh
- PhÃ¢n tÃ­ch ká»¹ thuáº­t
- Khuyáº¿n nghá»‹ mua/bÃ¡n

VnExpress (40%):
- Tin kinh táº¿ vÄ© mÃ´
- ChÃ­nh sÃ¡ch má»›i
- Scandal, sá»± kiá»‡n lá»›n
- Sentiment thá»‹ trÆ°á»ng
```

---

## 2. KIáº¾N TRÃšC CRAWLER

### ğŸ“Š Tá»•ng quan há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEWS CRAWLER SYSTEM                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ CafeF Crawlerâ”‚         â”‚VnExpress     â”‚             â”‚
â”‚  â”‚              â”‚         â”‚Crawler       â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                        â”‚                      â”‚
â”‚         â–¼                        â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚     Raw News Data Storage           â”‚               â”‚
â”‚  â”‚  (JSON files / Database)            â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                â”‚                                        â”‚
â”‚                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚     News Cleaning & Processing      â”‚               â”‚
â”‚  â”‚  - Remove HTML tags                 â”‚               â”‚
â”‚  â”‚  - Extract metadata                 â”‚               â”‚
â”‚  â”‚  - Deduplicate                      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                â”‚                                        â”‚
â”‚                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚     Link with Stock Symbols         â”‚               â”‚
â”‚  â”‚  - Detect ticker mentions           â”‚               â”‚
â”‚  â”‚  - Classify relevance               â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                â”‚                                        â”‚
â”‚                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚     Clean News Database             â”‚               â”‚
â”‚  â”‚  data/news/cafef/                   â”‚               â”‚
â”‚  â”‚  data/news/vnexpress/               â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ Tech Stack

```python
# Core libraries
requests          # HTTP requests
BeautifulSoup4    # HTML parsing
selenium          # Dynamic content (if needed)
scrapy            # Advanced crawling (optional)

# Vietnamese NLP
underthesea       # Vietnamese tokenizer
pyvi              # Vietnamese NLP toolkit
vncorenlp         # Vietnamese CoreNLP

# Storage
pandas            # Data manipulation
sqlite3           # Local database
pymongo           # MongoDB (optional)

# Utils
schedule          # Cron jobs
logging           # Logging
tqdm              # Progress bars
```

---

## 3. CRAWL CAFEF NEWS

### ğŸ¯ CafeF News Structure

**URL patterns:**
```
Tin tá»•ng há»£p:
https://cafef.vn/thi-truong-chung-khoan.chn

Tin theo mÃ£:
https://cafef.vn/FPT-ctcp-tap-doan-fpt.chn

RSS Feed:
https://cafef.vn/rss/thi-truong-chung-khoan.rss
```

### ğŸ“Š HTML Structure

```html
<div class="tlitem">
    <h3 class="title">
        <a href="/link-to-article">TiÃªu Ä‘á» bÃ i viáº¿t</a>
    </h3>
    <div class="sapo">TÃ³m táº¯t bÃ i viáº¿t...</div>
    <div class="time">10:30 28/01/2026</div>
    <div class="category">Chá»©ng khoÃ¡n</div>
</div>
```

### ğŸ”§ Implementation

**BÆ°á»›c 1: Basic Crawler**
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time

class CafeFNewsCrawler:
    """
    Crawler cho tin tá»©c CafeF
    """
    
    def __init__(self):
        self.base_url = "https://cafef.vn"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def crawl_news_list(self, category='thi-truong-chung-khoan', pages=5):
        """
        Crawl danh sÃ¡ch tin tá»©c
        
        Args:
            category: Danh má»¥c tin (default: thá»‹ trÆ°á»ng chá»©ng khoÃ¡n)
            pages: Sá»‘ trang cáº§n crawl
        
        Returns:
            List of news items
        """
        news_list = []
        
        for page in range(1, pages + 1):
            url = f"{self.base_url}/{category}/trang-{page}.chn"
            
            try:
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # TÃ¬m táº¥t cáº£ tin tá»©c
                articles = soup.find_all('div', class_='tlitem')
                
                for article in articles:
                    news_item = self._parse_article_item(article)
                    if news_item:
                        news_list.append(news_item)
                
                print(f"Crawled page {page}: {len(articles)} articles")
                
                # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block
                time.sleep(2)
                
            except Exception as e:
                print(f"Error crawling page {page}: {e}")
                continue
        
        return news_list
    
    def _parse_article_item(self, article):
        """
        Parse thÃ´ng tin tá»« 1 article item
        """
        try:
            # TiÃªu Ä‘á» vÃ  link
            title_tag = article.find('h3', class_='title')
            if not title_tag:
                return None
            
            link_tag = title_tag.find('a')
            title = link_tag.text.strip()
            link = self.base_url + link_tag['href']
            
            # TÃ³m táº¯t
            sapo_tag = article.find('div', class_='sapo')
            summary = sapo_tag.text.strip() if sapo_tag else ""
            
            # Thá»i gian
            time_tag = article.find('div', class_='time')
            pub_time = time_tag.text.strip() if time_tag else ""
            
            # Category
            cat_tag = article.find('div', class_='category')
            category = cat_tag.text.strip() if cat_tag else ""
            
            return {
                'title': title,
                'summary': summary,
                'link': link,
                'published_time': pub_time,
                'category': category,
                'source': 'CafeF',
                'crawled_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"Error parsing article: {e}")
            return None
    
    def crawl_article_content(self, url):
        """
        Crawl ná»™i dung chi tiáº¿t bÃ i viáº¿t
        
        Args:
            url: URL bÃ i viáº¿t
        
        Returns:
            Article content
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m ná»™i dung chÃ­nh
            content_div = soup.find('div', class_='detail-content')
            
            if not content_div:
                return None
            
            # Láº¥y táº¥t cáº£ paragraphs
            paragraphs = content_div.find_all('p')
            content = '\n'.join([p.text.strip() for p in paragraphs])
            
            # Láº¥y tags/keywords
            tags = []
            tag_div = soup.find('div', class_='tags')
            if tag_div:
                tag_links = tag_div.find_all('a')
                tags = [tag.text.strip() for tag in tag_links]
            
            return {
                'content': content,
                'tags': tags
            }
            
        except Exception as e:
            print(f"Error crawling article content: {e}")
            return None
```

**BÆ°á»›c 2: Crawl vá»›i Full Content**
```python
def crawl_full_news(crawler, category='thi-truong-chung-khoan', pages=5):
    """
    Crawl tin tá»©c vá»›i full content
    """
    # BÆ°á»›c 1: Crawl danh sÃ¡ch
    print("Step 1: Crawling news list...")
    news_list = crawler.crawl_news_list(category=category, pages=pages)
    print(f"Found {len(news_list)} articles")
    
    # BÆ°á»›c 2: Crawl content cho tá»«ng bÃ i
    print("\nStep 2: Crawling full content...")
    for i, news in enumerate(news_list, 1):
        print(f"[{i}/{len(news_list)}] Crawling: {news['title'][:50]}...")
        
        content_data = crawler.crawl_article_content(news['link'])
        
        if content_data:
            news['content'] = content_data['content']
            news['tags'] = content_data['tags']
        else:
            news['content'] = ""
            news['tags'] = []
        
        # Delay
        time.sleep(1)
    
    # BÆ°á»›c 3: Save to DataFrame
    df = pd.DataFrame(news_list)
    
    return df

# Sá»­ dá»¥ng
crawler = CafeFNewsCrawler()
df_cafef = crawl_full_news(crawler, pages=10)

# Save
df_cafef.to_csv('data/news/cafef/news_raw.csv', index=False, encoding='utf-8-sig')
print(f"\nSaved {len(df_cafef)} articles to data/news/cafef/news_raw.csv")
```

---

## 4. CRAWL VNEXPRESS NEWS

### ğŸ¯ VnExpress Structure

**URL patterns:**
```
Kinh doanh:
https://vnexpress.net/kinh-doanh

Chá»©ng khoÃ¡n:
https://vnexpress.net/kinh-doanh/chung-khoan

RSS:
https://vnexpress.net/rss/kinh-doanh.rss
```

### ğŸ”§ Implementation

```python
class VnExpressNewsCrawler:
    """
    Crawler cho tin tá»©c VnExpress
    """
    
    def __init__(self):
        self.base_url = "https://vnexpress.net"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def crawl_news_list(self, category='kinh-doanh/chung-khoan', pages=5):
        """
        Crawl danh sÃ¡ch tin VnExpress
        """
        news_list = []
        
        for page in range(1, pages + 1):
            url = f"{self.base_url}/{category}-p{page}"
            
            try:
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # VnExpress dÃ¹ng class 'item-news'
                articles = soup.find_all('article', class_='item-news')
                
                for article in articles:
                    news_item = self._parse_article_item(article)
                    if news_item:
                        news_list.append(news_item)
                
                print(f"Crawled page {page}: {len(articles)} articles")
                time.sleep(2)
                
            except Exception as e:
                print(f"Error crawling page {page}: {e}")
                continue
        
        return news_list
    
    def _parse_article_item(self, article):
        """
        Parse article item VnExpress
        """
        try:
            # Title vÃ  link
            title_tag = article.find('h3', class_='title-news')
            if not title_tag:
                return None
            
            link_tag = title_tag.find('a')
            title = link_tag['title']
            link = link_tag['href']
            
            # Náº¿u link relative, thÃªm base_url
            if not link.startswith('http'):
                link = self.base_url + link
            
            # Summary
            desc_tag = article.find('p', class_='description')
            summary = desc_tag.text.strip() if desc_tag else ""
            
            # Time
            time_tag = article.find('span', class_='time')
            pub_time = time_tag.text.strip() if time_tag else ""
            
            return {
                'title': title,
                'summary': summary,
                'link': link,
                'published_time': pub_time,
                'category': 'Kinh doanh',
                'source': 'VnExpress',
                'crawled_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"Error parsing article: {e}")
            return None
    
    def crawl_article_content(self, url):
        """
        Crawl ná»™i dung bÃ i viáº¿t VnExpress
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # VnExpress dÃ¹ng class 'fck_detail'
            content_div = soup.find('article', class_='fck_detail')
            
            if not content_div:
                return None
            
            # Láº¥y paragraphs
            paragraphs = content_div.find_all('p', class_='Normal')
            content = '\n'.join([p.text.strip() for p in paragraphs])
            
            # Tags
            tags = []
            tag_div = soup.find('div', class_='tags')
            if tag_div:
                tag_links = tag_div.find_all('a')
                tags = [tag.text.strip() for tag in tag_links]
            
            return {
                'content': content,
                'tags': tags
            }
            
        except Exception as e:
            print(f"Error crawling content: {e}")
            return None
```

---

## 5. DATA SCHEMA

### ğŸ“Š News Data Schema

```python
news_schema = {
    'id': 'unique_id',                    # UUID
    'title': 'TiÃªu Ä‘á» bÃ i viáº¿t',         # str
    'summary': 'TÃ³m táº¯t',                 # str
    'content': 'Ná»™i dung Ä‘áº§y Ä‘á»§',        # str (long text)
    'link': 'URL bÃ i viáº¿t',               # str
    'published_time': '28/01/2026 10:30', # str (cáº§n parse)
    'category': 'Chá»©ng khoÃ¡n',           # str
    'tags': ['FPT', 'CÃ´ng nghá»‡'],       # list
    'source': 'CafeF',                    # str (CafeF/VnExpress)
    'crawled_at': '2026-01-28T10:30:00', # ISO format
    
    # ThÃªm sau khi process
    'tickers_mentioned': ['FPT', 'VCB'], # list (detected)
    'sentiment_score': 0.75,              # float [-1, 1]
    'event_type': 'earnings',             # str (classified)
    'is_relevant': True,                  # bool
}
```

### ğŸ’¾ Storage Structure

```
data/news/
â”œâ”€â”€ cafef/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ 2026-01-28.csv
â”‚   â”‚   â”œâ”€â”€ 2026-01-29.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ 2026-01-28_processed.csv
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ vnexpress/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ combined/
    â”œâ”€â”€ news_all.csv
    â””â”€â”€ news_with_tickers.csv
```

---

## 6. BEST PRACTICES

### âš ï¸ Ethical Crawling

**1. Respect robots.txt:**
```python
# Check robots.txt trÆ°á»›c khi crawl
import urllib.robotparser

rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://cafef.vn/robots.txt")
rp.read()

if rp.can_fetch("*", "https://cafef.vn/thi-truong-chung-khoan.chn"):
    # OK to crawl
    pass
```

**2. Rate limiting:**
```python
import time
from datetime import datetime

class RateLimiter:
    def __init__(self, requests_per_minute=30):
        self.requests_per_minute = requests_per_minute
        self.min_interval = 60.0 / requests_per_minute
        self.last_request = None
    
    def wait(self):
        if self.last_request:
            elapsed = (datetime.now() - self.last_request).total_seconds()
            if elapsed < self.min_interval:
                time.sleep(self.min_interval - elapsed)
        
        self.last_request = datetime.now()

# Sá»­ dá»¥ng
limiter = RateLimiter(requests_per_minute=30)

for url in urls:
    limiter.wait()
    response = requests.get(url)
```

**3. User-Agent rotation:**
```python
import random

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
]

def get_random_headers():
    return {'User-Agent': random.choice(USER_AGENTS)}
```

### ğŸ”§ Error Handling

```python
import logging
from tenacity import retry, stop_after_attempt, wait_exponential

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def crawl_with_retry(url):
    """
    Crawl vá»›i retry logic
    """
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return response
    
    except requests.Timeout:
        logger.error(f"Timeout: {url}")
        raise
    
    except requests.HTTPError as e:
        if e.response.status_code == 404:
            logger.warning(f"404 Not Found: {url}")
            return None
        else:
            logger.error(f"HTTP Error {e.response.status_code}: {url}")
            raise
    
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise
```

### ğŸ“… Scheduling

```python
import schedule

def daily_crawl_job():
    """
    Job cháº¡y hÃ ng ngÃ y
    """
    print(f"Starting daily crawl at {datetime.now()}")
    
    # Crawl CafeF
    cafef_crawler = CafeFNewsCrawler()
    df_cafef = crawl_full_news(cafef_crawler, pages=5)
    
    # Crawl VnExpress
    vnexpress_crawler = VnExpressNewsCrawler()
    df_vnexpress = crawl_full_news(vnexpress_crawler, pages=5)
    
    # Save
    today = datetime.now().strftime('%Y-%m-%d')
    df_cafef.to_csv(f'data/news/cafef/raw/{today}.csv', index=False)
    df_vnexpress.to_csv(f'data/news/vnexpress/raw/{today}.csv', index=False)
    
    print(f"Completed: {len(df_cafef)} CafeF + {len(df_vnexpress)} VnExpress")

# Schedule: Cháº¡y má»—i ngÃ y lÃºc 8:00 AM
schedule.every().day.at("08:00").do(daily_crawl_job)

# Run scheduler
while True:
    schedule.run_pending()
    time.sleep(60)
```

---

## 7. BÃ€I Táº¬P THá»°C HÃ€NH

### ğŸ¯ BÃ i táº­p 1: Crawl CafeF News

**Äá» bÃ i:**
Implement crawler cho CafeF, crawl 100 bÃ i viáº¿t gáº§n nháº¥t

**YÃªu cáº§u:**
- Crawl cáº£ title, summary, content
- Save to CSV vá»›i encoding UTF-8
- Handle errors gracefully
- Implement rate limiting

**Kiá»ƒm tra:**
- [ ] Crawl Ä‘Æ°á»£c 100 bÃ i
- [ ] Content Ä‘áº§y Ä‘á»§, khÃ´ng bá»‹ lá»—i encoding
- [ ] CÃ³ error handling
- [ ] CÃ³ rate limiting

---

### ğŸ¯ BÃ i táº­p 2: Crawl VnExpress News

**Äá» bÃ i:**
Implement crawler cho VnExpress, crawl tin kinh doanh 7 ngÃ y gáº§n nháº¥t

**YÃªu cáº§u:**
- Crawl tá»« category "Kinh doanh"
- Filter chá»‰ láº¥y tin liÃªn quan chá»©ng khoÃ¡n
- Detect vÃ  extract ticker mentions
- Save to database (SQLite)

**Kiá»ƒm tra:**
- [ ] Crawl Ä‘Æ°á»£c tin 7 ngÃ y
- [ ] Filter Ä‘Ãºng tin chá»©ng khoÃ¡n
- [ ] Detect Ä‘Æ°á»£c tickers
- [ ] Save vÃ o SQLite

---

### ğŸ¯ BÃ i táº­p 3: Combined Crawler

**Äá» bÃ i:**
Káº¿t há»£p 2 crawlers, táº¡o unified news database

**YÃªu cáº§u:**
- Crawl Ä‘á»“ng thá»i CafeF + VnExpress
- Deduplicate (loáº¡i tin trÃ¹ng)
- Link vá»›i VN30 tickers
- Create daily reports

**Kiá»ƒm tra:**
- [ ] Crawl Ä‘Æ°á»£c cáº£ 2 nguá»“n
- [ ] Deduplicate thÃ nh cÃ´ng
- [ ] Link vá»›i tickers
- [ ] Generate reports

---

## âœ… KIá»‚M TRA HIá»‚U BÃ€I

TrÆ°á»›c khi sang bÃ i tiáº¿p theo, hÃ£y Ä‘áº£m báº£o báº¡n:

- [ ] Hiá»ƒu táº¡i sao chá»n CafeF & VnExpress
- [ ] Implement Ä‘Æ°á»£c CafeF crawler
- [ ] Implement Ä‘Æ°á»£c VnExpress crawler
- [ ] Hiá»ƒu HTML structure cá»§a 2 sites
- [ ] Handle Ä‘Æ°á»£c errors vÃ  rate limiting
- [ ] Save Ä‘Æ°á»£c data vá»›i encoding Ä‘Ãºng
- [ ] LÃ m Ä‘Æ°á»£c 3 bÃ i táº­p thá»±c hÃ nh

**Náº¿u chÆ°a pass háº¿t checklist, Ä‘á»c láº¡i pháº§n tÆ°Æ¡ng á»©ng!**

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

**Libraries:**
- BeautifulSoup4: HTML parsing
- Scrapy: Advanced crawling framework
- Selenium: Dynamic content

**Vietnamese NLP:**
- underthesea: Vietnamese NLP toolkit
- pyvi: Vietnamese word segmentation
- vncorenlp: Vietnamese CoreNLP

**Best Practices:**
- "Web Scraping with Python" - Ryan Mitchell
- Scrapy documentation
- robots.txt guidelines

---

## ğŸš€ BÆ¯á»šC TIáº¾P THEO

Sau khi hoÃ n thÃ nh bÃ i nÃ y, sang:
- `02_VIETNAMESE_TEXT_PROCESSING.md` - Xá»­ lÃ½ tiáº¿ng Viá»‡t & sentiment

**ChÃºc báº¡n há»c tá»‘t! ğŸ“**
